<!DOCTYPE html>
<html lang="en-US">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math">
<section class="paragraphs"><h4 class="heading"><span class="title"><span class="process-math">\(\defn\)</span> – Multilinear and Alternating.</span></h4> <div class="para logical">
<div class="para">For any commutative ring <span class="process-math">\(R\)</span> and <span class="process-math">\(R\)</span>-module <span class="process-math">\(V\text{,}\)</span> given a function of the form</div>
<div class="displaymath process-math">
\begin{equation*}
\phi: \overbrace{V \times \cdots \times V}^n \to R
\end{equation*}
</div>
<div class="para">we say: 1. <span class="process-math">\(\phi\)</span> is <span class="process-math">\(R\)</span>-<em class="emphasis"><span class="process-math">\(\defnn{mutli-linear}\)</span></em> if, for each <span class="process-math">\(i\text{,}\)</span> we have</div>
<div class="displaymath process-math">
\begin{equation*}
\phi(v_1, \dots, v_{i-1}, v_i + v'_i, v_{i+1}, \dots, v_n) = \phi(v_1, \dots, v_{i-1}, v_i , v_{i+1}, \dots, v_n) +\phi(v_1, \dots, v_{i-1},  v_i', v_{i+1}, \dots, v_n)
\end{equation*}
</div>
<div class="para">and</div>
<div class="displaymath process-math">
\begin{equation*}
\phi(v_1, \dots, v_{i-1}, r v_i, v_{i+1}, \dots, v_n) = r \phi(v_1, \dots, v_{i-1},  v_i, v_{i+1}, \dots, v_n) 
\end{equation*}
</div>
<div class="para">for all elements <span class="process-math">\(v_1, \dots, v_n, v_i'\)</span> in <span class="process-math">\(V\)</span> and all <span class="process-math">\(r \in R\text{.}\)</span> 2. <span class="process-math">\(\phi\)</span> is <em class="emphasis">alternating</em> if <span class="process-math">\(\phi(v_1, \dots, v_n) = 0\)</span> if <span class="process-math">\(v_i = v_j\)</span> for some <span class="process-math">\(i \ne j\text{.}\)</span>
</div>
</div> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\exe\)</span> – Multilinear Maps when <span class="process-math">\(n=1\)</span>.</span></h5> <div class="para">When <span class="process-math">\(n=1\text{,}\)</span> a multi-linear map is the same thing as an <span class="process-math">\(R\)</span>-module homomorphism.(The alternating condition is vacuous in this case.)</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\exe\)</span> – <span class="process-math">\(R\)</span>-Bilinear Maps (<span class="process-math">\(n=2\)</span>).</span></h5> <div class="para">When <span class="process-math">\(n= 2\text{,}\)</span> instead of “<span class="process-math">\(R\)</span>-multilinear’’ we say”<span class="process-math">\(R\)</span>-bilinear’‘. If <span class="process-math">\(\phi: V \times V \to R\)</span> is <span class="process-math">\(R\)</span>-bilinear then<div class="displaymath process-math">
\begin{equation*}
\phi(v+w, u+m) = \phi(v,u) + \phi(v,m) + \phi(w,u)+\phi(w,m).
\end{equation*}
</div>Also <span class="process-math">\(\phi(rv,w) = r\phi(v,w) = \phi(v, rw)\text{.}\)</span> For any example of this, take <span class="process-math">\(V = R^m\text{.}\)</span> Then the “dot product’’ <span class="process-math">\(\phi:V \times V \to R\)</span> defined by <span class="process-math">\(\phi((a_1, \dots, a_m)^\tau, (b_1, \dots, b_m)^\tau) = \sum_i a_i b_i\)</span> is <span class="process-math">\(R\)</span>-bilinear. But the dot product isn’t alternating.</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\exe\)</span> – Determinant Formula Using Bilinear Map.</span></h5> <div class="para logical">
<div class="para">For <span class="process-math">\(n =2\)</span> and <span class="process-math">\(V = R^2\text{,}\)</span> <span class="process-math">\(\phi: V \times V \to R\)</span> defined by</div>
<div class="displaymath process-math">
\begin{equation*}
\phi(\begin{bmatrix} x \\ y \end{bmatrix}, \begin{bmatrix} z \\ w \end{bmatrix}) = xw - yz
\end{equation*}
</div>
<div class="para">is both <span class="process-math">\(R\)</span>-bilinear and alternating. This is of course the familiar determinant formula.</div>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\rem\)</span>.</span></h5> <div class="para logical">
<div class="para">The multi-linearity property of a function <span class="process-math">\(\phi: V^{\times n} \to R\)</span> can equivalently be stated as follows: If we fixed <span class="process-math">\(n-1\)</span> of the <span class="process-math">\(n\)</span> inputs and let the remaining input vary, then the resulting function is an <span class="process-math">\(R\)</span>-module homomorphism. More precisely: <span class="process-math">\(\phi\)</span> is <span class="process-math">\(R\)</span>-multi-linear if and only if for each <span class="process-math">\(i\)</span> and for each choice of elements <span class="process-math">\(v_1, \dots, v_{i-1}, v_{i+1}, \dots, v_n\)</span> in <span class="process-math">\(V\text{,}\)</span> the function</div>
<div class="displaymath process-math">
\begin{equation*}
g: V \to R
\end{equation*}
</div>
<div class="para">defined by <span class="process-math">\(g(w) = \phi(v_1, \dots, v_{i-1}, w, v_{i+1}, \dots, v_n)\)</span> is a <span class="process-math">\(R\)</span>-module homomorphism.</div>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\lem\)</span> – Multilinear Maps and Permutations.</span></h5> <div class="para logical">
<div class="para">Assume <span class="process-math">\(\phi: V^{\times n} \to R\)</span> is <span class="process-math">\(R\)</span>-multilinear and alternating. Then for any permutation <span class="process-math">\(\tau \in S_n\)</span> and <span class="process-math">\(v_1, \dots, v_n \in V\text{,}\)</span> we have</div>
<div class="displaymath process-math">
\begin{equation*}
\phi(v_1, \dots, v_n) = \sign(\tau) \phi(v_{\tau(1)}, \dots, v_{\tau(n)})
\end{equation*}
</div>
<div class="para">where <span class="process-math">\(\sign(\tau)\)</span> is the sign of the permutation <span class="process-math">\(\tau\text{.}\)</span>
</div>
</div> <section class="paragraphs"><h6 class="heading"><span class="title"><em class="emphasis">Proof.</em>.</span></h6> <div class="para logical">
<div class="para">Let us first prove this in the case when <span class="process-math">\(n = 2\)</span> and <span class="process-math">\(\tau\)</span> is the only non-trivial element of <span class="process-math">\(S_2\text{.}\)</span> Say</div>
<div class="displaymath process-math">
\begin{equation*}
\psi: V^{\times 2} \to R
\end{equation*}
</div>
<div class="para">is <span class="process-math">\(R\)</span>-multi-linear (<span class="process-math">\(R\)</span>-bilinear) and alternating. The goal is to prove</div>
<div class="displaymath process-math">
\begin{equation*}
\psi(v,w) = -\psi(w,v)
\end{equation*}
</div>
<div class="para">for any <span class="process-math">\(v,w\text{.}\)</span>
</div>
</div> <div class="para logical">
<div class="para">We have</div>
<div class="displaymath process-math">
\begin{equation*}
\psi(v+w, v+w) = 0
\end{equation*}
</div>
<div class="para">on the one hand and</div>
<div class="displaymath process-math">
\begin{equation*}
\psi(v+w, v+w) = \psi(v, v+w) + \psi(w, v+w) = \psi(v,v) + \psi(v, w) + \psi(w,v) + \psi(w,w) = \psi(v, w) + \psi(w,v)  
\end{equation*}
</div>
<div class="para">on the other hand. It follows that</div>
<div class="displaymath process-math">
\begin{equation*}
\psi(v, w) + \psi(w,v)  = 0
\end{equation*}
</div>
<div class="para">which proves the Lemma in this case.</div>
</div> <div class="para logical">
<div class="para">Now let <span class="process-math">\(n\)</span> be arbitrary and <span class="process-math">\(\tau = (i \, j)\)</span> for some <span class="process-math">\(1 \leq i &lt; j \leq n\text{.}\)</span> In this case we need to show</div>
<div class="displaymath process-math">
\begin{equation*}
\phi(v_1, \dots, v_n) = - \phi(v_1, \dots, v_{i-1}, v_{j}, v_{i+1}, v_{i+2}, \dots, v_{j-1}, v_i, v_{j+1}, \dots, v_n)
\end{equation*}
</div>
<div class="para">for all <span class="process-math">\(v_1, \dots, v_n \in V\text{.}\)</span> Fix <span class="process-math">\(n-2\)</span> elements <span class="process-math">\(v_1, \dots, v_{i-1}, v_{i+1}, \dots, v_{j-1}, v_{j+1}, \dots, v_n\)</span> and define</div>
<div class="displaymath process-math">
\begin{equation*}
\psi: V^{\times 2} \to R
\end{equation*}
</div>
<div class="para">by</div>
<div class="displaymath process-math">
\begin{equation*}
\psi(v,w) = \phi(v_1, \dots, v_{i-1}, v, v_{i+1}, \dots, v_{j-1}, w,  v_{j+1}, \dots, v_n)
\end{equation*}
</div>
<div class="para">for any <span class="process-math">\(v,w \in V\text{.}\)</span> With this notation the goal is to show</div>
<div class="displaymath process-math">
\begin{equation*}
\psi(v,w) = -\psi(w,v)
\end{equation*}
</div>
<div class="para">Since <span class="process-math">\(\phi\)</span> is <span class="process-math">\(R\)</span>-multi-linear and alternating, <span class="process-math">\(\psi: V \times V\to R\)</span> is <span class="process-math">\(R\)</span>-bilinear and alternating, and so the case already proven gives the result.</div>
</div> <div class="para logical">
<div class="para">Finally let <span class="process-math">\(n\)</span> and <span class="process-math">\(\tau\)</span> be arbitrary. Then <span class="process-math">\(\tau\)</span> is a product of transpositions — say <span class="process-math">\(\tau = \tau_r \cdots \tau_1\)</span> with each <span class="process-math">\(\tau_i\)</span> a transposition. Proceed by induction on <span class="process-math">\(r\text{.}\)</span> The previous case establishes the result when <span class="process-math">\(r = 1\text{.}\)</span> If <span class="process-math">\(r \geq 2\text{,}\)</span> then using the previous case gives</div>
<div class="displaymath process-math">
\begin{equation*}
\psi(v_\tau(1), \dots, v_\tau(n)) = -\psi(v_\tau'(1), \dots, v_\tau'(n))
\end{equation*}
</div>
<div class="para">where <span class="process-math">\(\tau' = \tau_{r-1} \cdots \tau_1\)</span> and by induction <span class="process-math">\(\psi(v_\tau'(1), \dots, v_\tau'(n)) = (-1)^{r-1}\psi(v_1, \dots, v_n)\text{.}\)</span> Hence</div>
<div class="displaymath process-math">
\begin{equation*}
\psi(v_\tau(1), \dots, v_\tau(n)) = (-1)^{r}\psi(v_1, \dots, v_n)
\end{equation*}
</div>
<div class="para">which proves the result.</div>
</div></section></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\defn\)</span> – Determinant Function.</span></h5> <div class="para logical">
<div class="para">Define the function</div>
<div class="displaymath process-math">
\begin{equation*}
\det: \Mat_{n \times n} (R) \to R
\end{equation*}
</div>
<div class="para">by</div>
<div class="displaymath process-math">
\begin{equation*}
\det(A) = \sum_{\s \in S_n} \sign(\s) \prod_{i=1}^n a_{\s(i), i}
\end{equation*}
</div>
<div class="para">where <span class="process-math">\(S_n\)</span> is the group of all permutations of <span class="process-math">\(\{1, \dots, n\}\)</span> and <span class="process-math">\(\sign(\s)\)</span> refers to the sign of a permutation.</div>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\exe\)</span> – Determinant when <span class="process-math">\(n=2\)</span>.</span></h5> <div class="para logical">
<div class="para">For <span class="process-math">\(n = 2\)</span> we have[^1]</div>
<div class="displaymath process-math">
\begin{equation*}
\det(\begin{bmatrix} a_{1,1} &amp; a_{1,2} \\ a_{2,1} &amp; a_{2,2} \end{bmatrix}) = a_{1,1} a_{2,2} - a_{2,1} a_{1,2}.
\end{equation*}
</div>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\exe\)</span> – Determinant of Upper Triangular Matrix.</span></h5> <div class="para">If <span class="process-math">\(A\)</span> is upper triangular, then <span class="process-math">\(\det(A) = \prod_i a_{i,i}\)</span>[^1]. For note that for such a matrix the only non-zero terms in the formula for <span class="process-math">\(\det(A)\)</span> occur when <span class="process-math">\(\s(i) \leq i\)</span> for all <span class="process-math">\(i\text{,}\)</span> and this can only happen when <span class="process-math">\(\s = \id\text{.}\)</span> Similarly, if <span class="process-math">\(A\)</span> is lower triangular, then <span class="process-math">\(\det(A) = \prod_i a_{i,i}\text{.}\)</span>
</div> <div class="para">In particular, if <span class="process-math">\(E\)</span> is a Type I elementary matrix, then <span class="process-math">\(\det(E) = 1\)</span> and if <span class="process-math">\(E\)</span> is a type II elementary matrix with scalar <span class="process-math">\(u \ne 0\text{,}\)</span> then <span class="process-math">\(\det(E) = u\text{.}\)</span> We’ll look at the determinants of Type III elementary matrices later.</div> <div class="para">An even more special case is <span class="process-math">\(\det(I_n) = 1\text{.}\)</span>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\exe\)</span> – Transpose Preserves Det.</span></h5> <div class="para">Prove <span class="process-math">\(\det(A) = \det(A^\tau)\)</span>[^1] where <span class="process-math">\(\tau\)</span> denotes transpose.</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\rem\)</span>.</span></h5> <div class="para logical">
<div class="para">Let us write elements of <span class="process-math">\(\Mat_{n \times n}(R)\)</span> as <span class="process-math">\(n\)</span>-tuples of columns vectors; that is, we identify <span class="process-math">\(\Mat_{n \times n}(R)\)</span> with <span class="process-math">\(\overbrace{R^n \times \cdots \times R^n}^n\)</span> by identifying a matrix with the <span class="process-math">\(n\)</span>-tuple of its columns. Then <span class="process-math">\(\det\)</span> is a function of the form</div>
<div class="displaymath process-math">
\begin{equation*}
\det: \overbrace{R^n \times \cdots \times R^n}^n \to R
\end{equation*}
</div>
<div class="para">where for <span class="process-math">\(v_1, \dots, v_n \in R^n\text{,}\)</span> we let <span class="process-math">\(\det(v_1, \dots, v_n)\)</span> be the determinant of the <span class="process-math">\(n \times n\)</span> matrix whose columns are <span class="process-math">\(v_1, \dots, v_n\text{.}\)</span>
</div>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\thm\)</span> – Uniqueness of Determinant.</span></h5> <div class="para logical">
<div class="para">For each fix positive integer <span class="process-math">\(n\)</span> and non-zero commutative ring <span class="process-math">\(R\text{,}\)</span> the determinant function is the unique function of the form</div>
<div class="displaymath process-math">
\begin{equation*}
\phi: \overbrace{R^n \times \cdots \times R^n}^n \to R
\end{equation*}
</div>
<div class="para">such that 1. <span class="process-math">\(\phi\)</span> is <span class="process-math">\(R\)</span>-multilinear, 2. <span class="process-math">\(\phi\)</span> is alternating, and 3. <span class="process-math">\(\phi(I_n) =1\)</span> (i.e., <span class="process-math">\(\phi(e_1, \dots, e_n) = 1\)</span> where <span class="process-math">\(e_i\)</span> is the <span class="process-math">\(i\)</span>-th standard column vector).</div>
</div> <section class="paragraphs"><h6 class="heading"><span class="title"><em class="emphasis">Proof.</em>.</span></h6> <div class="para">We start by proving that <span class="process-math">\(\det\)</span> has these properties.</div> <div class="para logical">
<div class="para">Let <span class="process-math">\(v_1, \dots, v_n\)</span> form the columns of a matrix <span class="process-math">\(A\text{.}\)</span> (I.e, the <span class="process-math">\(s\)</span>-th entry of <span class="process-math">\(v_t\)</span> is <span class="process-math">\(a_{s,t}\text{.}\)</span>) We have</div>
<div class="displaymath process-math">
\begin{equation*}
\begin{aligned}
\det(v_1, \dots, v_{i-1}, r v_i + v'_i, v_{i+1}, \dots, v_n) 
&amp; =  \sum_{\s} \sign(\s) \prod_{j\ne i} a_{\s(j), j} \cdot (r a_{\s(i), i} + a'_{\s(i), i})\\
&amp; =  r \sum_{\s} \sign(\s) \prod_{j\ne i} a_{\s(j), j} \cdot (a_{\s(i), i})  \\
&amp; + \sum_{\s} \sign(\s) \prod_{j\ne i} a_{\s(j), j} \cdot (a'_{\s(i), i})  \\
&amp; = r \det(v_1, \dots, v_{i-1}, v_i , v_{i+1}, \dots, v_n) +
\det(v_1, \dots, v_{i-1},  v_i', v_{i+1}, \dots, v_n) \\
\end{aligned}
\end{equation*}
</div>
<div class="para">and this proves <span class="process-math">\(R\)</span>-multi-linearity.</div>
</div> <div class="para logical">
<div class="para">For the alternating property, for notational simplicity, let us assume that <span class="process-math">\(v_1 = v_2\)</span> (i.e., the first two columns of the matrix are equal). (The proof of the general case is essentially the same.) Note that</div>
<div class="displaymath process-math">
\begin{equation*}
\det(A) = \sum_{(i,j, g)} \sign(i,j,g) a_{i,1} a_{j,2} a_{g(3), 3} a_{g(4), 4} \cdots a_{g(n), n}
\end{equation*}
</div>
<div class="para">where the sum ranges over triples <span class="process-math">\((i,j,g)\)</span> where <span class="process-math">\(i \ne j\)</span> and <span class="process-math">\(g\)</span> is an injective function from <span class="process-math">\(\{3, \dots, n\}\)</span> to <span class="process-math">\(\{1, \dots, n\} \setminus \{i,j\}\)</span> and by <span class="process-math">\(\sign(i,j,g)\)</span> we mean the sign of the permutation sending <span class="process-math">\(1\)</span> to <span class="process-math">\(i\text{,}\)</span> <span class="process-math">\(2\)</span> to <span class="process-math">\(j\)</span> and <span class="process-math">\(s\)</span> to <span class="process-math">\(g(s)\)</span> for all other <span class="process-math">\(s\text{.}\)</span> For each <span class="process-math">\((i,j,g)\text{,}\)</span> we have <span class="process-math">\(\sign(i,j,g) = -\sign(j,i,g)\text{,}\)</span> since the corresponding permutations differ by a transposition. Since <span class="process-math">\(a_{i,1} = a_{i,2}\)</span> and <span class="process-math">\(a_{j,1} = a_{j,2}\text{,}\)</span> it follows that the terms in the formula for <span class="process-math">\(\det(A)\)</span> cancel in pairs to give <span class="process-math">\(0\text{.}\)</span>
</div>
</div> <div class="para">The property <span class="process-math">\(\det(I_n) = 1\)</span> follows from the formula. (See Example .)</div> <div class="para">Let us now prove the uniqueness part of the Theorem:</div> <div class="para logical">
<div class="para">Let <span class="process-math">\(\phi\)</span> be any function with the three properties stated in the Theorem. Let <span class="process-math">\(A = (a_{i,j})\)</span> be any square matrix with columns <span class="process-math">\(v_1, \dots, v_n\text{.}\)</span> Note that <span class="process-math">\(v_j = \sum_i a_{i,j} e_i\)</span> where <span class="process-math">\(e_i\)</span> is the <span class="process-math">\(i\)</span>-th standard basis (column) vector. Using the <span class="process-math">\(R\)</span>-multi-linearity property repeatedly we have</div>
<div class="displaymath process-math">
\begin{equation*}
\begin{aligned}
\phi(A) &amp; = \phi(\sum_{i_1} a_{i_1,1} e_{i_1},  \sum_{i_2} a_{i_2,2} e_{i_2}, \cdots, \sum_{i_n} a_{i_n,n} e_{i_n}) \\
&amp; = \sum_{i_1} a_{i_1,1} \phi(e_{i_1},  \sum_{i_2} a_{i_2,2} e_{i_2}, \cdots, \sum_{i_n} a_{i_n,n} e_{i_n}) \\
&amp; = \sum_{i_1}a_{i_1,1} \sum_{i_2} a_{i_2,2} \phi(e_{i_1},   e_{i_2}, \cdots, \sum_{i_n} a_{i_n,n} e_{i_n}) \\
&amp; = \vdots \\
&amp; = \sum_{ i_1, \dots, i_n } a_{i_1, 1} \cdots a_{i_n,n} \phi(e_{i_1}, \dots, e_{i_n}) \\
\end{aligned}
\end{equation*}
</div>
<div class="para">where each of <span class="process-math">\(i_1, \dots, i_n\)</span> ranges from <span class="process-math">\(1\)</span> to <span class="process-math">\(n\text{.}\)</span> By the alternating property <span class="process-math">\(\phi(e_{i_1}, \dots, e_{i_n}) = 0\)</span> unless all the <span class="process-math">\(i_1, \dots, i_n\)</span> are distinct. If they are distinct, then by Lemma  we have</div>
<div class="displaymath process-math">
\begin{equation*}
\phi(e_{i_1}, \dots, e_{i_n}) = \sign(\s) \phi(e_1, \dots, e_n)
\end{equation*}
</div>
<div class="para">where <span class="process-math">\(\s\)</span> is the permutation defined as <span class="process-math">\(\s(t) = i_t\text{.}\)</span> Finally, recall <span class="process-math">\(\phi(e_1, \dots, e_n) = 1\)</span> by assumption. Putting all of this together gives</div>
<div class="displaymath process-math">
\begin{equation*}
\phi(A) = \sum_{\text{ distinct  $i_1, \dots, i_n$}} \sign(\text{the permutation $t \mapsto i_t$}) a_{i_1, 1} \cdots a_{i_n,n}.
\end{equation*}
</div>
<div class="para">This is just a re-writing of the formula for <span class="process-math">\(\det\text{;}\)</span> so <span class="process-math">\(\phi = \det\text{.}\)</span>
</div>
</div></section></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\prop\)</span> – Computing Determinant of Square Matrix.</span></h5> <div class="para">Let <span class="process-math">\(R\)</span> be any non-zero commutative ring. Let <span class="process-math">\(A\)</span> be a square matrix and let <span class="process-math">\(B\)</span> be a matrix obtained form <span class="process-math">\(A\)</span> by a single elementary column operation: #fix 1. If the operation is of type I, <span class="process-math">\(\det(B) = \det(A)\)</span>[^1]. 2. If the operation is of type II, given by multiplying a column of <span class="process-math">\(A\)</span> by a unit <span class="process-math">\(u\text{,}\)</span> then <span class="process-math">\(\det(B)=u\det(A)\text{.}\)</span> 3. If the operation is of type III, <span class="process-math">\(\det(B)=-\det(A)\text{.}\)</span>
</div> <section class="paragraphs"><h6 class="heading"><span class="title"><em class="emphasis">Proof.</em>.</span></h6> <div class="para logical">
<div class="para">The first claim follows from multi-linearity and alternating properties: For notational simplicity say <span class="process-math">\(A = (v_1, v_2, \dots)\)</span> and <span class="process-math">\(B = (v_1 + rv_2, v_2, \dots)\text{.}\)</span> Then</div>
<div class="displaymath process-math">
\begin{equation*}
\det(B) = \det(v_1, v_2, \dots) + r \det(v_2, v_2, \dots) = \det(A) + r \cdot 0 = \det(A)
\end{equation*}
</div>
<div class="para">The second is immediate from (the second part of) <span class="process-math">\(R\)</span>-multi-linearity. The last is a special case of Lemma .</div>
</div></section></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\rem\)</span>.</span></h5> <div class="para">The previous Lemma gives an efficient method of computing the determinant of a square matrix: Apply Gaussian reduction to transform such a matrix <span class="process-math">\(A\)</span> to <span class="process-math">\(I_n\text{,}\)</span> keeping track of the operations used. Since <span class="process-math">\(\det(I_n) = 1\text{,}\)</span> we can deduce what the <span class="process-math">\(\det(A)\)</span> is in this way, using the Lemma.</div> <div class="para logical">
<div class="para">The number of type I (and type II) operations needed to transform <span class="process-math">\(A\)</span> to <span class="process-math">\(I_n\)</span> is at most about</div>
<div class="displaymath process-math">
\begin{equation*}
2(n-1) + 2(n-2) + \cdots \approx n^2
\end{equation*}
</div>
<div class="para">Each operation involves at most at most <span class="process-math">\(n-1\)</span> multiplications and additions. All total, the number of operations needed to compute the determinant of an <span class="process-math">\(n \times n\)</span> matrix is a polynomial function of <span class="process-math">\(n\)</span> (I think it is cubic, in fact). Observe that the determinant formula involves <span class="process-math">\(n!\)</span> terms each with <span class="process-math">\(n\)</span> products, which is certainly exponential in <span class="process-math">\(n\text{.}\)</span>
</div>
</div> <div class="para">Thus, Gaussian elimination, among other things, gives a polynomial time solution to a computation that at first blush involves an exponential number of calculations.</div> <div class="para">The “permanant’’ of a square matrix is given by the same formula but without the signs. It too involves an exponential (in <span class="process-math">\(n\)</span>) number of calculations. Unlike the determinant, there is no known polynomial time algorithm to compute it. Indeed, if one exists then <span class="process-math">\(P = NP\text{!}\)</span> This is a Theorem of Leslie Valiant.</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\cor\)</span> – Det Splits Across Elementary Multiplication.</span></h5> <div class="para logical">
<div class="para">For any commutative ring <span class="process-math">\(R\text{,}\)</span> if <span class="process-math">\(A\)</span> is an <span class="process-math">\(n \times n\)</span> matrix and <span class="process-math">\(E\)</span> is an <span class="process-math">\(n \times n\)</span> elementary matrix, then[^1]</div>
<div class="displaymath process-math">
\begin{equation*}
\det(AE) = \det(A) \det(E)
\end{equation*}
</div>
</div> <section class="paragraphs"><h6 class="heading"><span class="title"><em class="emphasis">Proof.</em>.</span></h6> <div class="para logical">
<div class="para">This follows from the Proposition and the fact that <span class="process-math">\(AE\)</span> is the result of doing the corresponding column operation on <span class="process-math">\(A\)</span> (see Lemma ), since</div>
<div class="displaymath process-math">
\begin{equation*}
\det(E)  = \begin{cases}
1 &amp; \text{if $E$ is elementary of type I} \\
u &amp; \text{if $E$ is elementary of type II with a unit $u$ on the diagonal} \\
-1 &amp; \text{if $E$ is elementary of type III.} \\
\end{cases}
\end{equation*}
</div>
<div class="para">These formulas follow from the formula for the determinant of an upper or lower triangular matrix — see Example .</div>
</div></section></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\cor\)</span> – Det not <span class="process-math">\(0\)</span> iff Matrix Invertible.</span></h5> <div class="para">For <span class="process-math">\(R = F\)</span> a field, we have <span class="process-math">\(\det(A) \ne 0\)</span>[^1] if and only if <span class="process-math">\(A\)</span> is invertible.</div> <section class="paragraphs"><h6 class="heading"><span class="title"><em class="emphasis">Proof.</em>.</span></h6> <div class="para logical">
<div class="para">If <span class="process-math">\(A\)</span> is not invertible, then the column space of <span class="process-math">\(A\)</span> is a proper subspace of <span class="process-math">\(F^n\)</span> and hence the columns of <span class="process-math">\(A\)</span> must be linearly dependent. Say the <span class="process-math">\(i\th\)</span> column is a linear combination of the rest: <span class="process-math">\(v_i = \sum_{j \ne i} c_j v_j\text{.}\)</span> Then</div>
<div class="displaymath process-math">
\begin{equation*}
\det(v_1, \dots, v_n) = \sum_{j \ne i} c_j \det(\text{a matrix with the $i$-th and $j$-th columns equal}) = 0.
\end{equation*}
</div>
<div class="para">If <span class="process-math">\(A\)</span> is invertible, then by Corollary  <span class="process-math">\(A\)</span> can be obtained from <span class="process-math">\(I_n\)</span> via a sequence of elementary column operations. The result thus follows from Proposition  and the fact that <span class="process-math">\(\det(I_n) = 1\text{.}\)</span>
</div>
</div></section></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\rem\)</span>.</span></h5> <div class="para">For an arbitrary commutative ring <span class="process-math">\(R\text{,}\)</span> the rule is: a square matrix <span class="process-math">\(A\)</span> is invertible if and only if <span class="process-math">\(\det(A)\)</span> is a unit of <span class="process-math">\(R\text{.}\)</span> We’ll give a proof later.</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\thm\)</span> – Det Splits Across Multiplication: Fields.</span></h5> <div class="para logical">
<div class="para">For a field <span class="process-math">\(F\)</span> and matrices <span class="process-math">\(A, B \in \Mat_{n \times n}(F)\)</span> we have[^1]</div>
<div class="displaymath process-math">
\begin{equation*}
\det(AB) = \det(A)\det(B).
\end{equation*}
</div>
<div class="para">###### <em class="emphasis">Proof.</em> If <span class="process-math">\(A\)</span> is not invertible, neither is <span class="process-math">\(AB\text{,}\)</span> since <span class="process-math">\(\im(AB) \subseteq \im(A)\text{,}\)</span> and if <span class="process-math">\(B\)</span> is not invertible, neither is is <span class="process-math">\(AB\text{,}\)</span> since <span class="process-math">\(\ker(AB) \supseteq \ker(A)\text{.}\)</span> So, by Corollary , if either <span class="process-math">\(A\)</span> or <span class="process-math">\(B\)</span> is not invertible, both sides of the equation are <span class="process-math">\(0\text{.}\)</span>
</div>
</div> <div class="para logical">
<div class="para">Assume now that <span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> are both invertible. Then by Corollary  we have</div>
<div class="displaymath process-math">
\begin{equation*}
A = E_1 \cdots E_n
\end{equation*}
</div>
<div class="para">and</div>
<div class="displaymath process-math">
\begin{equation*}
B = F_1 \cdots F_m
\end{equation*}
</div>
<div class="para">and hence</div>
<div class="displaymath process-math">
\begin{equation*}
AB = E_1 \cdots E_n F_1 \cdots F_m
\end{equation*}
</div>
<div class="para">where the <span class="process-math">\(E_i\)</span>’s and <span class="process-math">\(F_j\)</span>’s are elementary matrices. Applying Corollary  repeatedly gives</div>
<div class="displaymath process-math">
\begin{equation*}
\det(AB) = \det(E_1 \cdots E_n F_1 \cdots F_{m-1}) \det(F_m) = \cdots= \det(E_1) \cdots \det(E_n) \det(F_1) \cdots \det(F_m)
\end{equation*}
</div>
<div class="para">and similarly</div>
<div class="displaymath process-math">
\begin{equation*}
\det(A) \det(B) = \left(\det(E_1) \cdots \det(E_n)\right) \left( \det(F_1) \cdots \det(F_m)\right).
\end{equation*}
</div>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\lem\)</span> – Det Function is Natural.</span></h5> <div class="para logical">
<div class="para">The determinant function <span class="process-math">\(\det\)</span> is “natural’’: If <span class="process-math">\(g: R \to S\)</span> is a homomorphism of non-zero commutative rings and <span class="process-math">\(A \in \Mat_{n \times n}(R)\text{,}\)</span> then</div>
<div class="displaymath process-math">
\begin{equation*}
g \det(A) = \det(g(A))
\end{equation*}
</div>
<div class="para">where <span class="process-math">\(g(A)\)</span> denotes the matrix obtained by applying <span class="process-math">\(g\)</span> to the entries of <span class="process-math">\(A\text{.}\)</span>
</div>
</div> <section class="paragraphs"><h6 class="heading"><span class="title"><em class="emphasis">Proof.</em>.</span></h6> <div class="para">This follows from the formula for <span class="process-math">\(\det\text{,}\)</span> since <span class="process-math">\(g\)</span> commutes with sums and products.</div></section></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\thm\)</span> – Det Splits Across Multiplication: Commutative Ring.</span></h5> <div class="para logical">
<div class="para">For any non-zero commutative ring <span class="process-math">\(R\)</span> and matrices <span class="process-math">\(A, B \in \Mat_{n \times n}(R)\text{,}\)</span> we have[^1]</div>
<div class="displaymath process-math">
\begin{equation*}
\det(AB) = \det(A) \det(B) \in R. 
\end{equation*}
</div>
</div> <section class="paragraphs"><h6 class="heading"><span class="title"><em class="emphasis">Proof.</em>.</span></h6> <div class="para">We have already proven that this holds when <span class="process-math">\(R\)</span> is a field.</div> <div class="para">We next show that it holds whenever <span class="process-math">\(R\)</span> is an integral domain. In this case, <span class="process-math">\(R\)</span> is a subring of a field <span class="process-math">\(F\)</span> (namely, the field of fractions of <span class="process-math">\(R\)</span>). So, we know that the equation <span class="process-math">\(\det(AB) = \det(A) \det(B)\)</span> holds in <span class="process-math">\(F\)</span> if we interpret <span class="process-math">\(A\)</span> and <span class="process-math">\(B\)</span> as belonging to <span class="process-math">\(\Mat_{n \times n}(F)\text{.}\)</span> But the value of <span class="process-math">\(\det\)</span> is the same if we interpret these matrices as having entries in <span class="process-math">\(R\)</span> or in <span class="process-math">\(F\text{.}\)</span> So <span class="process-math">\(\det(AB) = \det(A) \det(B)\)</span> holds in <span class="process-math">\(R\text{.}\)</span>
</div> <div class="para logical">
<div class="para">We finally prove that the Theorem for any non-zero commutative ring <span class="process-math">\(R\)</span> by building on the fact that it holds for domains. We do so by contructing a ring homomorphism <span class="process-math">\(g: S \to R\)</span> and matrices <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> in <span class="process-math">\(\Mat_{n \times n}(S)\)</span> such that <span class="process-math">\(S\)</span> is an integral domain, <span class="process-math">\(g(X) = A\)</span> and <span class="process-math">\(g(Y) = B\text{.}\)</span> Granting such a <span class="process-math">\(S, g, X\)</span> and <span class="process-math">\(Y\)</span> exist, the result follows from the naturality of <span class="process-math">\(\det\)</span> (Lemma ). In detail, we know <span class="process-math">\(\det(XY) = \det(X) \det(Y)\text{,}\)</span> since <span class="process-math">\(S\)</span> is a domain. Since the rule for multiplying matrices involves only sums and products of ring elements, we have <span class="process-math">\(g(XY) = g(X)g(Y)\text{.}\)</span> So</div>
<div class="displaymath process-math">
\begin{equation*}
\begin{aligned}
\det(AB) &amp; = \det(g(X) g(Y)) 
= \det(g(XY))  = g \det(XY) \\ 
&amp; = g(\det(X) \det(Y)) = g(\det(X)) g(\det(Y)) = \det(g(X)) \det(g(Y)) = \det(A) \det(B).
\end{aligned}
\end{equation*}
</div>
<div class="para">It remains to prove such a <span class="process-math">\(S\text{,}\)</span> <span class="process-math">\(g\text{,}\)</span> <span class="process-math">\(X\)</span> and <span class="process-math">\(Y\)</span> exists. Suppose <span class="process-math">\(A = (a_{i,j})\)</span> and <span class="process-math">\(B = (b_{i,j})\text{.}\)</span> Form the polynomial ring</div>
<div class="displaymath process-math">
\begin{equation*}
S := \Z[\{x_{i,j} \mid 1 \leq i \leq n, 1 \leq j \leq n\} \cup \{y_{i,j}\mid 1 \leq i \leq n, 1 \leq j \leq n\}]
\end{equation*}
</div>
<div class="para">of <span class="process-math">\(2 n^2\)</span> variables with <span class="process-math">\(\Z\)</span>-coefficients. By the UMP for polynomial rings with integer coefficients, since <span class="process-math">\(R\)</span> is commutative, there is a (unique) ring map <span class="process-math">\(g: S \to R\)</span> such that <span class="process-math">\(g(x_{i,j}) = a_{i,j}\)</span> and <span class="process-math">\(g(y_{i,j}) = b_{i,j}\)</span> for all <span class="process-math">\(i,j\text{.}\)</span> That is, <span class="process-math">\(g\)</span> is the evaluation map given by setting <span class="process-math">\(x_{i,j} = a_{i,j}\)</span> and <span class="process-math">\(y_{i,j} = b_{i,j}\)</span> for all <span class="process-math">\(i,j\text{,}\)</span> and interpreting integers as elements of <span class="process-math">\(R\text{.}\)</span> Let</div>
<div class="displaymath process-math">
\begin{equation*}
X = (x_{i,j}), Y = (y_{i,j})  \in \Mat_{n \times n}(S)
\end{equation*}
</div>
<div class="para">be the evident matrices of indeterminants.Then <span class="process-math">\(S\)</span> is a domain, <span class="process-math">\(g: S \to R\)</span> is a ring homomorphism, <span class="process-math">\(g(X) = A\)</span> and <span class="process-math">\(g(Y) = B\text{,}\)</span> as desired.</div>
</div></section></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\rem\)</span>.</span></h5> <div class="para">Another way to deduce the Theorem for arbitrary commutative rings from the case of a domain is to use the following fact: If <span class="process-math">\(R\)</span> is a non-zero commutative ring, there there exists a surjective ring homomorphism of the form <span class="process-math">\(g: S \onto R\)</span> where <span class="process-math">\(S\)</span> is a domain. So see this, let <span class="process-math">\(X\)</span> be a (possibly very large) set of indeterminants such that there is a bijection of sets <span class="process-math">\(\rho: X \to R\text{.}\)</span> Let <span class="process-math">\(S = \Z[X]\text{,}\)</span> the polynomial ring with integer coefficients in the variables <span class="process-math">\(X\text{.}\)</span> So, a typical element of <span class="process-math">\(S\)</span> is a polynomial the form <span class="process-math">\(f(x_1, \dots,x_n)\)</span> for some finite subset <span class="process-math">\(\{x_1, \dots, x_n\}\)</span> of <span class="process-math">\(X\text{.}\)</span> By the UMP for polynomial rings, there is a unique ring map <span class="process-math">\(g: S \to R\)</span> such that <span class="process-math">\(g(x) = \rho(x)\)</span> for all <span class="process-math">\(x \in X\text{.}\)</span> That is, <span class="process-math">\(g\)</span> sends <span class="process-math">\(f(x_1, \dots, x_n)\)</span> as above to <span class="process-math">\(f(\rho(x_1), \dots, \rho(x_n))\text{.}\)</span> The ring map <span class="process-math">\(g\)</span> is clearly onto since for each <span class="process-math">\(r \in R\text{,}\)</span> there is an <span class="process-math">\(x \in X\)</span> with <span class="process-math">\(\rho(x) = r\)</span> and hence <span class="process-math">\(g(x) = r\text{.}\)</span> Finally, <span class="process-math">\(S\)</span> is an integral domain.</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\rem\)</span>.</span></h5> <div class="para logical">
<div class="para">The key idea in the proof of the Theorem can be used to deduce many other identities over commutative rings from known identities over fields. E.g., it can be used to show that</div>
<div class="displaymath process-math">
\begin{equation*}
A A^{adj} = \det(A) I_n = A^{adj} A
\end{equation*}
</div>
<div class="para">holds for any commutative ring <span class="process-math">\(R\)</span> and for any <span class="process-math">\(A \in \Mat_{n \times n}(R)\text{,}\)</span> where <span class="process-math">\(A^{adj}\)</span> is the “adjugate’’ of <span class="process-math">\(A\text{,}\)</span> granting that this equation holds when <span class="process-math">\(R\)</span> is a field. Recall <span class="process-math">\(A^{adj}\)</span> is the <span class="process-math">\(n \times n\)</span> matrix whose <span class="process-math">\((i,j)\)</span> entry is <span class="process-math">\((-1)^{i+j} \det(A_{i,j})\)</span> where <span class="process-math">\(A_{i,j}\)</span> is the <span class="process-math">\((n-1) \times (n-1)\)</span> matrix obtained by deleting the <span class="process-math">\(i\)</span>-th column and <span class="process-math">\(j\)</span>-th row of <span class="process-math">\(A\text{.}\)</span> I will likely have you do this as an exercise.</div>
</div> <div class="para">Granting <span class="process-math">\(A A^{adj} = \det(A) I_n = A^{adj} A\)</span> holds for any non-zero commutative ring <span class="process-math">\(R,\)</span> we can prove the following:</div></section> <section class="paragraphs"><h5 class="heading"><span class="title">Problem – Determinantal Technique.</span></h5> <div class="para">Let <span class="process-math">\(R\)</span> be a non-zero commutative ring and <span class="process-math">\(A \in \Mat_{n \times n}(R)\text{.}\)</span> Let <span class="process-math">\(A^{adj}\)</span> denote the classical adjugate of <span class="process-math">\(A\)</span> — the <span class="process-math">\((i,j)\)</span> entry of <span class="process-math">\(A^{adj}\)</span> is defined to be <span class="process-math">\((-1)^{i+j} \det(A_{j,i})\)</span> where <span class="process-math">\(A_{j,i}\)</span> is <span class="process-math">\((n-1) \times (n-1)\)</span> matrix obtained by deleting the <span class="process-math">\(j\)</span>-th row and <span class="process-math">\(i\)</span>-th column of <span class="process-math">\(A\text{.}\)</span> You may assume without proof that when <span class="process-math">\(R\)</span> is a field, we have <span class="process-math">\(A A^{adj} = \det(A) I_n = A^{adj} A\text{.}\)</span> Prove <span class="process-math">\(A A^{adj} = \det(A) I_n = A^{adj} A\)</span> holds for any non-zero commutative ring <span class="process-math">\(R\text{.}\)</span>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\prop\)</span> – Matrix Invertible iff Det is a Unit.</span></h5> <div class="para">For any non-zero commutative ring <span class="process-math">\(R\)</span> and any <span class="process-math">\(A \in \Mat_{n \times n}(R)\text{,}\)</span> <span class="process-math">\(A\)</span> is invertible if and only if <span class="process-math">\(\det(A)\)</span>[^1] is a unit of <span class="process-math">\(R\text{.}\)</span>
</div> <section class="paragraphs"><h6 class="heading"><span class="title"><em class="emphasis">Proof.</em>.</span></h6> <div class="para">If <span class="process-math">\(A\)</span> is invertible then <span class="process-math">\(1 = \det(I_n) = \det(AA^{-1}) = \det(A) \det(A^{-1})\)</span> by the Theorem, and hence <span class="process-math">\(\det(A)\)</span> is a unit.</div> <div class="para logical">
<div class="para">If <span class="process-math">\(\det(A)\)</span> is a unit, then, using the equation above, we have</div>
<div class="displaymath process-math">
\begin{equation*}
A (A^{adj} \cdot \frac{1}{\det(A)}) =I_n = (A^{adj} \cdot \frac{1}{\det(A)}) A
\end{equation*}
</div>
<div class="para">which proves <span class="process-math">\(A\)</span> has a two-sided inverse (namely, <span class="process-math">\(A^{adj} \frac{1}{\det(A)}\)</span>).</div>
</div></section></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\defn\)</span> – Determinant.</span></h5> <div class="para logical">
<div class="para">Let <span class="process-math">\(R\)</span> be a commutative ring and <span class="process-math">\(V\)</span> a free <span class="process-math">\(R\)</span>-module of finite rank. Given an endomorphism <span class="process-math">\(\a: V \to V\text{,}\)</span> we define its <em class="emphasis"><span class="process-math">\(\defnn{determinant}\)</span></em> to be[^1]</div>
<div class="displaymath process-math">
\begin{equation*}
\det(\a) = \det([\a]_B^B)
\end{equation*}
</div>
<div class="para">where <span class="process-math">\(B\)</span> is any basis of <span class="process-math">\(V\text{.}\)</span>
</div>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\lem\)</span> – Det Independent of Basis Choice.</span></h5> <div class="para">The definition of <span class="process-math">\(\det(\a)\)</span> is independent of choice of <span class="process-math">\(B\text{.}\)</span>
</div> <section class="paragraphs"><h6 class="heading"><span class="title"><em class="emphasis">Proof.</em>.</span></h6> <div class="para logical">
<div class="para">If <span class="process-math">\(C\)</span> is another basis, then</div>
<div class="displaymath process-math">
\begin{equation*}
[\a]_C^C = P [\a]_B^B P^{-1}
\end{equation*}
</div>
<div class="para">for an invertible matrix <span class="process-math">\(P\)</span> (specifically, <span class="process-math">\(P = [\id]_B^C\)</span>). Then</div>
<div class="displaymath process-math">
\begin{equation*}
\det([\a]_C^C)  = \det(P [\a]_B^B P^{-1}) = \det(P) \det([\a]_B^B) \det(P^{-1}) =\det(P) \det(P^{-1}) \det([\a]_B^B) 
\end{equation*}
</div>
<div class="para">using the Theorem. Also by the Theorem <span class="process-math">\(\det(P) \det(P^{-1}) = \det(PP^{-1}) = \det(I_n) = 1\text{.}\)</span>
</div>
</div></section></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\exe\)</span> – Companion Matrix.</span></h5> <div class="para logical">
<div class="para">Let <span class="process-math">\(V = F[x]/I\)</span> where <span class="process-math">\(F\)</span> is a field, <span class="process-math">\(I = F[x] \cdot f(x)\)</span> with <span class="process-math">\(f\)</span> a monic polynomial. Say <span class="process-math">\(f(x) = x^n + a_{n-1} x^{n-1} + \cdots + a_1 x + a_0\text{.}\)</span>Recall that every element of <span class="process-math">\(V\)</span> is uniquely represented by a coset of the form</div>
<div class="displaymath process-math">
\begin{equation*}
\ov{p(x)} := p(x) + I
\end{equation*}
</div>
<div class="para">where <span class="process-math">\(p(x)\)</span> is a polynomial of degree at most <span class="process-math">\(n-1\text{.}\)</span>
</div>
</div> <div class="para">We will regard <span class="process-math">\(V\)</span> as an <span class="process-math">\(F\)</span>-vector space (via restriction of scalars along <span class="process-math">\(F \into F[x]\)</span>). Then <span class="process-math">\(V\)</span> is finite dimensional — for instance, a basis of <span class="process-math">\(V\)</span> is given by <span class="process-math">\(\ov{1}, \ov{x}, \dots, \ov{x^{n-1}}\text{.}\)</span>
</div> <div class="para logical">
<div class="para">Let <span class="process-math">\(g: V \to V\)</span> be the function given as multiplication by <span class="process-math">\(x\text{.}\)</span> Then <span class="process-math">\(g\)</span> is an <span class="process-math">\(F\)</span>-linear operator, since <span class="process-math">\(g(v+v') = x(v+v') = xv + xv' = g(v) + g(v')\)</span> and <span class="process-math">\(g(cv) = x(cv) = c (xv) = c g(v)\text{.}\)</span> (In fact, <span class="process-math">\(g\)</span> is <span class="process-math">\(F[x]\)</span>-linear, but we won’t use that fact.) Relative to the ordered basis <span class="process-math">\(\ov{1}, \ov{x}, \dots, \ov{x^{n-1}}\text{,}\)</span> the matrix of <span class="process-math">\(g\)</span> is</div>
<div class="displaymath process-math">
\begin{equation*}
C(f) :=
\begin{bmatrix}
0 &amp; 0 &amp; \cdots &amp; 0 &amp; -a_0 \\
1 &amp; 0 &amp; \cdots &amp; 0 &amp; -a_1 \\
0 &amp; 1 &amp; \cdots &amp; 0 &amp; -a_2 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 1 &amp; -a_{n-1} \\   
\end{bmatrix}
\end{equation*}
</div>
<div class="para">The right-most column is due to the fact that, since <span class="process-math">\(\overline{f(x)} = 0\)</span> in <span class="process-math">\(V\text{,}\)</span> we have</div>
<div class="displaymath process-math">
\begin{equation*}
x \cdot \overline{x^{n-1}} =
\overline{x^n} = -a_{n-1} \overline{x^{n-1}} -a_{n-2} \overline{x^{n-2}} - \cdots - a_1 \overline{x} - a_0 \overline{1}.
\end{equation*}
</div>
<div class="para">The matrix <span class="process-math">\(C(f)\)</span> is known as the <span class="process-math">\(\defnn{companion matrix}\)</span> of <span class="process-math">\(f(x)\)</span>} – it is defined for any monic polynomials with entries in a field.</div>
</div> <div class="para">We have <span class="process-math">\(\det(C(f)) = (-1)^n a_0\text{,}\)</span> since the only permutation that gives a non-zero term in the formula for <span class="process-math">\(\det\)</span> is the <span class="process-math">\(n\)</span>-cycle <span class="process-math">\(\s = (2 \, 3 \, \dots \, n \, 1)\text{,}\)</span> and its has sign is <span class="process-math">\((-1)^{n-1}\text{.}\)</span> So <span class="process-math">\(\det(g) = (-1)^n a_0\text{.}\)</span>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\rem\)</span>.</span></h5> <div class="para">As we showed above, the determinant of linear operator on a free module of finite rank is an “invariant’’ of the operator, in the sense that it is independent of how we represent the operator as a matrix. Here is another such invariant:</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\defn\)</span> – Trace.</span></h5> <div class="para">For a commutative ring <span class="process-math">\(R\)</span> and a square matrix <span class="process-math">\(A \in \Mat_{n \times n}(R)\text{,}\)</span> the <em class="emphasis"><span class="process-math">\(\defnn{trace}\)</span></em> of <span class="process-math">\(A\text{,}\)</span> written <span class="process-math">\(\trace(A)\text{,}\)</span> is the sum of its diagonal entries: <span class="process-math">\(\trace(A) = \sum_i a_{i,i}\text{.}\)</span>
</div> <div class="para">If <span class="process-math">\(V\)</span> is a free <span class="process-math">\(R\)</span>-module of finite rank and <span class="process-math">\(g: V \to V\)</span> is an <span class="process-math">\(R\)</span>-module homomorphism, we define <span class="process-math">\(\trace(g)\)</span> to be <span class="process-math">\(\trace([g]^B_B)\)</span>[^1] for any basis <span class="process-math">\(B\text{.}\)</span>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\lem\)</span> – Trace is Commutative with Two Matrices.</span></h5> <div class="para">If <span class="process-math">\(R\)</span> is a commutative ring and <span class="process-math">\(X \in \Mat_{m \times n}(R)\)</span> and <span class="process-math">\(Y \in \Mat_{n \times m}(R)\text{,}\)</span> then <span class="process-math">\(\trace(XY) = \trace(YX)\)</span>[^1].</div> <div class="para">In particular, with the notation of the previous definition, <span class="process-math">\(\trace(g)\)</span> is well-defined.</div> <section class="paragraphs"><h6 class="heading"><span class="title"><em class="emphasis">Proof.</em>.</span></h6> <div class="para">The <span class="process-math">\((i,i)\)</span> entry of <span class="process-math">\(XY\)</span> is <span class="process-math">\(\sum_l x_{i,l} y_{l, i}\)</span> and so <span class="process-math">\(\trace(XY) = \sum_i \sum_l x_{i,l} y_{l,i}\text{.}\)</span> The <span class="process-math">\((j,j)\)</span> entry of <span class="process-math">\(YX\)</span> is <span class="process-math">\(\sum_t y_{j,t} x_{t,j}\)</span> and so <span class="process-math">\(\trace(YX) = \sum_j \sum_t y_{j,t} x_{t,j}\text{.}\)</span> Since <span class="process-math">\(R\)</span> is commutative, these are equal.</div> <div class="para logical">
<div class="para">Say <span class="process-math">\(B\)</span> and <span class="process-math">\(C\)</span> are two bases of <span class="process-math">\(V\text{.}\)</span> Then <span class="process-math">\([g]_B^B = P [g]_{C}^{C} P^{-1}\)</span> for some invertible matrix <span class="process-math">\(P\)</span> (namely, <span class="process-math">\(P = [\id_V]_C^B\)</span>). Using the first part of the Lemma we have</div>
<div class="displaymath process-math">
\begin{equation*}
\trace([g]_B^B) = \trace(P [g]_{C}^{C} P^{-1}) = \trace((P [g]_{C}^{C}) P^{-1}) = 
\trace(P^{-1} (P [g]_{C}^{C})) =
\trace((P^{-1} P) [g]_{C}^{C})) =
\trace([g]_{C}^{C}).
\end{equation*}
</div>
</div></section></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\war\)</span>.</span></h5> <div class="para">The Lemma tells us that the trace of a product of two matrices in either order is the same, but it does not follow that the trace is unchanged if you permute arbitrarily a product of three of more matrices. For instance, <span class="process-math">\(\trace(XYZ)\)</span> is usually not equal to <span class="process-math">\(\trace(XZY)\text{.}\)</span>
</div> <div class="para logical">
<div class="para">It is true that trace is invariant under all “cyclic permutations’’ of products of matrices. For instance, <span class="process-math">\(\trace(XYZ) = \trace(ZXY) = \trace(YZX)\)</span> and more generally we have</div>
<div class="displaymath process-math">
\begin{equation*}
\trace(X_1 \cdots X_n) = \trace(X_i X_{i+1} \cdots X_n X_1 X_2 \cdots X_{i-1})
\end{equation*}
</div>
<div class="para">for all <span class="process-math">\(i\text{.}\)</span>
</div>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\exe\)</span> – Trace and Multiplication by <span class="process-math">\(x\)</span>.</span></h5> <div class="para">With the notation of Example , <span class="process-math">\(\trace(g) = -a_{n-1}\text{.}\)</span>[^1]</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\exe\)</span> – Det and Trace of <span class="process-math">\(\C\)</span> and Linear Operator.</span></h5> <div class="para">Let <span class="process-math">\(\C\)</span> denote the complex numbers. We may regard <span class="process-math">\(\C\)</span> as an <span class="process-math">\(\R\)</span>-vector space. Given <span class="process-math">\(z = a + bi \in \C\text{,}\)</span> multiplication by <span class="process-math">\(z\)</span> on <span class="process-math">\(\C\)</span> is an <span class="process-math">\(\R\)</span>-linear operator. What are the determinant and trace of this operator?</div> <div class="para logical">
<div class="para">Relative to the basis <span class="process-math">\(1, i\)</span> of <span class="process-math">\(\C\)</span> as a real vector space, multiplication by <span class="process-math">\(z = a = ib\)</span> is represented by the matrix</div>
<div class="displaymath process-math">
\begin{equation*}
\begin{bmatrix}
a &amp; -b \\ 
b &amp; a   \\
\end{bmatrix}
\end{equation*}
</div>
<div class="para">and hence the determinant of this operator is <span class="process-math">\(a^2 + b^2\text{,}\)</span> the square of the norm of <span class="process-math">\(z\text{.}\)</span> The trace is <span class="process-math">\(2a\text{.}\)</span> Note that the determinant of this operator is <span class="process-math">\(\|z\|^2\)</span> the square of the usual complex norm.</div>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\exe\)</span> – Matrix of Field Extension.</span></h5> <div class="para logical">
<div class="para">Similarly, we may consider the field extension <span class="process-math">\(\Q \subseteq \Q(\sqrt{-2})\text{.}\)</span> For any <span class="process-math">\(a + b \sqrt{-2}\)</span> in the larger field, define <span class="process-math">\(g: \Q(\sqrt{-2}) \to \Q(\sqrt{-2})\)</span> to be multiplication by <span class="process-math">\(a + b \sqrt{-2}\text{.}\)</span> Then <span class="process-math">\(\Q(\sqrt{-2})\)</span> is a <span class="process-math">\(\Q\)</span>-vector space and <span class="process-math">\(g\)</span> is a <span class="process-math">\(\Q\)</span>-linear operator. A <span class="process-math">\(\Q\)</span>-basis of <span class="process-math">\(\Q(\sqrt{-2}\)</span> is given by <span class="process-math">\(B = \{1, \sqrt{-2}\}\)</span> and relative to this basis the matrix representing <span class="process-math">\(g\)</span> is</div>
<div class="displaymath process-math">
\begin{equation*}
[g]_B^B = 
\begin{bmatrix}
a &amp; -2b \\ 
b &amp; a   \\
\end{bmatrix}
\end{equation*}
</div>
<div class="para">and hence <span class="process-math">\(\det(g) = a^2 + 2b^2\text{,}\)</span> once again the square of the of usual complex norm of the element <span class="process-math">\(a + b \sqrt{-2}\text{.}\)</span>
</div>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\defn\)</span> – Characteristic Polynomial.</span></h5> <div class="para logical">
<div class="para">Let <span class="process-math">\(A \in \Mat_{n \times n}(F)\)</span> where <span class="process-math">\(F\)</span> is a field. The <em class="emphasis"><span class="process-math">\(\defnn{characteristic polynomial}\)</span></em> of <span class="process-math">\(A\)</span> is[^1]</div>
<div class="displaymath process-math">
\begin{equation*}
\cp_A(x) := \det(x I_n - A) \in F[x].
\end{equation*}
</div>
<div class="para">Note that <span class="process-math">\(\cp_A(x)\)</span> is a monic polynomial of degree <span class="process-math">\(n\)</span> with coefficients in <span class="process-math">\(F\text{.}\)</span> More generally, if <span class="process-math">\(V\)</span> is a finite dimensional <span class="process-math">\(F\)</span>-vector space and <span class="process-math">\(g: V \to V\)</span> is an <span class="process-math">\(F\)</span>-linear operator on <span class="process-math">\(V\text{,}\)</span> then</div>
<div class="displaymath process-math">
\begin{equation*}
\cp_g(x) := \cp_A(x)
\end{equation*}
</div>
<div class="para">where <span class="process-math">\(A\)</span> is the matrix representing <span class="process-math">\(g\)</span> with respect to a choice of basis of <span class="process-math">\(V\text{.}\)</span>
</div>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\rem\)</span>.</span></h5> <div class="para logical">
<div class="para">We should pause to check that <span class="process-math">\(\cp_g(x)\)</span> is well-defined: Say <span class="process-math">\(B\)</span> and <span class="process-math">\(C\)</span> are two ordered bases of <span class="process-math">\(V\text{,}\)</span> so that <span class="process-math">\([g]_B^B\)</span> and <span class="process-math">\([g]_C^C\)</span> both represent <span class="process-math">\(g\text{.}\)</span> Then <span class="process-math">\([g]_C^C = P [g]_B^B P^{-1}\)</span> for some invertible matrix <span class="process-math">\(P\)</span> and hence</div>
<div class="displaymath process-math">
\begin{equation*}
P(xI_n - [g]_B^B)P^{-1} = PxI_nP^{-1} - P [g]_B^B P^{-1} = xI_n - [g]_C^C,
\end{equation*}
</div>
<div class="para">since the scalar matrix <span class="process-math">\(xI_n\)</span> commutes with all matrices. That is, <span class="process-math">\(xI_n - [g]_B^B\)</span> and <span class="process-math">\(xI_n - [g]_C^C\)</span> are similar matrices (with entries in <span class="process-math">\(F[x]\)</span>), and it follows that <span class="process-math">\(\det(xI_n - [g]_B^B) = \det(xI_n - [g]_C^C)\text{.}\)</span>
</div>
</div> <div class="para">The following two exercises show that the characteristic polynomial captures both the determinant and the trace of an operator:</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\exe\)</span> – <span class="process-math">\(\cp(0)\)</span>.</span></h5> <div class="para">Show <span class="process-math">\(\cp_g(0) = (-1)^n \det(g)\)</span>[^1].</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\exe\)</span> – CharPoly and Trace.</span></h5> <div class="para">Show that if <span class="process-math">\(\dim_F(V) = n\)</span>[^1], then the coefficient of <span class="process-math">\(x^{n-1}\)</span> in <span class="process-math">\(\cp_g(x)\)</span>[^2] is <span class="process-math">\(-\trace(g)\)</span>[^3].</div></section> <section class="paragraphs"><h5 class="heading"><span class="title"><span class="process-math">\(\exe\)</span> – Triangular Matrix and CharPoly.</span></h5> <div class="para">If <span class="process-math">\(A\)</span> is upper or lower triangular, then <span class="process-math">\(\cp_A(x) = \prod_i (x - a_{i,i})\)</span>[^1]. As you may recall from an undergraduate class, in this case <span class="process-math">\(a_{1,1}, \dots, a_{n,n}\)</span> are the eigenvalues of <span class="process-math">\(A\text{.}\)</span> More on this later.</div></section> <section class="paragraphs"><h5 class="heading"><span class="title">Problem.</span></h5> <div class="para logical">
<div class="para">Let <span class="process-math">\(V\)</span> be the <span class="process-math">\(\R\)</span>-vector space consisting of polynomials in the variable <span class="process-math">\(x\)</span> of degree at most <span class="process-math">\(3\)</span> and let <span class="process-math">\(g: V \to V\)</span> be the <span class="process-math">\(\R\)</span>-linear operator given by</div>
<div class="displaymath process-math">
\begin{equation*}
g(p(x)) = p''(x) + (x+1) p'(x) + 7 p(x),
\end{equation*}
</div>
<div class="para">where <span class="process-math">\(p''(x)\)</span> and <span class="process-math">\(p'(x)\)</span> denote the first and second derivatives of <span class="process-math">\(p(x)\text{.}\)</span> (You may take it on faith that <span class="process-math">\(g\)</span> is <span class="process-math">\(\R\)</span>-linear.) Find the determinant, the trace, and the characteristic polynomial of <span class="process-math">\(g\text{.}\)</span>
</div>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title">Problem.</span></h5> <div class="para">Let <span class="process-math">\(F\)</span> be a field and <span class="process-math">\(V = F[x]/I\)</span> with <span class="process-math">\(I = F[x] \cdot f(x)\)</span> for some monic polynomial <span class="process-math">\(f(x) = x^n + a_{n-1} x^{n-1} + \cdots + a_1 x + a_0\text{.}\)</span> Regard <span class="process-math">\(V\)</span> as an <span class="process-math">\(F\)</span>-vector space (via restriction of scalars along <span class="process-math">\(F \into F[x]\)</span>), and recall that the function <span class="process-math">\(g: V \to V\)</span> given by <span class="process-math">\(g(v) = x \cdot v\)</span> is an <span class="process-math">\(F\)</span>-linear operator on <span class="process-math">\(V\text{.}\)</span>
</div> <div class="para">Prove that the characteristic polynomial of <span class="process-math">\(g\)</span> is <span class="process-math">\(f(x)\text{.}\)</span>
</div></section> <section class="paragraphs"><h5 class="heading"><span class="title">Problem 8 #unfinished.</span></h5> <div class="para">The trace of a square matrix <span class="process-math">\(A\text{,}\)</span> denoted <span class="process-math">\(Tr(A)\text{,}\)</span> is the sum of its diagonalentries. Prove the following assertions.</div> <div class="para logical"><ol class="decimal">
<li><div class="para">
<span class="process-math">\(Tr(AB) = Tr(BA)\)</span> for any <span class="process-math">\(n \times n\)</span> matrices <span class="process-math">\(A, B\text{.}\)</span>
</div></li>
<li><div class="para">Use (a) to prove that the trace of a matrix <span class="process-math">\(A\)</span> over <span class="process-math">\(\C\)</span> is the sumof its eigenvalues (with multiplicities).</div></li>
</ol></div> <section class="paragraphs"><h6 class="heading"><span class="title"><em class="emphasis">Proof.</em>.</span></h6> <section class="paragraphs"><h6 class="heading"><span class="title">Part (a).</span></h6> <div class="para">Let <span class="process-math">\(A,B\)</span> be <span class="process-math">\(n\times n\)</span> matrices.</div></section> <section class="paragraphs"><h6 class="heading"><span class="title">Part (b).</span></h6> <div class="para">Let <span class="process-math">\(A\)</span> be a matrix in <span class="process-math">\(\C\text{.}\)</span> Eigenvalues are the roots of <span class="process-math">\(\cp_A(x)\text{,}\)</span> which is the determinant of the matrix <span class="process-math">\(A-xI\text{.}\)</span> Every matrix <span class="process-math">\(A\)</span> is similar to a matrix in Rational Canonical Form, and similar matrices share invariant factors and thus characteristic polynomials, the roots of which are the eigenvalues of the matrix.</div></section></section></section></section><span class="incontext"><a href="sec-det.html#defn-multilinear-and-alternating" class="internal">in-context</a></span>
</body>
</html>
