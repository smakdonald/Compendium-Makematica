<?xml version='1.0' encoding='utf-8'?>

<pretext xml:lang="en-US" xmlns:xi="http://www.w3.org/2001/XInclude">

  <xi:include href="./bookinfo.xml" />
  
  <book xml:id="modern-algebra">
    <title>Compendium Algebraica</title>
    <!-- <subtitle>All in one Place</subtitle> -->

    <frontmatter xml:id="frontmatter">
      <titlepage>
        <author>
          <personname>Sam Macdonald</personname>
          <department>Department of Mathematics</department>
          <institution>University of Nebraska -- Lincoln</institution>
        </author>
        <date>
          <today />
        </date>
      </titlepage>

      <colophon>

        <website>
          <name>
            <c>smakdonald.github</c>
          </name>
          <address>https://smakdonald.github.io/index.html</address>
        </website>

        <copyright>
          <year>2020<ndash />2023</year>
          <holder>Sam Macdonald</holder>
          <shortlicense> 
            This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License. To view a copy of this license, visit <url href="http://creativecommons.org/licenses/by-sa/4.0/" visual="creativecommons.org/licenses/by-sa/4.0"> CreativeCommons.org</url>
          </shortlicense>
        </copyright>
      </colophon>

    </frontmatter>

    <part xml:id="part-group">
      <title>Group Theory</title>
      
      <xi:include href="./IntroGroups.ptx" />

      <xi:include href="./GroupHomoms.ptx" />

      <xi:include href="./NormQuotient.ptx" />

      <xi:include href="./GroupActions.ptx" />

      <xi:include href="./Sylow.ptx" />

      <chapter xml:id="group-products">
        <title>Group Classifications</title>

        <section xml:id="sec-grp-spds">
          <title>Semidirect Products</title>

          <subsection xml:id="subsec-group-direct-prods">
            <title>Internal Direct Products</title>
            <p>
              We now discuss how to build new groups from old ones.
            </p>
  
            <definition xml:id="def-dp-external">
              <statement>
                <p>
                  Let <m>G_\alpha</m> be a group for all <m>\alpha</m> in an index set <m>J</m>. The <em>direct product</em> of the groups <m>G_\alpha</m> is the Cartesian product <m>\prod_{\alpha\in J} G_\alpha</m> with multiplication defined by 
                  <me>
                    (g_a)_{\alpha\in  J}(h_\alpha)_{\alpha\in J} = (g_\alpha h_\alpha)_{\alpha\in J}.
                  </me> 
                  The <em>direct sum</em> of the groups <m>G_\alpha</m> is the subset <m>\bigoplus_{\alpha\in J} G_\alpha</m> of the direct product <m>\prod_{\alpha\in J} G_\alpha</m> given by 
                  <me>
                    \bigoplus_{\alpha\in J} G_\alpha= \{(g_\alpha)_{\alpha\in J} \mid  g_\alpha= e_{G_\alpha} \text{ for all but finitely many } \alpha\},
                  </me> 
                  with the same multiplication as the direct product.
                </p>
              </statement>
            </definition>
  
            <theorem>
              <statement>
                <p>
                  The direct product of a collection of groups is a group, and the direct sum of the collection is a subgroup of the direct product.
                </p>
              </statement>
            </theorem>
  
            <example>
              <p>
                If <m>\operatorname{gcd}(m,n)=1</m> then <m>\mathbb{Z}/m\times \mathbb{Z}/n\cong \mathbb{Z}/mn</m>. Indeed consider the elements <m>x=(1,0)</m> and <m>y=(0,1)</m> in <m>\mathbb{Z}/m\times \mathbb{Z}/n</m>. Then <m>|x|=m, |y|=n</m> and <m>x+y=y+x=(1,1)</m>. Therefore <m>|xy|=\operatorname{lcm}(|x|,|y|)=mn</m>. Since <m>\langle x+y \rangle \subseteq \mathbb{Z}/m\times \mathbb{Z}/n</m> and both of these sets have cardinality <m>mn</m> it must be the case that <m>\mathbb{Z}/m\times \mathbb{Z}/n=\langle x+y \rangle=\langle (1,1) \rangle</m>. Since <m>\langle x+y \rangle</m> and <m>\mathbb{Z}/mn</m> are both cyclic groups of order <m>mn</m> they are isomorphic. Thus 
                <me>
                  \mathbb{Z}/m\times \mathbb{Z}/n\cong \mathbb{Z}/mn.
                </me>
              </p>
            </example>
  
            <theorem xml:id="thm-dp-recognition">
              <title>Recognition Theorem for Direct Products</title>
              
              
              <statement>
                <p>
                  Suppose <m>G</m> is a group with normal subgroups <m>H\mathrel{\unlhd}G</m> and <m>K\mathrel{\unlhd}G</m> such that <m>H\cap K=\{e\}</m> Then <m>HK\cong H\times K</m> via the isomorphism of groups <m>\theta: H \times K \to HK</m> defined by <m>\theta(h,k) = hk</m>. Moreover <m>H\cong \theta^{-1}(H)=\{(h,e)\mid h\in H\}\leq H\times K</m> and <m>K\cong \theta^{-1}(K)=\{(e,k)\mid k\in K\}\leq H\times K</m>.
                </p>
              </statement>
  
              <proof>
                <p>
                  Notice that the hypothesis implies <m>HK\leq G</m>. Furthermore <m>H\mathrel{\unlhd}G, K\mathrel{\unlhd}G</m> and <m>H\cap K=\{e\}</m> imply that the elements of <m>H</m> commute with the elements of <m>K</m>. Indeed, consider <m>h\in H,k\in K</m>. Then since <m>H\mathrel{\unlhd}G</m>, <m>khk^{-1} \in H</m>, so also <m>[k,h]=khk^{-1}h^{-1}\in H</m>. Similarly it follows that <m>[k,h]\in K</m>, but since <m>H \cap K = \{e\}</m> it follows that <m>[k,h]= e</m>, i.e. <m>hk = kh</m> for any <m>h\in H, k\in K</m>.
                </p>
  
                <p>
                  Using the above we have <me>\begin{aligned}
        \theta((h_1, k_1) (h_2, k_2))  &amp; = 
        \theta(h_1h_2, k_1k_2) \\
        &amp; = h_1h_2k_1k_2\\
        &amp; = h_1k_1h_2k_2 = \theta(h_1, k_1) \theta(h_2, k_2)
        \end{aligned}</me> and thus <m>\theta</m> is a homomorphism. It’s kernel is <m>\{(k,h) \mid k = h^{-1} \}</m>, which is just <m>\{e\}</m> since <m>H \cap K = \{e\}</m>. The image of <m>\theta</m> is clearly <m>HK</m>. This proves <m>\theta</m> is an isomorphism. ◻
                </p>
              </proof>
            </theorem>
            
            <definition xml:id="def-int-ext-dp">
              <statement>
                <p>
                  If <m>H\mathrel{\unlhd}G</m> and <m>K\mathrel{\unlhd}G</m> are such that <m>H\cap K=\{e\}</m> then we call <m>HK</m> is called the <em>internal direct product</em> of <m>H</m> and <m>K</m> and <m>H\times K</m> the <em>external direct product</em> of <m>H</m> and <m>K</m>.
                </p>
              </statement>
            </definition>
  
            <p>
              We now discuss an important generalization for the direct product and a new method of constructing a new groups from the action of one group on another.
            </p>
            
  
            <p>
              Suppose <m>G</m> is a group with subgroups <m>H\mathrel{\unlhd}G</m> and <m>K\leq G</m> such that <m>H\cap K=\{e\}</m>. Then we still have <m>HK\leq G</m>; let’s see what we would need the multiplication on the cartesian product <m>H\times K</m> to be in order for <m>\theta: H \times K \to HK</m> defined by <m>\theta(h,k) = hk</m> to still be a group homomorphism: <me>\theta(h_1, k_1) \theta(h_2, k_2) = h_1k_1h_2k_2 = h_1h'_2k_1k_2=\theta(h_1h_2',k_1k_2),</me> where <m>h'_2\in H</m> is such that <m>k_1h_2k_1^{-1}=h'_2</m>.
            </p>
    
            <p>
              This means that we would need to have <m>(h_1, k_1)(h_2, k_2)=(h_1h_2',k_1k_2)</m> for <m>\theta</m> to be a homomorphism. This motivates the following definition.
            </p>
  
            <problem>
              <p>
                Let <m>G</m> be a group that acts on a set <m>A</m>, and <m>H</m> a subgroup of <m>G</m> such that for any <m>a, b \in A</m> there exists a unique <m>h \in H</m> with <m>ha = b</m>. 
                <ol>
                  <li>
                    <p>
                      Prove that for every <m>a \in A, G = HG_a</m> and <m>H \cap Ga = \{1\}</m>, where <m>G_a = \{g \in G | ga = a\}.</m> 
                    </p>
                  </li>
  
                  <li>
                    <p>
                      Prove that if <m>H \subseteq Z(G)</m> then for every <m>a \in A</m>, <m>G</m> is the internal direct product of <m>H</m> and <m>G_a.</m>
                    </p>
                  </li>
                </ol>
              </p>
            </problem>
  
            <proof>
              <p>
                Let <m>G</m> be a group that acts on a set <m>A</m>, and <m>H</m> a subgroup of <m>G</m> such that<!-- linebreak -->for any <m>a, b \in A</m> there exists a unique <m>h \in H</m> with <m>ha = b</m>.
              </p>
  
              <p>
                Suppose there exists some <m>h\in H</m> such that <m>h\in G_a</m>. Then <m>ha=a</m>. But <m>ea=a</m> by the definition of group action. As the <m>h\in G_a</m> is unique, we see <m>h=e</m>. Thus <m>H\cap G_a=\{e\}</m>.
              </p>
  
              <p>
                Let <m>g\in G</m> and <m>a\in A</m>. If <m>g\in H</m> then we can write <m>g=ge</m>, as <m>e\in G_a</m>. Suppose <m>g\not\in H</m> and <m>ga=b</m> for some <m>b\in A</m>. There exists an <m>h\in H</m> such that <m>ha=b</m>. Then <m>ha=ga</m>, so <m>h\inv g a=a</m>, so <m>h\inv g\in G_a</m>. Thankfully, we can know write <m>g=h(h\inv g</m>), and so <m>G=HG_a</m>.
              </p>
  
  
                <p>
                  If <m>H\subseteq Z(G)</m> then we have <m>HG_a=G_aH</m>, which means <m>HG_a\leq G</m>. From Part (a) we have a trivial intersection, making <m>G</m> the internal direct product of <m>H</m> and <m>G_a</m>.
                </p>
            </proof>
          </subsection>

          <subsection xml:id="subsec-external-sdps">
            <title>External Semi Direct Products</title>
            
            <definition xml:id="def-sdp-external">
              <statement>
                <p>
                  Let <m>H</m> and <m>K</m> be groups and let <m>\rho:K\to \operatorname{Aut}(H)</m> be a homomorphism. The (external) <em>semidirect product</em> induced by <m>\rho</m> is the set <m>H \times K</m> with the binary operation defined by <me>(h,k)(h',k') = (h\rho(k)(h'),kk').</me> This group is denoted by <m>H \rtimes_\rho K</m>.
                </p>
              </statement>
            </definition>
  
            <p>
              Before we prove that the construction above actually gives a group, let’s compute a few examples.
            </p>
  
            <example>
              <p>
                Given <m>H</m> and <m>K</m> we could always take <m>\rho</m> to be the trivial homomorphism, so that <m>\rho(y)(x) = x</m> for all <m>y \in K</m> and <m>x \in H</m>. Then <m>K \rtimes_\rho H</m> is just the usual direct product: <me>(y_1, x_1) (y_2, x_2) = (y_1 y_2, x_1 x_2).</me>
              </p>
            </example>
  
            <example>
              <p>
                Fix a group <m>G</m>, a normal subgroup <m>H \unlhd G</m> and a subgroup <m>K \leq G</m>. Then the function <me>\rho: K \to \operatorname{Aut}(H)</me> given by <m>\rho(x)(y) = xyx^{-1}</m> for <m>x \in K, y \in H</m> is a homomorphism. Thus <m>K</m> acts on <m>H</m> via automorphisms.
              </p>
            </example>
  
            <example>
              <p>
                Let <m>K = \langle x \rangle</m> be cyclic of order <m>2</m> and <m>H = \langle y \rangle</m> be cyclic of order <m>n</m> for any <m>n \geq 1</m>. There is an automorphism of <m>K</m> that sends <m>y</m> to <m>y^{-1}</m>. This automorphism is clearly its own inverse; i.e., it has order <m>2</m>. Therefore, by the UMP for cyclic groups, there is a group homomorphism <me>\rho: K \to \operatorname{Aut}(H)</me> with <m>\rho(x)(y) = y^{-1}</m>. We may thus form the group <me>G := H \rtimes_\rho K.</me> The elements of <m>G</m> are <m>(y^i, x^j)</m> for <m>0 \leq i \leq n-1</m> and <m>0 \leq j \leq 1</m>, in particular <m>|G|=2n</m>. Set <me>\tilde y = (y,e) \in G \text{ and } \tilde x = (e,x) \in G</me> Then <me>\tilde y^n = (y,e_K)^n = (y^n,e_K) = (e_H, e_K)= e_G</me> <me>\tilde x^2 = (e_H,x)^2 = (e_H, x^2) = (e_H, e_K)= e_G</me> and <me>\tilde x \tilde y \tilde x \tilde y = (e_H,x)(y,e_K)(e_H, x)(y,e_K) = (\rho(x)(y),x)(\rho(x)(y),x) = (y^{-1},x)(y^{-1},x) = (y^{-1}y, e) = e_G.</me> Looks familar!
              </p>
      
              <p>
                Indeed, by the universal mapping property for <m>D_{2n}</m> we have a homomorphism <me>\theta: D_{2n} \to G</me> such that <m>\theta(r) = (y,e_K)</m> and <m>\theta(s) = (x,e_H)</m>. Moreover, <m>\theta</m> is onto since <me>\theta(r^is^j)=(y^i, x^j) \text{ for all } 0 \leq i \leq n-1, 0 \leq j \leq 1</me> and since <m>|D_{2n}|=|G|=2n</m> it follows that <m>\theta</m> is a bijection. So the dihedral group is a semidirect product, in which the two component groups are cyclic of orders <m>n</m> and <m>2</m> respectively: <me>D_{2n} \cong \langle y \rangle \rtimes_\rho \langle x \rangle</me> and <m>\rho</m> is the inversion homomorphism as described above.
              </p>
            </example>
  
            <theorem xml:id="thm-sdp">
              <statement>
                <p>
                  If <m>H</m> and <m>K</m> are groups and <m>\rho:K\to \operatorname{Aut}(H)</m> is a homomorphism, then setting :
                </p>
  
                <p><ol>
                  <li>
                          <p>
                  <em><m>H \rtimes_\rho K</m> is a group</em>
                </p>
                  </li>
  
                  <li>
                          <p>
                  <em><m>H\cong H':=\{(h,e)\mid h\in H\}\mathrel{\unlhd}H \rtimes_\rho K</m> and <m>K\cong K':=\{(e,k)\mid k\in K\}\leq H \rtimes_\rho K</m></em>
                </p>
                  </li>
  
                  <li>
                          <p>
                  <em><m>(H \rtimes_\rho K )/ H' \cong K</m>.</em>
                </p>
                  </li>
  
                </ol></p>
              </statement>
  
              <proof>
                <p>
                  (1.) The proof is straightforward but a bit messy. For associativity, note that <me>\begin{aligned}
                    (y_1,x_1) \left( (y_2, x_2) (y_3, x_3) \right)
                    &amp;=
                    (y_1,x_1) (y_2\rho(x_2)(y_3), x_2x_3)
                    =
                    (y_1\rho(x_1)\left(y_2\rho(x_2)(y_3)\right), x_1x_2x_3)\\
                    &amp;=
                    (y_1\rho(x_1)(y_2)(\rho(x_1)\circ \rho(x_2))(y_3), x_1x_2x_3)\\
                    &amp;=
                    (y_1\rho(x_1)(y_2)\rho(x_1x_2)(y_3), x_1x_2x_3)\\
                    \end{aligned}</me> On the other hand <me>\begin{aligned}
                    \left( (y_1,x_1) (y_2, x_2) \right) (y_3, x_3) 
                    &amp; = (y_1 \rho(x_1)(y_2), x_1 x_2)  (y_3, x_3) 
                    = (y_1 \rho(x_1)(y_2)\rho(x_1x_2)(y_3), x_1 x_2 x_3).
                    \end{aligned}</me> This gives associativity.
                            </p>
                    
                            <p>
                              The fact that <m>(e,e)</m> is a two-sided identity follows from the fact that <m>\rho(e)(y) = \text{id}_H(y)=y</m>.
                            </p>
                    
                            <p>
                              Finally <me>\begin{aligned}
                    (y,x) (\rho(x^{-1})(y^{-1}) ,x^{-1}) &amp;= (y \rho(x)\left(\rho(x^{-1})(y^{-1}) \right), e)
                    = (y (\rho(x)\circ\rho(x^{-1}))(y^{-1}), e) 
                    \\ &amp;= (y \rho(e)(y^{-1}), e) = (yy^{-1},e)=(e,e),
                    \end{aligned}</me> and similarly <me>(\rho(x^{-1})(y^{-1}) ,x^{-1})  (y,x) = (e,e).</me>
                            </p>
                    
                            <p>
                              (2.) Define a funtion <me>i: H \to H \rtimes_\rho K</me> as <m>i(y) = (y, e)</m>. Then <m>i</m> is a homomorphism, since <me>i(y_1) i(y_2) = (y_1,e)(y_2,e) = (y_1\rho(e)(y_2) , ee) = (y_1 y_2, e) = i(y_1y_2).</me> The map is clearly injective and hence its image is isomorphic to <m>H</m>. In fact, the image is normal since the second component of <me>(y,x) (y_2, e) (\rho(x^{-1})(y^{-1}), x^{-1})</me> is clearly <m>e</m>. Let us write this image as <me>H' := \{(y,e) \mid y \in H\} \unlhd H \rtimes_\rho K.</me>
                            </p>
                    
                            <p>
                              The function <me>j: K \to H \rtimes_\rho K</me> defined by <m>j(x) = (e,x)</m> is also an injective homomorphism and thus its image <me>K' := \{(e,x) \mid x \in H \} \leq H \rtimes_\rho K</me> is isomorphic to <m>K</m>. <m>K'</m> is typically <em>not</em> normal, however. Finally, it is easy to see that <m>H'K'= H \rtimes_\rho K</m> and <m>H' \cap K' = \{e\}</m>. Putting this all together we have
                            </p>
                    
                            <p><ul>
                              <li>
                                      <p>
                              <m>H' \unlhd H \rtimes_\rho K</m>,
                            </p>
                              </li>
                    
                              <li>
                                      <p>
                              <m>K' \leq H \rtimes_\rho K</m>,
                            </p>
                              </li>
                    
                              <li>
                                      <p>
                              <m>H' K' = H \rtimes_\rho K</m>, and
                            </p>
                              </li>
                    
                              <li>
                                      <p>
                              <m>H'\cap K' = \{e\}</m>.
                            </p>
                              </li>
                    
                            </ul></p>
                    
                            <p>
                              (3.) Consider the projection onto the second factor <m>\pi_2:H \rtimes_\rho K \to  K</m> given by <m>\pi_2(y,x)=x</m>. This is a goup homomorphism since the second component of <m>(y_1,x_1)(y_2,x_2)</m> is <m>x_1x_2</m> and is surjective by definition. Now <me>\operatorname{Ker}(\pi_2)=\{(y,e_K)\mid y\in H\}=H'\cong H.</me> By the first isomorphism theorem we conclude that <m>(H \rtimes_\rho K )/ H' \cong K</m>. ◻
                            </p>
              </proof>
            </theorem>
  
            <exercise>
              <p>
                If we idenify <m>K</m> with <m>K'</m> and <m>H</m> with <m>H'</m> via the isomorphisms <m>i</m> and <m>j</m>, prove the action of <m>H'</m> on <m>K'</m> via conjugation in <m>K \rtimes_\rho H</m> coincides with the original action <m>\rho</m>.
              </p>
            </exercise>
          </subsection>

          <subsection xml:id="subsec-internal-sdps">
            <title>Internal Semi Direct Products</title>
            
            <p>
              We can turn this around.
            </p>
  
            <theorem xml:id="thm-sdp-internal">
              <title>Recognition Theorem for Internal Semidirect Products</title>
              
              
              <statement>
                <p>
                  For a group <m>G</m>, suppose we are given <m>H</m> and <m>K</m> so that
          </p>
  
          <p><ul>
            <li>
                    <p>
            <m>H \unlhd G</m>,
          </p>
            </li>
  
            <li>
                    <p>
            <m>K \leq G</m>,
          </p>
            </li>
  
            <li>
                    <p>
            <m>HK = G</m>, and
          </p>
            </li>
  
            <li>
                    <p>
            <m>H \cap K = \{e\}</m>.
          </p>
            </li>
  
          </ul></p>
  
          <p>
            Let <m>\rho: K \to \operatorname{Aut}(H)</m> be the permutation representation of the action of <m>K</m> on <m>H</m> via automorphisms given by conjugation in <m>G</m>. (This means that for any <m>k\in K</m> <m>\rho(k)=c_k</m>, where <m>c_k\in \operatorname{Aut}(H)</m> is the function <m>c_k(h)=khk^{-1}</m> for all <m>h\in H</m>.) Then the function <me>\theta: H \rtimes_\rho K \to G</me> defined by <m>\theta(y,x) = yx</m> is an isomorphism of groups. Moreover, under this isomorphism, <m>K</m> corresponds to <m>K'</m> and <m>H</m> corresponds to <m>H'</m> (referring to the notation in Theorem <xref ref="thm-sdp" /> above).
                </p>
              </statement>
  
              <proof>
                <p>
                  We have <me>\begin{aligned}
                    \theta((y_1, x_1) (y_2, x_2))  &amp; = 
                    \theta(y_1 c_{x_1}(y_2), x_1 x_2) \\
                    &amp; = y_1x_1y_2x_1^{-1} x_1 x_2 \\
                    &amp; = y_1x_1y_2x_2 = \theta(y_1, x_1) \theta(y_2, x_2)
                    \end{aligned}</me> and thus <m>\theta</m> is a homomorphism. It’s kernel is <m>\{(y,x) \mid y = x^{-1}, y\in H, x\in K \}</m>, which is just <m>\{e\}</m> since <m>H \cap K = \{e\}</m>. The image of <m>\theta</m> is clearly <m>KH = G</m>. This proves <m>\theta</m> is an isomorphism. It is obvious that <m>\theta(K') = K</m> and <m>\theta(H')= H</m>.
                </p>
              </proof>
            </theorem>
            
            <definition xml:id="def-spd-internal">
              <statement>
                <p>
                  In this situation of the Proposition <xref ref="thm-sdp-internal" />, we will say that <m>G</m> is the <em>internal semi-direct product</em> of <m>H</m> and <m>K</m>.
                </p>
              </statement>
            </definition>
  
            <example>
              <p>
                Returning to <m>D_{2n}</m>, let <m>H = \langle s \rangle</m> and <m>K = \langle r \rangle</m>. Then <m>H \leq G</m>, <m>K \unlhd G</m>, <m>HK = G</m> and <m>H \cap K =
        \{e\}</m>. So, <m>G</m> is isomorphic to a semi-direct product, as we already showed.
              </p>
            </example>
  
            <example>
              <p>
                Let <m>G = S_n</m>, <m>K = A_n</m> and <m>H = \langle (1 \, 2) \rangle</m>. Then <m>K \unlhd G</m>, <m>H \leq G</m>, <m>KH = G</m> and <m>K \cap H  = \{e\}</m>. It follows that <me>S_n \cong A_n \rtimes_\rho C_2</me> where <m>C_2 = \langle x \rangle</m> is cyclic of order <m>2</m> and the action <m>\rho: C_2 \to \operatorname{Aut}(A_n)</m> sends <m>x</m> to conjugation by <m>(1 \, 2)</m>.
              </p>
            </example>
  
            <p>
              It is important to be aware that for a fixed pair of groups <m>H</m> and <m>K</m>, different actions of <m>H</m> on <m>K</m> via automorphisms can result in isomorphic semi-direct products. Indeed, determining when <m>K \rtimes_{\rho} H \cong K \rtimes_{\rho'} H</m> is in general a tricky business. The previous example shows this:
            </p>
  
            <example>
              <p>
                Let <m>G = S_n</m> and <m>K = A_n</m> again, but this time take <m>H' = \langle (1 \, 3) \rangle = (1 \,2 \, 3) \langle (1 \, 2) \rangle (1 \,2 \, 3)^{-1}</m> (assuming <m>n \geq 3</m>). Then we get <me>S_n \cong A_n \rtimes_{\rho'} C_2</me> where <m>C_2 = \langle x \rangle</m> is cyclic of order <m>2</m> and the action <m>\rho': C_2 \to \operatorname{Aut}(A_n)</m> sends <m>x</m> to conjugation by <m>(1 \,3)</m>.
              </p>
  
              <p>
                The actions <m>\rho</m> and <m>\rho'</m> are not identical. For example, assuming <m>n\geq 4</m> we have <me>\rho(x)(1 \, 2) (1 \, 2) (3 \, 4) = (1 \, 2) (3 \, 4) \text { maps } 1\mapsto 2</me> and <me>\rho'(x)(1 \, 2) (3 \, 4) = (1 \, 3) (1 \, 2) (3 \, 4) (1 \, 3) \text { maps } 1\mapsto 4.</me> Yet <me>A_n \rtimes_{\rho} H \cong
      A_n \rtimes_{\rho'} H'</me> since each is isomorphic to <m>S_n</m>.
              </p>
      
              <p>
                On HW 8 you will give a more conceptual reason for why these two semidirect products turned out to be isomorphic: it is because <m>H</m> and <m>H'</m> are conjugate in <m>S_n</m>. More generally, below is a criterion for a two semidirect products to be isomorphic.
              </p>
            </example>
  
            <theorem xml:id="thm-conjugate-aut">
              <statement>
                <p>
                  Let <m>K</m> be a finite cyclic group and let <m>H</m> be an arbitrary group. Suppose that the images of <m>\phi: K \to \operatorname{Aut}(H)</m> and <m>\theta: K \to \operatorname{Aut}(H)</m> are conjugate subgroups of <m>\operatorname{Aut}(H)</m>. Then <m>H \rtimes_\phi K \cong H \rtimes_\theta K</m>.
                </p>
              </statement>
            </theorem>
          </subsection>
          
          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>

        </section>

        <section xml:id="sec-order-pq">
          <title>Groups of Order <m>pq</m></title>

          <lemma xml:id="lem-aut-cn">
            <statement>
              <p>
                The automorphism group of <m>C_n</m> is isomorphic to the multiplicative group of units of <m>\mathbb{Z}/n</m> via the map 
                <me>
                  f: (\mathbb{Z}/n^\times, \cdot) \to \operatorname{Aut}(C_n), \quad f([j]_n)=[ y\mapsto y^j].
                </me> 
                where <m>\mathbb{Z}/n^\times=\{[j]_n\mid \operatorname{gcd}(j,n)=1\}</m>.
              </p>
            </statement>
          </lemma>

          <lemma xml:id="lem-aut-cp">
            <statement>
              <p>
                If <m>p</m> is prime the automorphism group of <m>C_p</m> is cyclic, namely <m>\operatorname{Aut}(C_p)\cong C_{p-1}</m>.
              </p>
            </statement>
          </lemma>

          <theorem xml:id="thm-groups-of-order-6">
            <statement>
              <p>
                Any group of order <m>6</m> is isomorphic either to <m>C_6</m> or to <m>D_6</m>.
              </p>
            </statement>

            <proof>
              <p>
                Let <m>G</m> be a group of order 6. Cayley’s theorem gives that there exist elements <m>x,y\in G</m> with <m>|x|=2</m> and <m>|y|=3</m>. Let <m>K=\langle x \rangle</m> and <m>H=\langle y\rangle</m>. Since <m>[G:H]=2</m>, <m>H</m> is a normal subgroup of <m>G</m> and since <m>H\cap K</m> is a common subgroup of <m>H</m> and <m>K</m> Lagrange’s theorem gives that <m>|H\cap K|\mid \operatorname{gcd}(|H|,|K|)=1</m>. Thus <m>H\cap K=\{e\}</m> and since <m>|HK|=\frac{|H||K|}{|H\cap K|}=6=|G|</m> we deduce that <m>HK=G</m>. Proposition <xref ref="prop:internaldirprod" /> now gives that <m>G</m> is the internal semidirect product of <m>H</m> and <m>K</m>. More to the point, <m>G\cong H\rtimes_\rho K</m>, where <m>\rho:K\to \operatorname{Aut}(H)</m> gives the action of <m>K</m> on <m>H</m> by conjugation.
				</p>

				<p>
					We now analyze the possibilities for <m>\rho</m>. By Lemma <xref ref="lem:Aut(C_n)" />, <m>\operatorname{Aut}(H)\cong\operatorname{Aut}(\mathbb{Z}/3)\cong (\mathbb{Z}/3^\times, \cdot)=(\{\pm 1\},\cdot)</m>. There are two possibilities for the image of <m>\rho</m>: either <m>\operatorname{Im}(\rho)=\{\text{id}_H\}</m> or <m>\operatorname{Im}(\rho)=\operatorname{Aut}(H)</m>.
				</p>

				<p>
					If <m>\operatorname{Im}(\rho)=\{\text{id}_H\}</m>, then <m>\rho(x)=c_x=\text{id}_H</m> (which implies <m>xy=yx</m>) and <m>H\rtimes_\rho K=H\times K</m>. Therefore, in this case <m>G\cong H\times K\cong C_3\times C_2\cong C_6</m>, where the last isomorphism uses the Chinese Remainder Theorem <xref ref="thm:CRT" />.
				</p>

				<p>
					If <m>\operatorname{Im}(\rho)=\operatorname{Aut}(H)</m>, then <m>\rho(x)</m> is the map <m>y^i\mapsto y^{-1}</m> and by an earlier example for this <m>\rho</m> we have <m>H\rtimes_\rho K\cong D_6</m>, so <m>G\cong D_6</m>.
				</p>

				<p>
					Finally, <m>C_6\not \cong D_6</m> because the former is abelian and the latter is not.
              </p>
            </proof>
          </theorem>
    
          <p>
            Let’s repeat the previous example for classifying groups of order <m>p\cdot q</m> with <m>p, q</m> distinct primes into isomorphism classes.
          </p>

          <theorem xml:id="thm-order-pq">
            <statement>
              <p>
                Let <m>p&lt;q</m> be primes.
              </p>
      
              <p><ol>
                <li>
                        <p>
                If <m>p\nmid q-1</m> there is a unique group of order <m>pq</m> up to isomorphism, namely <m>C_{pq}</m>.
              </p>
                </li>
      
                <li>
                        <p>
                If <m>p\mid q-1</m> there are exactly two groups of order <m>pq</m> up to isomorphism, namely <m>C_{pq}</m> and a non-abelian group.
              </p>
                </li>
      
              </ol></p>
            </statement>

            <proof>
              <p>
                Let <m>G</m> be a group of order <m>pq</m> and let <m>H, K</m> be Sylow subgroups of order <m>q</m> and <m>p</m> respectively. We see that <m>H</m> is a normal subgroup using a HW problem, since <m>[G:H]=p</m> is the smallest prime that divides <m>|G|</m>.
              </p>
      
              <p>
                Furthermore, since <m>H\cap K</m> is a subgroup of both <m>H</m> and <m>K</m> we have by Lagrange’s theorem that <m>|H\cap K|\mid \operatorname{gcd}(q,p)=1</m>, so that <m>H\cap K=\{e_G\}</m>. From here it follows that <me>|HK|=\frac{|H||K|}{|H\cap K|}=\frac{q\cdot p}{1}=pq=|G|</me> and so <m>HK=G</m>. The recognition theorem now yields that <me>G \cong H \rtimes_\rho K \cong C_q\rtimes_\rho C_p.</me> for some homomorphism <m>\rho: K \to \operatorname{Aut}(H)</m>, equivalently <m>\rho: C_p \to \operatorname{Aut}(C_q)</m>. By the UNM of cyclic groups to give such a homomorphism <m>\rho</m> is equivalent to giving an element <m>z\in \operatorname{Aut}(C_q)</m> so that <m>z^p=\text{id}</m>, which will give <m>\rho(y^j)=z^j</m> for <m>C_p=\langle y\rangle</m>. Thus <m>|z|\mid p</m> yielding that either <m>|z|=1</m> or <m>|z|=p</m>.
              </p>
      
              <p>
                <em>Case 1:</em> if <m>|z|=1</m> then <m>\rho</m> is the trivial homomorphism and thus <m>G\cong C_q\times C_p\cong C_{pq}</m>.
              </p>
      
              <p>
                <em>Case 2:</em> if <m>|z|=p</m> then it must be the case by Lagrange that <m>p\mid |\operatorname{Aut}(C_q)</m>. By Lemma <xref ref="lem:Aut(C_p)" /> we know that <m>\operatorname{Aut}(C_q)\cong C_{p-1}</m> is a cyclic group. Therefore we have that <m>p\mid (q-1)</m> if and only if there exists an element <m>z\in \operatorname{Aut}(C_q)</m> of order <m>p</m> by Theorem <xref ref="thm:cyclic" /> (2) . Moreover any such element <m>z</m> generates a subgroup of <m>\operatorname{Aut}(C_q)</m> of order <m>p</m>. Since there is a unique subgroup of a cyclic group of a given order by Theorem <xref ref="thm:cyclic" /> (2) we see that the image of <m>\rho</m> is independent of the choice of <m>z</m>. Thus by Proposition <xref ref="prop:conjugateAut" /> we conclude that all subgroups resulting from any choice of <m>z</m> of order <m>p</m> are isomorphic.
              </p>
      
              <p>
                Moreover, from the explicit presentation of semidirect products of cyclic groups given in a homework problem we see that the resulting group is non-abelian; in particular it is not isomorphic to <m>C_{pq}</m>. ◻
              </p>
            </proof>
          </theorem>
          

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

        <section xml:id="sec-ftfgag">
          <title>Abelian Classification</title>

          <p>
            In this section we see that we can classify finitely generated abelian groups into isomorphism classes.
          </p>

          <definition xml:id="def-f.g.-group">
            <statement>
              <p>
                A group <m>G</m> is finitely generated provided that <m>G=\langle A \rangle</m>, where <m>A</m> is a finite set.
              </p>
            </statement>
          </definition>

          <remark>
            <p>
              Any finite group is finitely generated (take <m>A=G</m>), but a finitely generated group need not be finite.
            </p>
          </remark>

          <p>
            The following is the classification theorem for finitely generated abelian groups. We present it without proving it for now. The full proof will be given in the spring semester.
          </p>

          <theorem xml:id="thm-ftfgag">
            <title>Fundamental Theorem of Finitely Generated Abelian Groups (FTFGAG)</title>
            
            
            <statement>
              <p>
                Let <m>G</m> be a finitely generated abelian group. Then <m>G</m> is a direct product of cyclic groups. More precisely
              </p>

              <p>
                <ol>
                <li>
                  <p>
                There exist <m>r,s \geq 0</m>, prime integers <m>p_1&lt;\ldots&lt; p_s</m> and positive integers <m>a_i \geq 1</m> such that: <m>G \cong \mathbb{Z}^r\times Q_1 \times \dots \times Q_s</m> where <m>|Q_i| = p_i^{a_i}</m> for all <m>i</m>.
                </p>
                </li>
      
                <li>
                 <p>
                For each index <m>1\leq i\leq s</m>, there is a partition <m>a_i = a_{i,1} + \dots + a_{i,j_i}</m> with each <m>a_{i,j} \geq 1</m>, such that <m>Q_i \cong (\mathbb{Z}/p_i^{a_{i1}}) \times \dots \times (\mathbb{Z}/p_i^{a_{i,j_i}})</m>, thus overall we have <me>G \cong (\mathbb{Z}/p_1^{a_{11}}) \times \dots \times (\mathbb{Z}/p_i^{a_{1,j_1}})\times \cdots \times (\mathbb{Z}/p_s^{a_{s1}}) \times \dots \times (\mathbb{Z}/p_s^{a_{s,j_s}}).</me>
                </p>
                </li>
      
                <li>
               <p>
                The <m>r,p_i</m>’s, <m>j_i</m>’s and <m>a_{i,j}</m>’s are uniquely determined by <m>G</m>.
                </p>
                 </li>
              
                  <li>
                    <p>
                      equivalently, there exist <m>r \geq 0, t \geq 0</m>, and <m>n_i \geq 2</m> for all <m>1\leq i\leq t</m>, satisfying <m>n_{i+1} \mid n_i</m> for all <m>i</m> so that <m>G \cong \mathbb{Z}^r \times (\mathbb{Z}/n_1) \times \dots \times (\mathbb{Z}/n_t)</m>.
                    </p>
                  </li>
      
                  <li>
                    <p>
                      The integers <m>r,s,n_1,\ldots ,n_t</m> are uniquely determined by <m>G</m>.
                    </p>
                  </li>
                </ol>
              </p>
            </statement>

            <proof>
              <p>
                It suffices prove that for a given group <m>G</m>, we can recover its invariant factor form from its elementary divisor form, and vice versa. We will be a bit hand-wavey for this following the ideas from the above examples. <me>G \cong  \mathbb{Z}^r \times \mathbb{Z}/p_1^{e_1} \times \cdots \times \mathbb{Z}/p_l^{e_l}.</me> by applying the Chinese Remainder Theorem we have <me>G \cong \mathbb{Z}^r \times \mathbb{Z}/d_1 \times \cdots \times \mathbb{Z}/d_n</me> where <m>d_1</m> is the product of the elementary divisors of highest power for each <em>distinct</em> prime in the list <m>p_1, \dots, p_l</m>, <m>d_2</m> is the product of the next highest possible prime powers, and so on. We will have that <m>d_2\mid d_1</m> and in general that <m>d_{i+1}\mid d_i</m> since by definition the exponent of <m>p_j\in d_i</m> is greater or equal to the exponent of <m>p_j\in d_{i+1}</m>.
              </p>

              <p>
                Conversely, given <me>G \cong \mathbb{Z}^r \times \mathbb{Z}/d_1 \times \cdots \times \mathbb{Z}/d_n</me> with <m>d_1 \mid d_2 \mid \cdots \mid d_n</m>, we may apply the CRT to each <m>\mathbb{Z}/d_i</m> to find its elementary divisor form. ◻
              </p>
            </proof>
          </theorem>

          <example>
            <p>
              For <m>G\cong\mathbb{Z}/3\times Z/5\times Z/5</m> we have <m>Q_1=\mathbb{Z}/3</m>, <m>Q_2=\mathbb{Z}/5\times Z/5</m>.
            </p>
          </example>

          <definition xml:id="def-eds-and-ifs">
            <statement>
              <p>
                In Theorem <xref ref="thm-ftfgag" />, the number <m>r</m> is the <em>rank</em> of <m>G</m>, the <m>p_i^{a_{i,k}}</m> are the <em>elementary divisors</em> of <m>G</m>, and the decomposition of <m>G</m> in parts (1–2) is called the <em>elementary divisor decomposition</em> of <m>G</m>. The decomposition in part (1) is also called a <em>primary decomposition</em>.
              </p>

              <p>
                In Theorem <xref ref="thm-ftfgag" />, the number <m>r</m> is the <em>rank</em> of <m>G</m>, the numbers <m>n_1,\ldots,n_t</m> are the <em>invariant factors</em> of <m>G</m>, and the decomposition of <m>G</m> in part (1) is the <em>invariant factor decomposition</em> of <m>G</m>.
              </p>
            </statement>
          </definition>

          <example>
            <p>
              Say I tell you <me>G \cong \mathbb{Z}^{3} \times \mathbb{Z}/4 \times \mathbb{Z}/8 \times \mathbb{Z}/9 \times \mathbb{Z}/27 \times \mathbb{Z}/25</me> The Chinese Remainder Theorem <xref ref="thm-sunzi" /> gives <me>\mathbb{Z}/8 \times \mathbb{Z}/27 \times \mathbb{Z}/25 \cong \mathbb{Z}/(8 \cdot 27 \cdot 25)</me> and <me>\mathbb{Z}/4\times \mathbb{Z}/9  \cong \mathbb{Z}/(4 \cdot 9)</me> so that <me>G \cong \mathbb{Z}^{3} \times \mathbb{Z}/(4 \cdot 9) \times \mathbb{Z}/(8 \cdot 27 \cdot 25).</me> Since <m>n_2:=(4 \cdot 9) \mid n_1:=(8 \cdot 27 \cdot 25)</m>, this is in invariant factor form, and hence the rank of <m>A</m> is <m>3</m> and the invariant factors of <m>A</m> are <m>4 \cdot 9</m> and <m>8 \cdot 27 \cdot 25</m>.
				</p>
          </example>

          <example>
            <p>
              Suppose now I tell you <me>G \cong \mathbb{Z}^{4} \times \mathbb{Z}/6 \times \mathbb{Z}/36 \times \mathbb{Z}/180.</me> Then by the Chinese Remainder Theorem <xref ref="thm-sunzi" /> <me>G \cong \mathbb{Z}^{4} \times \mathbb{Z}/2 \times \mathbb{Z}/3 \times \mathbb{Z}/4 \times \mathbb{Z}/9 \times \mathbb{Z}/4 \times \mathbb{Z}/5 \times \mathbb{Z}/9,</me> given the elementary divisor form.
            </p>
          </example>

          <theorem xml:id="thm-sunzi">
            <title>Sunzi's Remainder Theorem</title>
            

            <statement>
              <p>
                Suppose <m>m = p_1^{e_1} \cdots p_l^{e_l}</m> for distinct primes <m>p_1, \dots, p_l</m>. Then there is an isomorphism 
                <me>
                  \phi: \mathbb{Z}/m \xrightarrow{\cong} \mathbb{Z}/(p_1^{e_1}) \times \cdots \times \mathbb{Z}/(p_l^{e_l})
                </me> 
                given by 
                <me>
                  \phi([j]_m) = ([j]_{{p_1}^{e_1}}, \cdots, [j]_{{p_l}^{e_l}})
                </me> 
                where <m>[j]_b</m> denote the class of an integer <m>j</m> in <m>\mathbb{Z}/b</m>.
              </p>
            </statement>

            <proof>
              <p>
                Using the UMP for infinite cyclic groups, we let <m>\psi: \mathbb{Z}\to \mathbb{Z}/(p_1^{e_1}) \times \cdots \times \mathbb{Z}/(p_l^{e_l})</m> be the unique homomorhism that sends <m>1</m> to <m>([1]_{{p_1}^{e_1}}, \cdots, [1]_{{p_l}^{e_l}})</m>. Then <me>\psi(j) = ([j]_{{p_1}^{e_1}}, \cdots, [j]_{{p_l}^{e_l}}).</me> Clearly <m>m \in \operatorname{Ker}(\psi)</m> and so <m>\langle m \rangle \subseteq \operatorname{Ker}(\psi)</m>. Conversely, if <m>\psi(n) = 0</m>, then <m>p_i^{e_i} \mid n</m> for all <m>i</m> and since <m>p_1^{e_1}, \dots, p_l^{e_l}</m> are pairwise relatively prime, it follows that <m>m \mid n</m>. This proves <m>\operatorname{Ker}(\psi) = \langle m \rangle</m>. The claim follows by the first isomorphism theorem.
              </p>
            </proof>
          </theorem>

          <p>
            The FTFGAG makes classification of finite abelian groups a very quick matter.
          </p>

          <example>
            <p>
              Classify the abelian groups of order 75 up to isomorphism.
            </p>

            <p>
              Let <m>G</m> be an abelian group of order 75. Since <m>G</m> is finite the rank of <m>G</m> is <m>r=0</m>. Let’s determine the possible elementary divisors <m>p_i^{a_{i,j}}</m> so that <me>G\cong \prod_{k=1}^s   \prod_{j=1}^{i_k} \mathbb{Z}/p_i^{a_{k,j}}.</me> The above equation gives <m>75=|G|= \prod_{k=1}^s\prod_{j=1}^{i_k}p_i^{a_{k,j}}</m> and the possibilities for factoring <m>75</m> as a product of prime powers are <m>75=3\cdot 5 \cdot 5</m> or <m>75=3\cdot 25</m> which gives <me>G\cong \mathbb{Z}/25 \times \mathbb{Z}/3 \text{ or } G\cong \mathbb{Z}/5 \times \mathbb{Z}/5 \times \mathbb{Z}/3.</me>
            </p>
    
            <p>
              Note that the two groups above are not isomorphic. To see this, note that there is an element of order 25 in <m>\mathbb{Z}/25 \times \mathbb{Z}/3</m>, namely <m>([1]_{25},[0]_3)</m> whereas every element <m>(a,b,c)\in \mathbb{Z}/5 \times \mathbb{Z}/5 \times \mathbb{Z}/3</m> has order <me>|(a,b,c)=\operatorname{lcm}(|a|, |b|, |c|)\leq 15</me> since <m>|a|, |b|\in\{1,5\}</m> and <m>|c|\in\{1,3\}</m>.
            </p>
    
            <p>
              Alternatively we could argue that the uniqueness of the FTFGAG tells us that <m>G</m> uniquely determines the elementary divisors, so two groups with distinct elementary divisors cannot be isomorphic.
            </p>
          </example>
          <exercises xml:id="exercises-decomp">
            <exercise>
              <p>
                Prove that any group of order <m>3^2\cdot 11\cdot 17</m> is abelian.
              </p>

              <proof>
                <p>
                  Let <m>G</m> be a group of order <m>3^2\cdot 11\cdot 17</m>. By Sylow's Theorem we see the following: - <m>n_{11}|153</m> and <m>n_{11}\equiv 1\mod{11}</m>, and so <m>n_{11}=1</m>. - <m>n_{17}|99</m> and <m>n_{17}\equiv 1\mod{17}</m>, and so <m>n_{17}=1</m> as well. - <m>n_3|187</m> and <m>n_3\cong 1\mod{3}</m>, so actually <m>n_3=1</m> too. Thus the unique Sylow <m>11</m>-subgroup and Sylow <m>17</m>-subgroup, denoted <m>P</m> and <m>Q</m>, respectively, are normal in <m>G</m>.
                </p>
      
                <p>
                  As <m>P</m> and <m>Q</m> are normal in <m>G</m> and intersect trivially, we see that <m>PQ\leq G</m>. Let <m>g\in G</m> and consider <m>gPQg\inv</m>. Let <m>pq\in PQ</m> and notice <m>g(pq)g\inv=gpg\inv gqg\inv</m>. As <m>p\in P\nsg G</m> and <m>q\in Q\nsg G</m> we see <m>gpg\inv\in P</m> and <m>gqg\inv\in Q</m>, thus <m>gpqg\inv\in PQ</m>, making <m>PQ\nsg G</m>.
                </p>
      
                <p>
                  Let <m>R</m> be the unique Sylow <m>3</m>-subgroup, which has order <m>9</m>. As <m>R\nsg G</m> and intersects with <m>PQ</m> trivially, we see <m>G=PQ\times R</m>, a direct product of cyclic groups of relatively prime order, making <m>G</m> abelian.
                </p>
              </proof>
            </exercise>

            <exercise>
              <p>
                Suppose <m>G</m> is a group of order <m>5 \cdot 7 \cdot 23^2</m> and that <m>G</m> contains an element of order <m>35</m>. Prove <m>G</m> is abelian.
              </p>

              <proof>
                <p>
                  By Sylow's Theorem we know the number of Sylow <m>23</m>-subgroups of <m>G</m> must divide <m>35</m> and be congruent to <m>1\mod{23}</m>, the only option of which is <m>1</m>. Let <m>H</m> denote the unique Sylow <m>23</m>-subgroup and let <m>K</m> be the cyclic subgroup generated by the element of order <m>35</m>.
                </p>
      
                <p>
                  As <m>H</m> is unique it is normal in <m>G</m>, and it also means we have <m>HK\leq G</m>. Notice that as <m>H</m> and <m>K</m> are groups of relatively prime order we have <m>H\cap K=\{e\}</m>. Thus <m>|HK|=|G|</m>, and so <m>G=HK</m>, making <m>G\cong H\times K</m>, a direct product of abelian groups. Thus <m>G</m> is abelian.
                </p>
              </proof>
            </exercise>

            <exercise>
              <p>
                Let <m>G</m> be a group of order <m>p^2q</m> where <m>p</m> and <m>q</m> are distinct primes. 
                (a) Prove that <m>G</m> contains a normal Sylow subgroup. 
                (b) Suppose <m>p &lt; q</m> and the Sylow <m>p</m>-subgroup is cyclic and normal. Prove that <m>G</m> is abelian.
              </p>

              <proof>
                <p>
                  Let <m>G</m> be a group of order <m>p^2q</m> where <m>p</m> and <m>q</m> are distinct primes. Suppose by way of contradiction that <m>G</m> has no normal Sylow <m>p</m>-subgroup.
                </p>
    
                <p>
                  First, suppose <m>p&lt;q</m>. By Sylow’s Theorem we know the following: - <m>n_p|q</m> and <m>n_p\equiv 1\mod{p}</m>, so <m>n_p=1</m> or <m>q</m>, so <m>q</m> - <m>n_q|p^2</m> and <m>n_q\equiv 1\mod{q}</m>, so <m>n_q=1</m> or <m>p^2</m>, so <m>p^2</m>. We know there must be <m>(q-1)\cdot p^2=p^2q-p^2</m> elements of order <m>p</m>. Luckily, there is more than one Sylow <m>p</m>-subgroup with <m>p^2</m> elements, so there isn’t room for all of them.
                </p>
    
                <p>
                  Suppose then that <m>q&gt;p</m>. By Sylow’s Theorem we know the following: - <m>n_p|q</m> and <m>n_p\equiv 1\mod{p}</m>, so <m>n_p=1</m>, so we’re definitely good there.
                </p>

                <p>
                  Suppose <m>p &lt; q</m> and the Sylow <m>p</m>-subgroup, <m>P</m>, is cyclic and normal. We know from Part (a) that there are either <m>1</m> or <m>p^2</m> Sylow <m>q</m>-subgroups, but since there are already <m>p^2</m> elements of order <m>p</m> there is only room for one, <m>Q</m>, which is also cyclic, given its prime power. As <m>P</m> and <m>Q</m> are thus normal in <m>G</m> and only intersect trivially, we see that <m>PQ=G</m>, meaning that <m>G=P\times Q</m>. Thus <m>G</m> is the product of two cyclic groups of relatively prime order, making <m>G</m> cyclic as well. Cyclic groups are abelian, so we are done.
                </p>
              </proof>
            </exercise>

            <exercise>
              <p>
                Suppose <m>G</m> is a finite group which has precisely one subgroup of order <m>d</m> for each divisor <m>d</m> of <m>|G|</m>. Prove that <m>G</m> is cyclic.
              </p>

              <proof>
                <p>
                  First, suppose that <m>G</m> is a <m>p</m>-group. Let <m>g\in G</m> have biggest order. Let <m>h\in G</m>. So <m>|h|\bigg||g|</m>. Since <m>(g)\leq G</m>, it also has exactly one subgroup for each divisor. But (h) has the same order as one of those subgroups, so they must be the same group. So <m>h\in(g)</m>. Since <m>h</m> was arbitrary, then <m>G\leq (g)</m>. So when <m>G</m> is a <m>p</m>-group then it is cyclic.
                </p>
      
                <p>
                  If its not a <m>p</m>-group then we can decompose <m>|G|</m> into relatively prime powers of primes, all of which are <m>p</m>-groups and maintain this property. Thus <m>G</m> is the product of relatively prime cyclic groups, making it cyclic itself.
                </p>
              </proof>
            </exercise>


          </exercises>

        </section>

        <section xml:id="sec-classifications">
          <title>Groups Up to Isomorphism</title>

          <problem>
            <statement>
            <p>
              Determine all of the groups of order <m>45</m>, up to isomorphism.
            </p>
          </statement>

            <solution>
              <p>
                By Sylow's Theorem we know the following: - <m>n_3|5</m> and <m>n_3\equiv 1\mod{3}</m>, so <m>n_3=1</m>. - <m>n_5|</m> and <m>n_5\equiv1\mod{5}</m>, so <m>n_5=1</m> as well. Thus there is exactly one Sylow <m>5</m>-subgroup, <m>P</m>, and exactly one Sylow <m>3</m>-subgroup, <m>Q</m>. Both are normal in <m>G</m>. Notice that <m>Q</m> has order <m>9</m>, a prime squared. Thus <m>Q</m> is abelian. By the FTFGAG, <m>Q</m> is either isomorphic to <m>\Z_3\times\Z_3</m> or <m>\Z_9</m>. Thus <m>G\cong\Z_5\times\Z_9</m> or <m>G\cong\Z_5\times\Z_3\times\Z_3</m>.
              </p>
            </solution>
          </problem>

          <problem>
            <statement>
            <p>
              Let <m>p</m> be any positive prime integer. Prove that the number of groups of order <m>3\cdot p</m>, up to isomorphism, is exactly <me>\begin{cases} 2, \text{ when }p = 2, p = 3, \text{ or }p\equiv 1 \mod 3, \text{ and }\\ 1, \text{ otherwise }\end{cases}</me>
            </p>
          </statement>

            <solution>
              <p>
                Let <m>p</m> be any positive prime integer.
              </p>

              <p>
                First, suppose <m>p=2</m>. Thus <m>G</m> is a group of order <m>pq</m>, making it abelian. So the only groups of order <m>6</m> are <m>\Z_2\times\Z_3</m> and <m>\Z_6</m>. The same applies when <m>p=2</m>, where the groups are <m>\Z_3\times\Z_3</m> and <m>\Z_9</m>.
              </p>
    
              <p>
                Let <m>P</m> be a Sylow <m>p</m>-subgroup of <m>G</m>, and note that <m>[G:P]=3</m>, the smallest prime dividing the order of <m>G</m>, making <m>P\nsg G</m>. Let <m>Q</m> denote a Sylow <m>3</m>-subgroup of <m>G</m>. As <m>P</m> and <m>Q</m> are groups of relatively prime order we have <m>PQ=G</m> and thus <m>G\cong P\sdp_\rho Q</m>, where <m>\rho:Q\to\Aut(P)</m>. Notice that since <m>|P|=p</m>, we have <m>\Aut(P)\cong\Z^{\times}_p\cong\Z_{p-1}</m>. Thus, by the First Isomorphism Theorem <m>Q/\ker(\rho)\cong\im(\rho)\leq\Z_{p-1}.</m> As <m>Q</m> has three elements, the kernel of <m>\rho</m> must be either all of <m>Q</m> or trivial. However, the order of the image must divide <m>\Z_{p-1}</m>, which is only possible when <m>p\equiv 1 \mod 3</m>. Thus when this is the case there are two groups of order <m>3p</m>, otherwise the kernel is always trivial and we have <m>G=P\times Q</m> as the only group.
              </p>
            </solution>
          </problem>

          <problem>
          <statement>
            <p>
              Let <m>G</m> be a group of order <m>175 = 3^2\cdot 5^2</m> and suppose <m>G</m> contains an element of order <m>25</m>. Prove that <m>G</m> is abelian.
            </p>
          </statement>

            <solution>
              <p>
                First, note that <m>3^2\cdot 5^2=225</m>, not <m>175</m>. You hate to see it. Anyway, let <m>x</m> be an element of order <m>25</m> and consider <m>H=\langle x\rangle</m>, a cyclic subgroup of order <m>25</m>. The possible number of Sylow <m>5</m>-subgroups of <m>G</m> is exactly <m>1</m>, making <m>H</m> this subgroup and thus normal in <m>G</m>. Let <m>K</m> be a Sylow <m>3</m> subgroup, it intersects <m>H</m> trivially and thus <m>G\cong H\sdp_{\rho}K</m>, where <m>\rho:K\to\Aut(H)</m>. The order of <m>\Aut(H)</m> is <m>20</m>, which is relatively prime to <m>|K|</m>, making <m>\rho</m> trivial and <m>G=H\times K</m> and thus abelian.
              </p>
            </solution>
          </problem>



          <problem>
          <statement>
            <p>
              Let <m>G</m> be a group. A subgroup <m>H</m> of <m>G</m> is called <em>maximal</em> if <m>H\neq G</m> (that is, <m>H</m> is a proper subgroup of <m>G</m>) and whenever <m>K</m> is another subgroup of <m>G</m> containing <m>H</m>, either <m>K = H</m> or <m>K = G</m>. Show that every nontrivial finitely generated group <m>G</m> possesses maximal subgroups.
            </p>
          </statement>

            <solution>
              <p>
                Let <m>G</m> be a group.
              </p>
    
              <p>
                Let <m>S</m> be the poset of all proper subgroups of <m>G</m> ordered in terms of inclusion. Consider a string of these. Consider the union of them all. Luckily, unions of subgroups are subgroups if and only if there is containment, which there is, since everything is in the union. Thus its a subgroup. Since union in <m>S</m> and yields an upper bound, by Zorn’s Lemma we a maximal element. Thus <m>G</m> possesses maximal subgroups.
              </p>
            </solution>
          </problem>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>

          
        </section>
        
      </chapter>

      <chapter xml:id="ch-gp-extras">
        <title>Extras</title>

        <section xml:id="sec-quals">
          <title>Qualifying Exams</title>

          <subsection>
            <title>Winter 2023</title>

            <subsubsection>
              <title>Group Theory</title>
              
              <problem>
                <title>Problem 1</title>
                
                
              </problem>

              <problem>
                <title>Problem 2</title>
                
                
              </problem>

              <problem>
                <title>Problem 3</title>
                
                
              </problem>
              
            </subsubsection>

            <subsubsection>
              <title>Rings, Modules, Linear Algebra</title>

              <problem>
                <title>Problem 4</title>
                
                
              </problem>

              <problem>
                <title>Problem 5</title>
                
                
              </problem>

              <problem>
                <title>Problem 6</title>
                
                
              </problem>
              
            </subsubsection>

            <subsubsection>
              <title>Fields and Galois Theory</title>

              <problem>
                <title>Problem 7</title>
                
                
              </problem>

              <problem>
                <title>Problem 8</title>
                
                
              </problem>

              <problem>
                <title>Problem 9</title>
                
                
              </problem>
              
            </subsubsection>
            
          </subsection>

          <subsection>
            <title>Summer 2023</title>

            <subsubsection>
              <title>Group Theory</title>
              
              <problem>
                <title>Problem 1</title>
                
                
              </problem>

              <problem>
                <title>Problem 2</title>
                
                
              </problem>

              <problem>
                <title>Problem 3</title>
                
                
              </problem>
              
            </subsubsection>

            <subsubsection>
              <title>Rings, Modules, Linear Algebra</title>

              <problem>
                <title>Problem 4</title>
                
                
              </problem>

              <problem>
                <title>Problem 5</title>
                
                
              </problem>

              <problem>
                <title>Problem 6</title>
                
                
              </problem>
              
            </subsubsection>

            <subsubsection>
              <title>Fields and Galois Theory</title>

              <problem>
                <title>Problem 7</title>
                
                
              </problem>

              <problem>
                <title>Problem 8</title>
                
                
              </problem>

              <problem>
                <title>Problem 9</title>
                
                
              </problem>
              
            </subsubsection>
            
          </subsection>

          <subsection>
            <title>Winter 2022</title>

            <subsubsection>
              <title>Group Theory</title>
              
              <problem>
                <title>Problem 1</title>
                
                
              </problem>

              <problem>
                <title>Problem 2</title>
                
                
              </problem>

              <problem>
                <title>Problem 3</title>
                
                
              </problem>
              
            </subsubsection>

            <subsubsection>
              <title>Rings, Modules, Linear Algebra</title>

              <problem>
                <title>Problem 4</title>
                
                
              </problem>

              <problem>
                <title>Problem 5</title>
                
                
              </problem>

              <problem>
                <title>Problem 6</title>
                
                
              </problem>
              
            </subsubsection>

            <subsubsection>
              <title>Fields and Galois Theory</title>

              <problem>
                <title>Problem 7</title>
                
                
              </problem>

              <problem>
                <title>Problem 8</title>
                
                
              </problem>

              <problem>
                <title>Problem 9</title>
                
                
              </problem>
              
            </subsubsection>
            
          </subsection>

          <subsection>
            <title>Summer 2022</title>

            <subsubsection>
              <title>Group Theory</title>
              
              <problem>
                <title>Problem 1</title>
                
                
              </problem>

              <problem>
                <title>Problem 2</title>
                
                
              </problem>

              <problem>
                <title>Problem 3</title>
                
                
              </problem>
              
            </subsubsection>

            <subsubsection>
              <title>Rings, Modules, Linear Algebra</title>

              <problem>
                <title>Problem 4</title>
                
                
              </problem>

              <problem>
                <title>Problem 5</title>
                
                
              </problem>

              <problem>
                <title>Problem 6</title>
                
                
              </problem>
              
            </subsubsection>

            <subsubsection>
              <title>Fields and Galois Theory</title>

              <problem>
                <title>Problem 7</title>
                
                
              </problem>

              <problem>
                <title>Problem 8</title>
                
                
              </problem>

              <problem>
                <title>Problem 9</title>
                
                
              </problem>
              
            </subsubsection>
            
          </subsection>
          
        </section>

        <section xml:id="sec-solve">
          <title>Solvable Groups</title>
          <p></p>
          
        </section>

        <section xml:id="sec-sn-conjugacy">
          <title>Conjugation in <m>S_n</m></title>
          
          <p></p>
        </section>

        <section xml:id="sec-smallorder">
          <title>Groups of Small Order</title>
          
            <blockquote>
              <p>
                <q>Transformation happens in small groups.</q>
              </p>
              <attribution>Gloria Steinem</attribution>
            </blockquote>

        </section>
        
      </chapter>
    </part>

    <part xml:id="part-rings">
      <title>Ring Theory</title>
    
      <xi:include href="./IntroRings.ptx" />

      <xi:include href="./IdealQuotient.ptx" />

      <xi:include href="./CommutativeRings.ptx" />

      <xi:include href="./Domains.ptx" />

    <chapter xml:id="sec-irreducible">
      <title>Irreducibility</title>

      <section xml:id="sec-roots">
        <title>Roots of Unity</title>

        <proposition xml:id="prop-">
          <statement>
            <p>
              Fix a prime number <m>p</m>, and let <m>A</m> denote the abelian group of all complex roots of unity whose orders are powers of <m>p</m>; that is 
              <me>
                A = \{z \in \C | z^{p^n} = 1 \text{ for some integer } n \leq 1\} .
              </me>
              Prove the following statements. 
              <ol>
                <li>
                  <p>
                    Every non-trivial subgroup of <m>A</m> contains the group of <m>p^{\text{th}}</m> roots of unity. 
                  </p>
                </li>

                <li>
                  <p>
                    Every proper subgroup of <m>A</m> is cyclic. 
                  </p>
                </li>

                <li>
                  <p>
                    If <m>B</m> and <m>C</m> are subgroups of <m>A</m>, then either <m>B \subseteq C</m> or <m>C \subseteq B</m>. 
                  </p>
                </li>

                <li>
                  <p>
                    For each <m>n \geq 0</m> there exists a unique subgroup of <m>A</m> with <m>p^n</m> elements.
                  </p>
                </li>
              </ol>
            </p>
          </statement>

          <proof>
            <p>
              Let <m>H</m> be a non-trivial subgroup of <m>A</m>. Then there exists some <m>z</m> such that <m>z^{p^n}=1</m>. Then <m>z^{p^{n-1}}</m> yields a primitive <m>p\th</m> root of unity, which can be used to generate the other roots as well. Thus <m>H</m> contains the <m>p\th</m> roots of unity.
            </p>

            <p>
              Suppose <m>H</m> is a proper subgroup of <m>A</m>, meaning it is missing some <m>p^{n{\th}}</m> root of unity. But the subgroup of those roots of unity is cyclic and is generated by every element, so that entire subgroup must be missing. But that subgroup can be generated with any primitive root of a higher power of <m>p</m>, so <m>H</m> must be finite and there must be some element of maximum order, which can be used to generate the whole group. Thus <m>H</m> is cyclic.
            </p>

            <p>
              Suppose <m>B</m> and <m>C</m> are subgroups of <m>A</m> such that <m>B\not\subseteq C</m>. Then <m>C</m> is a proper subgroup of <m>A</m>, making it finite as seen above. If there exists a higher power of <m>p</m> in <m>B</m> then it generates <m>C</m>.
            </p>

            <p>
              Let <m>n\geq 0</m>. Then the subgroup generated by the <m>p^{n^{\th}}</m> roots of unity have <m>p^n</m> elements, and it is unique since it is generated by every such root.
            </p>
          </proof>
        </proposition>

        <exercises>

          <exercisegroup>
            <title>Computations and Examples</title>
            <introduction>
              <p>
              </p>
            </introduction>
        
            <exercise>
              <title></title>
              
              
              <statement>
                <p>
                  Coming soon to an OER near you!
                </p>
              </statement>

              <hint>
                <p>
                  Coming soon to an OER near you!
                </p>
              </hint>

              <solution>
                <p>
                  Coming soon to an OER near you!
                </p>
              </solution>
            </exercise>
        
          </exercisegroup>
        
          <exercisegroup>
            <title>Formal Proofs</title>
            <introduction>
              <p>
              </p>
            </introduction>
        
            <exercise>
              <title></title>
              
              
              <statement>
                <p>
                  Coming soon to an OER near you!
                </p>
              </statement>

              <hint>
                <p>
                  Coming soon to an OER near you!
                </p>
              </hint>

              <solution>
                <p>
                  Coming soon to an OER near you!
                </p>
              </solution>
            </exercise>
        
          </exercisegroup>
        
          <exercisegroup>
            <title>Qualifying Exam Problems</title>
            <introduction>
              <p>
              </p>
            </introduction>
        
            <exercise>
              <title></title>
              
              
              <statement>
                <p>
                  Coming soon to an OER near you!
                </p>
              </statement>

              <hint>
                <p>
                  Coming soon to an OER near you!
                </p>
              </hint>

              <solution>
                <p>
                  Coming soon to an OER near you!
                </p>
              </solution>
            </exercise>
        
          </exercisegroup>
        </exercises>

      </section>

      <section xml:id="sec-irrpoly">
        <title>Irredicuble Polynomials</title>

        <definition xml:id="def-irreducible-polynomial">
          <statement>
            <p>
              For an integral domain <m>R</m>, a polynomial <m>p(x) \in R[x]</m> is called <em>irreducible</em> if <m>p(x)</m> is not a unit and whenever <m>p(x) = f(x) g(x)</m>, either <m>f(x)</m> or <m>g(x)</m> is a unit. (It follows that such a <m>p(x)</m> also cannot be <m>0</m>.)
            </p>
          </statement>
        </definition>

        <theorem xml:id="thm-degree-and-irreducibility">
          <statement>
            <p>
              Let <m>F</m> be a field and <m>p(x) \in F[x]</m>. 
              <ol>
                <li>
                  <p>
                    If <m>p</m> has degree one, it is irreducible. 
                  </p>
                </li>

                <li>
                  <p>
                    If <m>p</m> has a root <m>a \in F</m> and <m>\deg(p) \geq 2</m>, then <m>p(x)</m> is not irreducible (since it factors as <m>p = (x-a)q</m> for some <m>q</m> of degree at least <m>1</m>). 
                  </p>
                </li>

                <li>
                  <p>
                    If <m>2 \leq \deg(p) \leq 3</m>, then <m>p(x)</m> is irreducible if and only if <m>p(x)</m> has no roots. 
                  </p>
                </li>

                <li>
                  <p>
                    (Rational Root Test) If <m>p(x) \in \Q[x]</m> and all the coefficients of <m>p = a_n x^n + \cdots + a_0</m> are integers and <m>\frac{i}{j}</m> is a root of <m>p</m> with <m>i,j \in \Z</m>, then <m>j</m> divides <m>a_n</m> and <m>i</m> divides <m>a_0</m>. More generally, the same holds with <m>\Z</m> replaced by any PID and <m>\Q</m> replaced by its field of fractions.
                  </p>
                </li>
              </ol>
            </p>
          </statement>
        </theorem>

        <remark>
          <p>
						Never, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever, ever try to use the converse of (2) or a version of (3) for polynomials of degree more than <m>3</m>; they are false.
					</p>
        </remark>

        <example xml:id="ex-does-it-have-roots">
          <p>
						Does <m>x^4 + 8x^3 + 21x^2 + 20x + 13 \in \Q[x]</m> have any roots? No. The only possible roots are <m>\pm 1</m> and <m>\pm 13</m>, and a careful check rules these out. Is <m>x^4 + 8x^3 + 21x^2 + 20x + 13 \in \Q[x]</m> irreducible? No, it factors as <m>x^2 + x + 1</m> times <m>x^2 + 7x + 13</m>
					</p>
        </example>

        <definition xml:id="def-content">
          <statement>
            <p>
              For a PID <m>R</m> and <m>f \in R[x]</m>[^1], define the <em><m>\defnn{content}</m></em> of <m>f</m>, written <m>\cont(f)</m>, to be the gcd of the coefficients of <m>f</m>. Equivalently, <m>\cont(f)</m> is the generator of the principal ideal generated by the coefficients of <m>f</m>.
            </p>
          </statement>
        </definition>

        <theorem xml:id="thm-gausss-lemma-part-1">
          <title>Gauss’s Lemma: Part 1</title>
          <statement>
            <p>
              Let <m>f(x), g(x) \in R[x]</m>[^1] where <m>R</m> is a PID. Then <m>\cont(fg) = \cont(f) \cont(g)</m>[^2] (up to associates).
            </p>
          </statement>

          <proof>
            <p>
              We first show the following special case: If <m>\cont(f) = 1</m> and <m>\cont(g) = 1</m>, then <m>\cont(fg) =1</m>.
            </p>

            <p>
							To see this, pick a prime <m>p \in R</m> and write <m>\overline{f} \in (R/p)[x]</m> for the result of modding out the coefficients of <m>f</m> by <m>p</m>. Then <m>\ov{fg} = \ov{f}\ov{g}</m> (since the map <m>R[x] \to (R/p)[x]</m> sending <m>f</m> to <m>\ov{f}</m> is a ring map). Since <m>\cont(f) =1 = \cont(g)</m>, we have that <m>\ov{f} \ne 0</m> and <m>\ov{g} \ne 0</m>. Since <m>(R/p)[x]</m> is a domain, it follows that <m>\ov{fg} \ne 0</m>. This proves <m>p</m> does not divide all of the coefficients of <m>fg</m>. Since <m>p</m> was arbitrary, <m>\cont(fg) = 1</m>.
						</p>

						<p>
							For the general case, let <m>f = a h(x)</m> and <m>g = b l(x)</m> where <m>a =\cont(f)</m> and <m>b = \cont(g)</m>. Note that <m>\cont(h) = 1 = \cont(l)</m> and <m>fg = (ab) hl</m>. By the case already proven, <m>\cont(lh) =1</m>. It follows <m>\cont (abhl) =ab \cont(hl) = ab = \cont(f) \cont(g)</m>.
						</p>
          </proof>
        </theorem>

        <theorem xml:id="thm-gausss-lemma-part-2">
          <title>Gauss’s Lemma: Part 2</title>
          <statement>
            <p>
              Let <m>R</m> be a PID and let <m>F</m> be its field of fractions, and assume <m>f(x) \in R[x]</m>[^1] is a non-constant polynomial with coefficients in <m>R</m>. <m>f(x)</m> is irreducible in <m>R[x]</m> if and only if it is irreducible in <m>F[x]</m> and <m>\cont(f) = 1</m>.
            </p>
          </statement>

          <proof>
            <p>
							Suppose <m>f = gh</m> with <m>g, h \in F[x]</m>. We can find an element <m>M</m> of <m>R</m> such that <m>M \cdot g(x) \in R[x]</m>. (We can take <m>M</m> to be the product of all the denominators of the coefficients of <m>g</m>, for example; this is called “clearing denominators’’.) Similarly, there is an <m>N \in R</m> such that <m>N \cdot h(x) \in R[x]</m>. Further, write <m>M \cdot g(x) = a \cdot l(x)</m> where <m>a = \cont(M g(x)) \in R</m>, <m>l(x) \in R[x]</m>, and <m>\cont(l) = 1,</m> and similarly <m>N \cdot g(x) = b \cdot m(x)</m> where <m>b = \cont(N h(x)) \in R</m>, <m>m(x) \in R[x]</m>, and <m>\cont(m) = 1</m>. We have 
              <me>
                abl(x)m(x) = MN f(x)
              </me>
              Using Gauss’s Lemma, Part 1, we have <m>\cont(lm) = \cont(l) \cont(m) = 1</m> and hence 
              <me>
                \cont(ab l(x) m(x)) = ab \cont(l(x) m(x)) = ab.
              </me> 
              We also have <m>\cont(MNf(x)) = MN\cont(f) = MN</m> (since we’ve already proven that <m>\cont(f) = 1</m>).
						</p>

						<p>
							So <m>ab = MN</m> and thus by dividing we arrive at 
              <me>
                l(x) m(x) = f(x).
              </me> 
              But <m>f(x)</m> is irreducible in <m>R[x]</m>. It follows that either <m>l</m> or <m>m</m> must be a unit in <m>R[x]</m> (i.e., a unit in <m>R</m>) and it follows that either <m>g(x)</m> or <m>h(x)</m> is a unit in <m>F[x]</m>. This proves <m>f(x)</m> is irreducible in <m>F[x]</m>.
						</p>

						<p>
							(<m>\Leftarrow</m>) Say <m>f(x) \in R[x]</m> irreducible in <m>F[x]</m> and <m>\cont(f) = 1</m>. If <m>f = q(x) r(x)</m> with <m>q,r \in R[x]</m> then, since <m>f</m> is irreducible in <m>F[x]</m>, one of <m>q</m> or <m>r</m> must be a constant. But since <m>\cont(f) = 1</m>, this constant must be <m>1</m>.
						</p>

          </proof>
        </theorem>

        <example xml:id="ex-irreducibility-and-gauss">
          <p>
						Let’s prove <m>f = x^4 + 7x^3 + 16 x^2 + 124 x + 23 \in \Q[x]</m> is irreducible. By Gauss’s Lemma, we just need to show it is irreducible in <m>\Z[x]</m>.
					</p>

					<p>
						If <m>f</m> did factor in <m>\Z[x]</m> as a product of monic polynomials, then <m>\ov{f} \in \Z/2[x]</m> would also factor in this way. Now <m>\ov{f} = x^4 + x^3 + 1 \in \Z/2[x]</m>. This has no roots (the only possibilities are <m>0</m> or <m>1</m>) and so if it did factor it would have to factor as a product of two quadratic, irreducible polynomials. The only such polynomial in <m>\Z/2[x]</m> is <m>q = x^2+ x + 1</m> (the others all have roots). But <m>q^2 \ne \ov{f}</m>. This proves <m>\ov{f}</m> is irreducible in <m>(\Z/2)[x]</m> and hence <m>f</m> must be irreducible in <m>\Z[x]</m>.
					</p>

        </example>

        <theorem xml:id="thm-eisensteins-criterion">
          <title>Eisenstein’s Criterion</title>
          
          
          <statement>
            <p>
              Let <m>R</m> be a PID with field of fractions <m>F</m> and assume <m>f(x) = a_n x^n + a_{n-1} x^{n-1} + \cdots + a_0 \in R[x]</m>[^1] is a polynomial. Suppose there is a prime element <m>p \in R</m> such that <m>p \nmid a_n</m>, <m>p \mid a_i</m>[^2] for all <m>0 \leq i \leq n-1</m>, and <m>p^2 \nmid a_0</m>. Then <m>f</m> is irreducible in <m>F[x]</m>.
            </p>
          </statement>

          <proof>
            <p>
            We have <m>f(x) = \cont(f) \cdot l(x)</m> with <m>l(x) \in R[x]</m> and <m>\cont(l) = 1</m>. Since <m>p</m> does not divide the leading coefficient of <m>f</m>, we have <m>p \nmid \cont(f)</m>. It therefore follows that the three assumptions involving the coefficients of <m>f(x)</m> are also satisfied by the coefficients <m>l(x)</m>. Moreover, <m>f</m> is irreducible in <m>F[x]</m> if and only if <m>l(x)</m> is irreducible in <m>F[x]</m>. We may therefore assume without loss of generality that <m>\cont(f) = 1</m>.
            </p>

            <p>
              By Gauss’s Lemma we just need to prove it is irreducible in <m>R[x]</m>. Suppose <m>f(x) = g(x)h(x)</m> with <m>g(x), h(x) \in R[x]</m>, where <me>g(x) = b_m x^m + b_{m-1} x^{m-1} + \cdots + b_0</me>and <me>h(x) = c_l x^l + c_{l-1} x^{l-1} + \cdots + c_0</me>where <m>m+l = n</m> and <m>b_m c_l = a_n</m>.
            </p>

            <p>
              Upon modding out by <m>p</m> we get <me>\ov{f} = \ov{g} \ov{h} \in (R/p)[x].</me>The assumptions on <m>f</m> give that <me>\ov{f} = \ov{a_n} x^n</me>with <m>\ov{a_n} \ne 0</m>. Since <m>R/p</m> is a domain, it follows that <me>\ov{g} = \ov{b_m} x^m \, \text{ and  } \, \ov{h} = \ov{c_l} x^l</me>On the other hand, we also have <me>a_0 = b_0c_0,</me>and since <m>p^2 \nmid a_0</m>, we have that <m>p \nmid b_0</m> or <m>p \nmid c_0</m>. So <m>\ov{b_0} \ne 0</m> or <m>\ov{c_0} \ne 0</m>. This can only occur if <m>m = 0</m> or <m>l = 0</m>.
            </p>

            <p>
              We have shown that if <m>f</m> factors in <m>R[x]</m> as <m>f = g \cdot h</m>, then at least one of <m>g</m> of <m>h</m> must be a constant polynomial. Since <m>\cont(f) = 1</m>, this factor must be a unit. So <m>f</m> is irreducible in <m>R[x]</m>.
            </p>
          </proof>
        </theorem>

        <example xml:id="ex-using-eisenstein">
          <p>
						For instance, <m>5x^{11} - 30 x^4 +60 x^3 +360 x- 30</m> is irreducible in <m>\Q[x]</m> thanks to Eisenstein applied with <m>p = 2</m>. (Note that it isn’t irreducible in <m>\Z[x]</m> since it does not have unit content.)
					</p>
        </example>

        <proposition xml:id="prop-cyclotomic-polynomial-irreducible">
          <statement>
            <p>
              For any prime <m>p</m>, the <m>p</m>-th <em>cyclotomic polynomial</em> <me>f(x) = x^{p-1} + x^{p-2} + \cdots + x^2 + x+1 \in \Z[x]</me>is irreducible in <m>\Q[x]</m>[^1].
            </p>
          </statement>

          <proof>
            <p>
							Consider the ring isomorphism <m>\phi: \Q[x] \xra{\cong} \Q[y]</m> given by <m>\phi(h(x)) = h(y+1)</m>. I claim that 
              <me>
                \phi(f(x)) = y^{p-1} + p y^{p-2} + {p \choose 2} y^{p-3} +{p \choose 3} y^{p-4} +\cdots + {p \choose p-1} y + p.
              </me> 
              To see this, we note that <m>f(x) (x-1) = x^p - 1</m> and by the binomial theorem we have 
              <me>
                \phi(x^p-1) = (y+1)^p - 1 = y^p + p y^{p-1} + {p \choose 2} y^{p-2} + \cdots + py.
              </me> 
              Since <m>\phi(x^p-1) = \phi(f(x)) \phi(x-1) = \phi(f(x)) y</m>, the claim follows.
						</p>

						<p>
							By Eisenstein, <m>\phi(f(x))</m> is irreducible in <m>\Q[y]</m> and, since <m>\phi</m> is an isomorphism, it follows that <m>f(x)</m> is irreducible in <m>\Q[x]</m>.
						</p>
          </proof>
        </proposition>

        <proposition xml:id="prop-primes-and-irreducibility">
          <statement>
            <p>
              Assume <m>R</m> is an integral domain and <m>f(x) \in R[x]</m> is a monic polynomial with coefficients in <m>R</m>. If there is a prime element <m>p \in R</m> such that <m>\ov{f} \in (R/p)[x]</m> is irreducible in <m>(R/p)[x]</m>, then <m>f</m> is irreducible in <m>R[x]</m>.
            </p>
          </statement>

          <proof>
            <p>
							We prove the contra-positive. Suppose <m>f</m> is reducible in <m>R[x]</m>. Then we have <m>f(x) = g(x) h(x)</m> for some monic, non-constant polynomials <m>g</m> and <m>h</m> in <m>R[x]</m>. It follows that <m>\ov{f} = \ov{gh} = \ov{g} \ov{h}</m> holds in <m>(R/p)[x]</m>. But since <m>g</m> and <m>h</m> are monic, non-constant polynomials, <m>\ov{g}</m> and <m>\ov{h}</m> are both non-constant polynomials in <m>(R/p)[x]</m> and hence <m>\ov{f}</m> is reducible in <m>(R/p)[x]</m>.
						</p>
          </proof>
        </proposition>

        <example xml:id="ex-monic-is-necessary">
          <p>
						The assumption that <m>f</m> be monic in this Proposition is necessary. Consider for instance <m>f(x) = (3x+1)(x+1) = 3 x^2 + 4x + 1 \in \Z[x]</m>. Modding out by <m>3</m> yields a linear polynomial in <m>(\Z/3)[x]</m> which is thus irreducible. But clearly <m>f</m> isn’t irreducible in <m>\Z[x]</m>. (The Proposition can be generalized to non-monic polynomials by adding the assumption that <m>p</m> does not divided the leading coefficient of <m>f</m>.)
					</p>
        </example>

        <proposition xml:id="prop-fields-and-group-of-units">
          <statement>
            <p>
              Let <m>F</m> be a field and <m>G</m> a finite subgroup of the multiplicative group <m>F^\times</m>. Prove that <m>G</m> is cyclic.
            </p>
          </statement>

          <proof>
            <p>
							Let <m>F</m> be a field and <m>G</m> a finite subgroup of order <m>n</m> the multiplicative group <m>F^\times</m>. Let <m>k</m> denote the LCM of all orders of elements in <m>G</m>. Notice that as <m>g^n=1</m> for all <m>g\in G</m> we have <m>k\leq n</m>.
						</p>

						<p>
							However, as <m>g^k=1</m> for all <m>g\in G</m> we know that <m>g</m> is the root of the polynomial <m>f(x)=x^k-1</m> in <m>F[x]</m>. By the [[Mathematics/Number Theory/Results/Lemma – Factor Theorem|Factor Theorem]] there are thus at most <m>k</m> roots of <m>f</m>, but there are <m>n</m> distinct roots. Thus <m>k=n</m>.
						</p>

						<p>
							Thus the LCM of all orders of elements in <m>G</m> is <m>n</m>. Notice that as <m>G</m> is abelian[^2] every Sylow <m>p</m>-subgroup of <m>G</m> is normal,[^3] and thus there is only one of each[^4]. This means that <m>G</m> can be written as a direct product of cyclic groups of relatively prime order;[^5] Hence <m>G</m> is itself cyclic.
            </p>
          </proof>
        </proposition>

        <proposition xml:id="prop-gaussian-irreducible">
          <statement>
            <p>
              Let <m>\Q(i) = \{a + bi \mid a, b \in \Q\}</m> and <m>\Z[i] = \{a + ib \mid a, b \in \Z\}</m> and recall (from 817) that <m>\Z[i]</m> is a PID and <m>\Q(i)</m> is its field of fractions. 
              <ol>
                <li>
                  <p>
                    Prove <m>x^6 + 7</m> is irreducible in <m>\Q(i)[x]</m>. 
                  </p>
                </li>

                <li>
                  <p>
                    Prove <m>x^4 - 6x^2 + 10</m> is irreducible in <m>\Q[x]</m> but is reducible in <m>\Q(i)[x]</m>.
                  </p>
                </li>
              </ol>
            </p>
          </statement>

          <proof>
            <p>
							Let <m>\Q(i) = \{a + bi \mid a, b \in \Q\}</m> and <m>\Z[i] = \{a + ib \mid a, b \in \Z\}</m>.
						</p>

            <p>
              Suppose by way of contradiction that <m>7</m> is not prime in <m>\Z[i]</m>. Thus <m>7=\alpha\beta</m>, with <m>\alpha,\beta</m> non-units and where <m>\alpha=a+bi</m> and <m>\beta=c+di</m>, for <m>a,b,c,d\in Z</m>. Thus <m>\overline{7}=\overline{\alpha\beta}=\overline{\alpha}\overline{\beta}</m>. As <m>\overline{7+0i}=49</m>, we see that <m>\overline{\alpha}=(a^2+b^2)|49</m>. Note that the possible factorizations of 49 in <m>\Z</m> are <m>(1)(49)</m> and <m>(7)(7)</m>.
            </p>

            <p>
              However, if <m>\overline{\alpha}=49</m> then <m>\overline{\beta}=1</m>, making it a unit. So <m>\overline{\alpha}=7</m> and thus <m>a^2+b^2=7</m>. As <m>a^2, b^2\leq0</m>, either <m>a=0</m> or <m>b=0</m>. But then either <m>a^2=7</m> or <m>b^2=7</m>, and as <m>\sqrt{7}\not\in\Z</m>, we have a contradiction. Thus <m>7</m> is prime in <m>\Z[i]</m>. Using Eisenstein’s Criterion with <m>p=7</m>, we see that <m>x^6 + 7</m> is irreducible in <m>\Q(i)[x]</m>.
            </p>

            <p>
              Using Eisenstein’s Criterion with <m>p=2</m>, we see that <m>x^4 - 6x^2 + 10</m> is irreducible in <m>\Q[x]</m>. However, in <m>\Q(i)[x]</m> observe that <me>x^4-6x^2+10=(x^2-(3+i))(x^2-(3-i)).</me> As neither <m>(x^2-(3+i))</m> nor <m>(x^2-(3-i))</m> are units in <m>\Q(i)[x]</m> (as <m>x</m> has no inverse), we see that <m>x^4-6x^2+10</m> is reducible in <m>\Q(i)[x]</m>.
            </p>
          </proof>
        </proposition>

        <proposition xml:id="prop-irreducible-poly">
          <statement>
            <p>
              Show that <m>x^3+3x+2 \in \Q[x]</m> is irreducible.
            </p>
          </statement>
        </proposition>

        <proposition xml:id="prop-construct-field-of-size-16">
          <statement>
            <p>
              <ol>
                <li>
                  Let <m>F</m> be a field and <m>f\subseteq F [x]</m>[^1] be irreducible. Prove the <m>F [x]/(f )</m>[^2] is a field.
                </li>
    
                <li>
                  Give an explicit construction (with justification) of a field of size <m>16</m>. (You may use without proof that the unique irreducible quadratic in <m>\F_2[x]</m> is <m>x^2 + x + 1.)</m>
                </li>
              </ol>
            </p>
          </statement>

          <proof>
            <p>
							Let <m>F</m> be a field and <m>f\subseteq F [x]</m> be irreducible. Thus <m>f</m> is prime[^3], making <m>(f)</m> a prime ideal in <m>F[x]</m>[^4] and <m>F[x]/(f)</m> a domain[^5]. Suppose that there existed a proper ideal <m>I</m> such that <m>(f)\subseteq I</m>. However, as <m>F</m> is a field we know that <m>F[x]</m> is a PID[^6], meaning <m>I=(g)</m> for some <m>g\in F[x]</m>[^7]. If <m>f\in(g)</m> then <m>g|f</m>[^8], meaning that <m>f=g</m>. Thus <m>(f)</m> is a maximal ideal and we have <m>F[x]/(f)</m> a field[^9].
						</p>

            <p>
              Let <m>F=\F_2</m>. Let <m>f=x^4+x+1</m>. Suppose <m>f=gh</m> with <m>g,h</m> irreducible. Thus either both <m>g</m> and <m>h</m> have degree <m>2</m> or one of them has degree <m>1</m>. As <m>f\neq (x^2+x+1)^2</m>, we see that without loss of generality <m>\deg g=1</m>. Then <m>g=x</m> or <m>x+1</m>, but neither divide <m>f</m>. Thus <m>f</m> is irreducible, making <m>F[x]/(f)</m> a field, the elements of which are: 1. 0 2. 1 3. <m>x</m> 4. <m>x+1</m> 5. <m>x^2</m> 6. <m>x^2+1</m> 7. <m>x^2+x</m> 8. <m>x^2+x+1</m> 9. <m>x^3</m> 10. <m>x^3+1</m> 11. <m>x^3+x</m> 12. <m>x^3+x^2</m> 13. <m>x^3+x+1</m> 14. <m>x^3+x^2+1</m> 15. <m>x^3+x^2+x</m> 16. <m>x^3+x^2+x+1</m>, making it a field of order <m>16</m>.
            </p>
          </proof>
        </proposition>

        <proposition xml:id="prop-fields-and-group-of-units-1">
          <statement>
            <p>
              Let <m>F</m> be a field and <m>G</m> a finite subgroup of the multiplicative group <m>F^\times</m>. Prove that <m>G</m> is cyclic.
            </p>
          </statement>

          <proof>
            <p>
							Let <m>F</m> be a field and <m>G</m> a finite subgroup of order <m>n</m> the multiplicative group <m>F^\times</m>. Let <m>k</m> denote the LCM of all orders of elements in <m>G</m>. Notice that as <m>g^n=1</m> for all <m>g\in G</m> we have <m>k\leq n</m>.
						</p>

						<p>
							However, as <m>g^k=1</m> for all <m>g\in G</m> we know that <m>g</m> is the root of the polynomial <m>f(x)=x^k-1</m> in <m>F[x]</m>. By the [[Mathematics/Number Theory/Results/Lemma – Factor Theorem|Factor Theorem]] there are thus at most <m>k</m> roots of <m>f</m>, but there are <m>n</m> distinct roots. Thus <m>k=n</m>.
						</p>

						<p>
							Thus the LCM of all orders of elements in <m>G</m> is <m>n</m>. Notice that as <m>G</m> is abelian[^2] every Sylow <m>p</m>-subgroup of <m>G</m> is normal,[^3] and thus there is only one of each[^4]. This means that <m>G</m> can be written as a direct product of cyclic groups of relatively prime order;[^5] Hence <m>G</m> is itself cyclic.[^6]
            </p> 
          </proof>
        </proposition>

        <proposition xml:id="prop-fields-and-group-of-units-2">
          <statement>
            <p>
              Let <m>F</m> be a field and <m>F^*</m> its group of units.
            </p>

            <p><ol>
							<li>
							Prove that any finite subgroup of <m>F^*</m> is cyclic.
							</li>

							<li>
							Suppose that <m>F</m> is algebraically closed that has characteristic <m>p&gt;0</m>. For any positive integer <m>n</m>, prove that <m>F^*</m> has a subgroup of order <m>n</m> if and only if <m>p</m> does not divide <m>n</m>.
							</li>

						</ol></p>
          </statement>

          <proof>
            <p>
							Let <m>F</m> be a field and <m>F^*</m> its group of units.
						</p>

            <p>
              Let <m>G</m> be a finite subgroup of <m>F^*</m>. Let <m>|G|=n</m>.
            </p>

            <p>
              Let <m>k</m> be the LCM of all orders of elements in <m>G</m>. Then <m>g^k=1</m> and thus <m>g</m> is a root of the polynomial <m>x^k-1</m> for all <m>g\in G</m>. By Lagrange’s Theorem every element divides <m>n</m>, and so we have <m>k\leq n</m>. However, by the Factor Theorem the polynomial <m>x^k-1</m> can have at most <m>k</m> roots, and we have <m>n</m> distinct elements, and thus we have <m>k=n</m>. Thus there must exist an element of order <m>n</m> in <m>G</m>, making <m>G</m> cyclic, as desired.
            </p>

            <p>
              Let <m>n\in\N</m> and suppose that <m>F</m> has characteristic <m>p&gt;0</m> and is algebraically closed.
            </p>

            <p>
              First, suppose by way of contradiction that <m>F^*</m> has a subgroup of order <m>n</m>, <m>G</m>, and <m>p|n</m>. From Part (a) <m>G</m> is cyclic and generated by some <m>x</m> such that <m>|x|=n</m>. However, as <m>p|n</m> we see that <m>x^p=0</m>, given that we are in an additive group. This contradicts the fact that <m>n</m> is the smallest number such that <m>x^n=0</m>.
            </p>

            <p>
              Now we proceed via the contrapositive. Suppose <m>F^*</m> does not have a subgroup of order <m>n</m>. Then there cannot exist a unit <m>x</m> such that <m>|x|=n</m>. Consider the polynomial <m>x^n-1</m>. As <m>F</m> is algebraically closed there exists some root <m>r\in F</m>. (how do we know this isn’t 1???) Notice that this means <m>r^n=1</m> and so <m>r</m> is a unit in <m>F</m>. As <m>r</m> cannot have order <m>n</m>, it must have an order that divides <m>n</m>. We also know that <m>r^p=1</m> as we are in a field of characteristic <m>p</m>. Thus <m>|r|</m> either divides <m>p</m> or is <m>p</m>. Either there is a non-identity root of <m>x^n-1=(x-1)^n</m>. Note that in this case <m>n&gt;p</m> as we are...
            </p>
          </proof>
        </proposition>

        <exercises>

          <exercisegroup>
            <title>Computations and Examples</title>
            <introduction>
              <p>
              </p>
            </introduction>
        
            <exercise>
              <title></title>
              
              
              <statement>
                <p>
                  Coming soon to an OER near you!
                </p>
              </statement>

              <hint>
                <p>
                  Coming soon to an OER near you!
                </p>
              </hint>

              <solution>
                <p>
                  Coming soon to an OER near you!
                </p>
              </solution>
            </exercise>
        
          </exercisegroup>
        
          <exercisegroup>
            <title>Formal Proofs</title>
            <introduction>
              <p>
              </p>
            </introduction>
        
            <exercise>
              <title></title>
              
              
              <statement>
                <p>
                  Coming soon to an OER near you!
                </p>
              </statement>

              <hint>
                <p>
                  Coming soon to an OER near you!
                </p>
              </hint>

              <solution>
                <p>
                  Coming soon to an OER near you!
                </p>
              </solution>
            </exercise>
        
          </exercisegroup>
        
          <exercisegroup>
            <title>Qualifying Exam Problems</title>
            <introduction>
              <p>
              </p>
            </introduction>
        
            <exercise>
              <title></title>
              
              
              <statement>
                <p>
                  Coming soon to an OER near you!
                </p>
              </statement>

              <hint>
                <p>
                  Coming soon to an OER near you!
                </p>
              </hint>

              <solution>
                <p>
                  Coming soon to an OER near you!
                </p>
              </solution>
            </exercise>
        
          </exercisegroup>
        </exercises>
        
      </section>

  
    </chapter>

    <chapter xml:id="ch-ring-extras">
      <title>Extras</title>

      <section xml:id="sec-gpring">
        <title>Group Rings</title>
        
      </section>
      
    </chapter>

    </part>

    <part xml:id="part-modules">
      <title>Module Theory</title>

      <chapter xml:id="ch-mod">
        <title>Modules</title>

        <section xml:id="sec-defmod">
          <title>Module Basics</title>


          <subsection xml:id="module-basics">
            <title>Module Basics</title>
      
      
            <paragraphs xml:id="defn-module">
              <title><m>\defn</m> – Module</title>
      
              <p>
                Let <m>R</m> be a ring (with <m>1</m>). A left <m>R</m>-<m>\defnn{module}</m> is an abelian group <m>(M,+)</m> together with a pairing <m>R \times M \to M</m>, written <m>(r,m) \mapsto rm</m>, such that for all <m>r,s \in R</m> and <m>m,n \in M</m> 1. <m>(r + s)m = rm + sm</m>, 2. <m>(rs)m = r(sm)</m>, 3. <m>r(m + n) = rm + rn</m>, and 4. <m>1m = m</m>.
              </p>
      
      
              <paragraphs xml:id="rem-11">
                <title><m>\rem</m></title>
      
                <p>
                  If <m>R</m> is a commutative ring, then any left <m>R</m>-module <m>M</m> may be regarded as a right <m>R</m>-module by setting <m>m r = r m</m>. Likewise, any right <m>R</m>-module may be regarded as a left <m>R</m>-module. But for non-commutative rings, left and right modules are not the same: trying to make a left <m>R</m>-module <m>M</m> into a right one by setting <m>mr = rm</m> fails to satisfy the second axiom, since <m>m(rs) = (rs)m</m> and <m>(mr)s = (rm)s = s(rm) = (sr)m</m>, and, unless <m>rs =sr</m>, we cannot conclude that <m>m(rs) = (mr)s</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-opposite-ring">
                <title><m>\exe</m> – Opposite Ring</title>
      
                <p>
                  Given a ring <m>R</m>, let <m>R^{op}</m>, the <sq>opposite ring</sq>, denote the same set with same rule for <m>+</m>, but with multiplication redefined as <m>a \cdot_{op} b := b a</m> (where <m>ba</m> on the right is the original multiplication ring). Then <m>R^{op}</m> is also a ring – note that it coincides with <m>R</m> if and only if <m>R</m> is commutative. Given a left <m>R</m>-module <m>M</m>, show that <m>M</m> is a right <m>R^{op}</m>-module via the rule for scaling given by <m>m r := rm</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-3">
                <title>Problem</title>
      
                <p>
                  Given a ring <m>R</m>, let <m>R^{op}</m> denote the “opposite ring’’. This is the same underlying set as <m>R</m> equipped with the same rule for <m>+</m> as <m>R</m>, but with multiplication rule (which I will write here as <m>\cdot_{op}</m>) redefined to be <m>a \cdot_{op} b := b a</m> (where <m>ba</m> refers to the original multiplication rule for <m>R</m>). Then <m>R^{op}</m> is also a ring — you don’t need to prove that.
                </p>
      
                <p>
                  Given a left <m>R</m>-module <m>M</m>, prove that <m>M</m> is a right <m>R^{op}</m>-module via the same rule for addition but with the rule for scaling on the right defined to be <m>m r := rm</m> for any <m>r \in R</m> and <m>m \in M</m>. #### <m>\lem</m> – Arithmetic in Modules Let <m>R</m> be a ring and let <m>M</m> be a (left) <m>R</m>-module. Then for all <m>r \in R</m> and <m>m \in M</m> we have 1. <m>0_Rm = 0_M</m>, 2. <m>r 0_M = 0_M</m>, 3. <m>(-1_R)m = -m</m>, and 4. <m>(-r)m = -(rm)</m> .
                </p>
      
      
                <paragraphs xml:id="proof.-69">
                  <title><em>Proof.</em></title>
      
                  <p><ol>
                    <li>
                    For the first, <m>0m = (0+0)m = 0m + 0m</m> and hence <m>0m = 0</m>.
                    </li>
      
                    <li>
                    For the second, <m>0_Rm = (0_R + 0_R)m = 0_Rm + 0_Rm</m> and hence <m>0_Rm = 0_M</m> since <m>M</m> is an abelian group.
                    </li>
      
                    <li>
                    For the third, <m>m + (-1)m = 1 m + (-1)m = (1 + (-1))m = (1 - 1)m = 0_Rm = 0_M</m> (using the second) and hence <m>(-1)m</m> is the additive inverse of <m>m</m>.
                    </li>
      
                    <li>
                    Finally, <m>(-r)m = ((-1)r)(m) = (-1)(rm) = -(rm)</m> (using the fact that <m>(-1)r = -r</m> holds in any ring and the previous result).
                    </li>
      
                  </ol></p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="ex-modules">
                <title><m>\ex</m> – Modules</title>
      
                <p><ul>
                  <li>
                  For any ring <m>R</m>, the trivial module is <m>0=\{0\}</m> with <m>r0=0</m> for any <m>r\in R</m>.
                  </li>
      
                  <li>
                  Every ring <m>R</m> is a left module over itself whit the rule for scaling given by the ring multiplication rule. It is also a right module over itself.
                  </li>
      
                  <li>
                  More generally, if <m>R</m> is any ring and <m>I</m> is a left ideal, then <m>I</m> is a left-<m>R</m>-module.
                  </li>
      
                  <li>
                  Let <m>F</m> be a field and <m>R = \Mat_{n \times n}(F)</m> (the ring of <m>n \times n</m> matrices with entries in <m>F</m>). Let <m>M</m> be the collection of column vectors with entries in <m>F</m> having <m>n</m> entries. The usual rules for adding column vectors and multiplying column vectors on the left by matrices make <m>M</m> into a left <m>R</m>-module. Likewise if <m>N</m> is the collection of all row vector with <m>n</m> entries in <m>F</m>, the <m>N</m> is a right <m>R</m>-module via addition and matrix multiplication.
                  </li>
      
                </ul></p>
      
              </paragraphs>
      
              <paragraphs xml:id="ex-standard-free-module">
                <title><m>\ex</m> – Standard Free Module</title>
      
                <p>
                  For a non-negative integer <m>n</m>, the “standard’’ free (left) <m>R</m>-module of rank <m>n</m> is the set <me>
      R^n=\left\{\begin{bmatrix} r_1\\ \vdots \\r_n \end{bmatrix} \mid r_i\in R, 1\leq i\leq n\right\}
      </me> equipped with the operations <me>
      \begin{bmatrix} r_1\\ \vdots\\r_n \end{bmatrix} +\begin{bmatrix} r'_1\\ \vdots\\r'_n \end{bmatrix} =\begin{bmatrix} r_1+r'_1\\ \vdots\\r_n +r'_n\end{bmatrix} \text{ and }
      r\begin{bmatrix} r_1\\ \vdots\\r_n \end{bmatrix}=\begin{bmatrix} rr_1\\ \vdots\\ rr_n \end{bmatrix}. 
      </me>
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-vector-space-module">
                <title><m>\defn</m> – Vector Space (Module)</title>
      
                <p>
                  Let <m>F</m> be a field. An <m>F</m>-<m>\defnn{vector space}</m> is a (left) <m>F</m>-module.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="prop-abelian-groups-are-z-modules">
                <title><m>\prop</m> – Abelian Groups are <m>\Z</m>-modules</title>
      
                <p>
                  Let <m>M</m> be an abelian group under <m>+</m>. Then <m>M</m> becomes a <m>\Z</m>-module upon defining the rule for scaling to be <m>nm :=</m> <me>0 \textrm{ if } n=0,</me> <me>\overbrace{m + \cdots + m}^{\text{n times}},  \text{ if } n &gt; 0, \text{ and }</me> <me>(\overbrace{m + \cdots + m}^{\text{|n| times}}), \text{ if }n &lt; 0,</me> for any <m>n \in \Z</m> and <m>m \in M</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-4">
                <title>Problem</title>
      
                <p>
                  Let <m>n</m> be a positive integer and recall that <m>\Z/n</m> denotes the ring of integers modulo <m>n</m> (whose elements I will write as <m>\ov{0}, \ov1, \cdots, \ov{n-1}</m>). 1. Show that if <m>M</m> is any abelian group (under the operation <m>+</m>) such that <m>nx = 0</m> for all <m>x \in M</m> (where <m>nx := \overbrace{x + x + \cdots + x}^{\text{n times}}</m>), then the pairing <m>\Z/n \times M \to M</m> given <m>(\ov{j}, m) \mapsto jm</m> makes <m>M</m> into a <m>\Z/n</m>-module. (Be sure to check this pairing is well-defined.) 2. Conversely, show that if <m>M</m> is a <m>\Z/n</m>-module, then the underlying abelian group <m>(M, +)</m> has the property that <m>nx = 0</m> for all <m>x \in M</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="prop-restriction-of-scalars">
                <title><m>\prop</m> – Restriction of Scalars</title>
      
                <p>
                  Let <m>\phi: R \to S</m> be a ring homomorphism. Any left <m>S</m>-module <m>M</m> may be regarded via <em>restriction of scalars</em> as a left <m>R</m>-module with the following structure: - the rule for addition <m>+</m> on <m>M</m> is the same as in the original structure and - the rule for scaling by elements of <m>R</m> is <me>r \cdot m := \phi(r)m \text{ for any }r \in R \text{ and }m\in M.</me> ###### <em>Proof.</em> Let <m>x,y \in R</m> and <m>m,n \in M</m>. One checks that the properties in the definition of module hold for the given action using properties of ring homomorphisms. In detail, <me>
      (x+y)m =\phi(x + y)m= (\phi(x)+\phi(y))m=\phi(x)m + \phi(y)m=xm+ym,
      </me> since <m>\phi</m> preserves addition, <me>
      x(ym)=\phi(x) (\phi(y)m) = (\phi(x) \phi(y)) m = \phi(xy) m = (xy) m,
      </me> since <m>\phi</m> preserves multiplication, and <me>
      1 \cdot m = \phi(1) m = 1_S m = m
      </me> since <m>\phi</m> preserves multiplicative identities. This gives three of the axioms. The final also holds: <me>x(m + n) = \phi(x) (m+n) = \phi(x)m + \phi(x)n = xm + xn.</me>
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-complex-vector-spaces-are-real-vector-spaces">
                <title><m>\exe</m> – Complex Vector Spaces are Real Vector Spaces</title>
      
                <p>
                  For example, since <m>\R</m> is a subring of <m>\C</m>, every complex vector space may be regarded as a real vector space, by restriction of scalars from <m>\C</m> to <m>\R</m>. Likewise, any real vector space may be regarded as a rational vector space. Etc.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-homomorphisms-and-restriction-of-scalars">
                <title><m>\exe</m> – Homomorphisms and Restriction of Scalars</title>
      
                <p>
                  If <m>S</m> and <m>R</m> are rings and <m>\phi: S \to R</m> is a ring homomorphism, then since <m>S</m> is a left <m>S</m>-module, it is also a left <m>R</m>-module via restriction of scalars. Note that the rule for scaling given by <m>r s := \phi(r) s</m>.
                </p>
      
                <p>
                  As a special case of this, if <m>S</m> is a subring of a ring <m>R</m> then <m>R</m> is an <m>S</m>-module (restriction of scalars along the inclusion map).
                </p>
      
                <p>
                  For instance, <m>R[x_1,\ldots,x_n]</m> is a left <m>R</m>-module for any <m>n\geq 0</m> via the evident injective ring homomorphism <m>R \into R[x_1, \ldots, x_n]</m>.
                </p>
      
                <p>
                  Also, <m>\Mat_{n \times n}(R)</m> is a left <m>R</m>-module for <m>n\geq 1</m> given by the ring map <m>R \to \Mat_{n \times n}(R)</m> sending <m>r</m> to <m>r \cdot I_n</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-ri-module-is-an-r-module">
                <title><m>\exe</m> – <m>R/I</m>-Module is an <m>R</m>-Module</title>
      
                <p>
                  If <m>I</m> is a (two-sided) ideal of a ring <m>R</m> then applying restriction of scalars along the quotient homomorphism[^1] <m>q:R\to R/I</m> gives that any left <m>R/I</m>-module is also a left <m>R</m>-module.
                </p>
      
                <p>
                  In particular, applying this to the <m>R/I</m>-module <m>R/I</m> gives that <m>R/I</m> is a left <m>R</m>-module. The rule for scaling is <m>r \cdot (r' + I) = rr' + I</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-submodule">
                <title><m>\defn</m> – Submodule</title>
      
                <p>
                  Let <m>R</m> be a ring and let <m>M</m> be a left <m>R</m>-module. An <em><m>R</m>-<m>\defnn{submodule}</m></em> of <m>M</m> is a subset <m>N \subseteq M</m> such that 1. <m>N</m> is a subgroup of <m>M</m> under <m>+</m> (so, we have <m>0 \in N</m>, if <m>n \in N</m> then <m>-n \in N</m>, and if <m>n_1, n_2 \in N</m> then <m>n_1 + n_2 \in N</m>), and 2. <m>rn \in N</m> for all <m>r \in R</m> and <m>n \in N</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-5">
                <title>Problem –</title>
      
                <p>
                  Let <m>M</m> be an <m>R</m>-module and <m>I</m> be an ideal in <m>R</m>. Show that the set <me>N = \{ m \in M \mid am = 0 \textrm{ for all } a \in I \}</me> is an <m>R</m>-submodule of <m>M</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-r-module-homomorphism">
                <title><m>\defn</m> – <m>R</m>-Module Homomorphism</title>
      
                <p>
                  Let <m>R</m> be a ring and let <m>M</m> and <m>N</m> be <m>R</m>-modules. An <m>R</m>-module <em><m>\defnn{homomorphism}</m></em> from <m>M</m> to <m>N,</m> sometimes called an <m>R</m>-<em><m>\defnn{map}</m></em>, is a function <m>h: M \to N</m> such that for all <m>r \in R</m> and <m>m,n \in M</m> we have 1. <m>h(m+n)=h(m)+h(n)</m>, i.e. <m>h</m> is an additive group homomorphism, and 2. <m>h(rm) = rh(m)</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-linear-transformation-module">
                <title><m>\defn</m> – Linear Transformation (Module)</title>
      
                <p>
                  Let <m>F</m> be a field and let <m>M</m> and <m>N</m> be vector spaces over <m>F</m>. A <em><m>\defnn{linear transformation}</m></em> from <m>M</m> to <m>N</m> is an <m>F</m>-module homomorphism <m>h:M \to N</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-r-module-isomorphism">
                <title><m>\defn</m> – <m>R</m>-Module Isomorphism</title>
      
                <p>
                  An <m>R</m>-module homomorphism <m>h: M \to N</m> is an <m>R</m>-module <em><m>\defnn{isomorphism}</m></em> if there is another <m>R</m>-module homomorphism <m>g: N \to M</m> such that <m>h \circ g = \id_M</m> and <m>g \circ h = \id_N</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-module-isomorphisms-and-bijections">
                <title><m>\lem</m> – Module Isomorphisms and Bijections</title>
      
                <p>
                  Given a ring <m>R</m>, <m>R</m>-modules <m>M</m> and <m>N</m>, and a <m>R</m>-module homomorphism <m>f: M \to N</m>, <m>f</m> is an <m>R</m>-module isomorphism if and only if <m>f</m> is a bijection.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-submodules-are-ideals">
                <title><m>\exe</m> – Submodules are Ideals</title>
      
                <p>
                  A subset <m>I</m> of a ring <m>R</m> is a left submodule of <m>R</m> if and only if it is a (left) ideal.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-kernels-and-images-of-homomorphisms-are-submodules">
                <title><m>\lem</m> – Kernels and Images of Homomorphisms are Submodules</title>
      
                <p>
                  Let <m>R</m> be a ring and let <m>h: M \to M'</m> be an <m>R</m>-module homomorphism. Then <m>\ker(h) := \{m \in M \mid h(m) = 0_N\}</m> is an <m>R</m>-submodule of <m>M</m> and <m>\im(h)</m> is an <m>R</m>-submodule of <m>M'</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-6">
                <title>Problem</title>
      
                <p>
                  Let <m>F</m> be any field and <m>R</m> the set of infinte-by-infinite, column-finite matrices. That is, <m>R</m> consists of arrays <m>A = (a_{i,j})_{i \in \N, j \in \N}</m>, where <m>\N = \{1, 2, \dots \}</m> and <m>a_{i,j} \in R</m> for all <m>i,j</m>, such that for each <m>j</m>, there exists an <m>I</m> (depending on <m>j</m>) such that <m>a_{i,j} = 0</m> for all <m>i \geq I</m>. That is, <m>R</m> consists of elements of the form <me>
      A = \begin{bmatrix}
      a_{1,1} &amp; a_{1,2} &amp; \cdots \\
      a_{2,1} &amp; a_{2,2} &amp;  \cdots \\
      \vdots &amp; \vdots &amp;  \\
      \end{bmatrix}
      </me> such that each column has only a finite number of non-zero entries (but, importantly, there is no uniform bound on the number of non-zero entries a column may have). It is not difficult to see that <m>R</m> is a ring under the usual rules for adding and multiplying matrices: Given <m>A</m> as above and <m>B=(b_{i,j})_{i \in \N, j \in \N}</m> we define <me>
      \text{the $(i,j)$ entry of $A + B$ to be $a_{i,j} + b_{i,j}$}
      </me> and <me>
      \text{the $(i,j)$ entry of $AB$ to be $\sum_l a_{i,l} b_{l , j}$}.
      </me> Note that the latter is a finite sum since, given <m>j</m>,<!-- linebreak -->there is an <m>L</m> such that <m>b_{l,j} = 0</m> for all <m>l \geq L</m>. Moreover, <m>AB</m> is column-finite since, for each such <m>j</m>, each of the columns <m>1, \dots, L-1</m> of <m>A</m> has only a finite number of non-zero elements. The multiplicative identity is the infinite identity matrix. You need not prove any of this.
                </p>
      
                <p>
                  Do prove that <m>R \cong R^2</m> as left <m>R</m>-modules. {} Think about splitting up an element of <m>R</m> into its even and odd columns.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-7">
                <title>Problem –</title>
      
                <p>
                  Given a homomorphism of <m>R</m>-modules <m>f\!: M \to N</m>, show that <m>\ker(f)</m> is an <m>R</m>-submodule of <m>M</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-8">
                <title>Problem –</title>
      
                <p>
                  Show that for every nonzero integers <m>m</m> and <m>n</m> there is a <m>\Z</m>-module isomorphism <m>\Hom_\Z(\Z/(n),\Z/(m)) \cong \Z / (\gcd(m,n))</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-9">
                <title>Problem –</title>
      
                <p>
                  Let <m>R</m> be a commutative ring. Given an <m>R</m>-module <m>M</m>, its annihilator is the ideal <me>\ann(M) := \{ a \in R \mid am = 0 \textrm{ for all } m \in M \}.</me> Show that if there is an isomorphism of <m>R</m>-modules <m>M \cong N</m>, then <m>\ann(M) = \ann(N)</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-10">
                <title>Problem –</title>
      
                <p>
                  Let <m>R</m> be a commutative ring with <m>1 \neq 0</m>. An <m>R</m>-module <m>M</m> is simple if it has no nontrivial submodules. Show that <m>M \neq 0</m> is simple if and only if there exists a maximal ideal <m>\fm</m> of <m>R</m> such that <m>M \cong R/\fm</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-8-unfinished">
                <title>Problem 8 #unfinished</title>
      
                <p>
                  Let N be a submodule of an R-module M. Using Zorn’s Lemma, prove that there is a submodule N′ such that 1. <m>N\cap N' = (0)</m>, and 2. <m>N''\cap(N + N')\neq(0)</m> for every non-zero submodule N′′ of M.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-5-unfinished-1">
                <title>Problem 5 #unfinished</title>
      
                <p>
                  Let <m>A</m> be a <m>\Z</m>–module and let <m>n</m> be any integer. Show that there is a <m>\Z</m>-module isomorphism <me>\Hom_\Z(\Z/n\Z, A)\cong A_n, \text{ where }A_n = \{a \in A | na = 0\}</me> is a submodule of <m>A</m>. (Note: You may use the fact that <m>A_n</m> is a <m>\Z</m>–module without proof.)
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-5-unfinished-2">
                <title>Problem 5 #unfinished</title>
      
                <p>
                  Let <m>R</m> be a non-zero, unital ring, and let <m>R^m</m> and <m>R^n</m> be the standard free left <m>R</m>-modules of finite rank <m>m</m> and <m>n</m>. Assume there is an isomorphism of <m>R</m>-modules <m>R^m \cong R^n.</m>
                </p>
      
                <p><ol>
                  <li>
                  Prove that if <m>R</m> is commutative then <m>m = n</m>. You may assume without justification that this holds in the special case when <m>R</m> is a field.
                  </li>
      
                  <li>
                  Show, by example, than <m>m</m> need not equal <m>n</m> if <m>R</m> is not assumed to be commutative.
                  </li>
      
                </ol></p>
      
              </paragraphs>
            </paragraphs>
          </subsection>






















          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

        <section xml:id="sec-modhom">
          <title>Module Homomorphisms</title>

          <p></p>


          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

        <section xml:id="sec-quote">
          <title>Quotient Modules</title>

          <subsection xml:id="quotient-modules">
            <title>Quotient Modules</title>
      
      
            <paragraphs xml:id="defn-quotient-module">
              <title><m>\defn</m> – Quotient Module</title>
      
              <p>
                Let <m>R</m> be a ring, let <m>M</m> be an R-module, and let <m>N</m> be a submodule of <m>M</m>. The <em><m>\defnn{quotient module}</m></em> <m>M/N</m> is the quotient group <m>M/N</m> under <m>+</m> (so elements of <m>M/N</m> are additive cosets of the form <m>m + N</m> with <m>m \in M</m> and addition is defined by <m>(m_1 + N) + (m_2 + N) = (m_1+m_2) + N</m>) and with the rule for scaling by <m>R</m> defined to be <me>r \cdot (m + N) = rm + N</me> for all <m>r \in R</m> and <m>m + N \in M/N</m>.
              </p>
      
      
              <paragraphs xml:id="lem-quotient-map-is-r-module-homomorphism">
                <title><m>\lem</m> – Quotient Map is <m>R</m>-module Homomorphism</title>
      
                <p>
                  Let <m>R</m> be a ring, let <m>M</m> be an <m>R</m>-module, and let <m>N</m> be a submodule of <m>M</m>. The rule for scaling introduced above is well-defined and it, along with the rule for <m>+</m>, makes <m>M/N</m> into an <m>R</m>-module. Moreover, the canonical quotient map <m>\pi: M \to M/N</m>, defined by <m>\pi(m) = m + N</m>, is an <m>R</m>-module homomorphism whose kernel is <m>\ker(\pi) = N</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-76">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Among the many things to check here, we will only check a couple.
                  </p>
      
                  <p>
                    We need to prove the rule for scaling by <m>R</m> on <m>M/N</m> is well-defined: If <m>m+N=m'+N</m> then <m>m-m'\in N</m> so <m>r(m-m')\in N</m> by the definition of submodule. This gives that <m>rm-rm'\in N</m>, hence <m>rm+N=rm'+N</m>. The module axioms are then pretty straightforward. We already know from 817 that <m>M/N</m> is an abelian group under <m>+</m>.
                  </p>
      
                  <p>
                    Let us check one of the four axioms involving scaling. We have <me>r( (m_1 +N) + (m_2 +N)) = r((m_1 + m_2) + N) = (r(m_1+m_2)) + N = (rm_1 + rm_2) + N = (rm_1 + N) + (rm_2 + N),</me> which gives the third such axiom. The other three are also straightforward.
                  </p>
      
                  <p>
                    The fact that <m>\pi</m> is an <m>R</m>-module homomorphism is also straightforward. Its kernel is <m>\{m \in M \mid m + N = N\}</m>, which is equal to <m>N</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="exe-z-modules-and-quotients">
                <title><m>\exe</m> – <m>\Z</m>-modules and Quotients</title>
      
                <p>
                  Recall that <m>\Z</m>-module are the same as abelian groups. submodules and quotient <m>\Z</m>-modules are the same things as subgroups and quotients of abelian groups.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="thm-module-isomorphism-theorems">
                <title><m>\thm</m> – Module Isomorphism Theorems</title>
      
                <p>
                  Let <m>R</m> be a ring, and let <m>M</m> be a <m>R</m>-module. - (UMP for Quotient Modules) Let <m>N</m> be a submodule of <m>M</m>, let <m>T</m> be an <m>R</m>-module, and let <m>f: M \to T</m> be an <m>R</m>-module homomorphism. If <m>f(M) = 0</m> (i.e., if <m>N \subseteq \ker(f)</m>) then the function <m>\ov{f}: M/N \to T</m> given by <m>\ov{f}(m + N) = f(m)</m> is a well-defined, <m>R</m>-module homomorphism. In fact, <m>\ov{f}: M/N \to T</m> is the unique <m>R</m>-module homomorphism such that <m>\ov{f}\circ \can = f</m> where <m>\can</m> denotes the canonical surjection <m>M \onto M/N</m>.<!-- linebreak -->- (First Isomorphism Theorem) Let <m>T</m> be an <m>R</m>-module and let <m>h: M \to T</m> be an <m>R</m>-module homomorphism. Then <m>\ker(h)</m> is a submodule of <m>M</m> and there is an <m>R</m>-module isomorphism <m>M/\ker(h) \cong \im(h)</m> given by <m>m + \ker(h) \mapsto h(m)</m>. - (Second Isomorphism Theorem) Let <m>A</m> and <m>B</m> be submodules of <m>M</m>, and define <m>A + B = \{a+b \mid a \in A, b \in B\}</m>. Then <m>A + B</m> is a submodule of <m>M</m>, <m>A \cap B</m> is a submodule of <m>A</m>, and there is an <m>R</m>-module isomorphism <m>(A + B)/B \cong A/(A \cap B)</m>. - (Third Isomorphism Theorem) Let <m>A</m> and <m>B</m> be submodules of <m>M</m> with <m>A \subseteq B</m>. Then <m>B/A</m> is a submodule of <m>M/A</m> and there is an <m>R</m>-module isomorphism <m>(M/A)/(B/A) \cong M/B</m> given by sending <m>(m + A) + B/A</m> to <m>m + B</m>. - (Lattice Isomorphism Theorem) Let <m>R</m> be a ring, let <m>N</m> be a R-submodule of <m>M</m>, and let <m>\pi: M \to M/N</m> be the canonical quotient map. Then the function <me>\Psi : \{R-\text{submodules of } M \text{ containing }N\} \to \{R-\text{submodules of }M/N\}</me>defined by <m>\Psi(K) = K/N</m> is a bijection, with inverse given by <m>\Psi^{-1}(T) = \pi^{-1}(T)</m> for each <m>R</m> submodule <m>T</m> of <m>M/N</m>. Moreover, <m>\Psi</m> and <m>\Psi^{-1}</m> preserve sums and intersections.
                </p>
      
      
                <paragraphs xml:id="proof.-77">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Ignoring the rules for scaling by <m>R</m>, we know each of the frist four results holds for abelian groups (and the maps are the same). So, we merely need to prove that the rules for scaling are respected in each case. In more detail:
                  </p>
      
                  <p>
                    For the UMP, we already know that <m>\ov{f}</m> is a well-defined homomorphism of groups under <m>+</m> and that it is the unique one such that <m>\ov{f} \circ \can = f</m>. It remains only to show <m>\ov{f}</m> preserves scaling: This follows quickly from the definitions: <me>
      \ov{f}(r (m +N)) = \ov{f} (rm + N) = f(rm) = r f(m) = r \ov{f}(m + N).
      </me> where the third equation uses that <m>f</m> preserves scaling.
                  </p>
      
                  <p>
                    For the First Isomorphism Theorem, we already know that there is an isomorphism of abelian groups under <m>+</m>, <me>
      \ov{h}: M/\ker(h) \xra{\cong} \im(h),
      </me> given by <m>\ov{h}(m + \ker(h)) = h(m)</m>, and it remains only to show this map preserves scaling. This is a special case of what we proved in part (0).
                  </p>
      
                  <p>
                    For the second isomorphism theorem, we need to first check that <m>A+B</m> and <m>A \cap B</m> are submodules. From 817 we already know they are subgroups under <m>+</m>, and it is evident from the definitions that each is closed under scaling by elements of <m>R</m>. Now, we know from 817 that there is an isomorphism of abelian groups <m>h: A/(A \cap B) \xra{\cong} (A+B)/B</m> given by <m>h(a + (A\cap B)) = a + B</m>. It remains only to show <m>h</m> preserves scaling: <me>
      h(r(a+(A \cap B))) = h(ra + A \cap B) = ra + B = r(a +B) = rh(a + (A \cap B)).
      </me>
                  </p>
      
                  <p>
                    For the third, we already know (from 817) that <m>B/A</m> is a subgroup of <m>M/A</m> under <m>+</m>. Given <m>r \in R</m> and <m>b +A \in B/A</m> we have <m>r(b+A) = rb + A</m> which belongs to <m>B/A</m> since <m>rb \in B</m>. This proves <m>B/A</m> is a submodule of <m>M/A</m>. Also from 817 we know there is an isomorphism of abelian groups <m>h: (M/A)/(B/A) \to M/B</m> given by <m>h((m+A) + B/A) = m + B</m> and it remains only to show it is <m>R</m>-linear: <m>h(r((m+A) + B/A)) =h(r(m+A) + B/A) = h((rm + A) + B/A) = rm + B = r(m +B) = r h((m+A) + B/A)</m>.
                  </p>
      
                  <p>
                    The Lattice Theorem is the most complicaed to gerenlize. From 817 we know thre is a bijection between the set of sub groups of <m>M</m> and that contain <m>N</m> and subgroups of the quotient group <m>M/N</m>, and the maps are the same as given in the statment. We just need to prove that these maps send submodules to submodules. If <m>K</m> is a submodule of <m>M</m> containing <m>N</m>, then by part (3) we know <m>K/N</m> is a submodule of <m>M/N</m>.
                  </p>
      
                  <p>
                    If <m>T</m> is a submodule of <m>M/N</m>, then <m>\pi^{-1}(T)</m> is an abelian group. For <m>r \in R</m> adn <m>m \in \pi^{-1}(T)</m> we have <m>\pi(m) \in T</m> and hence <m>\pi(rm) = r\pi(m) \in T</m> too, since <m>T</m> is a submodule. This proves <m>\pi^{-1}(T)</m> is a submodule.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-5-modules-and-maximal-ideals">
                <title>Problem 5 – Modules and Maximal Ideals</title>
      
                <p>
                  Let <m>R</m> be a commutative ring (with <m>1</m>)[^1] and let <m>M</m> be an <m>R</m>-module. Show that if <m>M\neq 0</m> and the only submodules of <m>M</m> are <m>0</m> and <m>M</m>, then there is a maximal ideal I of <m>R</m> such that <m>M</m> is isomorphic to <m>R/I</m>[^2].
                </p>
      
      
                <paragraphs xml:id="proof.-78">
                  <title><em>Proof</em>.</title>
      
                  <p>
                    Let <m>R</m> be a commutative ring (with <m>1</m>) and let <m>M</m> be an <m>R</m>-module such that <m>M\neq 0</m> and the only submodules of <m>M</m> are <m>0</m> and <m>M</m>.
                  </p>
      
                  <p>
                    As <m>M\neq 0</m>, there exists a non-zero element <m>m\in M</m>. Let <m>f:R\to M</m> be defined by <m>f(r)=rm</m>. Since <m>1\in R</m> and <m>1m=m\in Rm</m>, we have <m>Rm=M</m>. By the First Isomorphism Theorem we see that <m>R/\ker(f)\cong M</m>, making <m>\ker(f)</m> an ideal in <m>R</m>, which we shall conspicuously denote as <m>I</m> henceforth. By the Lattice Isomorphism Theorem the only two ideals of <m>R/I</m> are <m>0</m> and <m>R/I</m>, making <m>R/I</m> a field, and <m>I</m> a maximal ideal in <m>R</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-17">
                <title>Problem –</title>
      
                <p>
                  Let <m>R</m> be a ring with <m>0\neq 1</m>. Prove that if <m>M</m> is an <m>R</m>-module and <m>N</m> is a submodule of <m>M</m> such that <m>N</m> and <m>M/N</m> are finitely generated, then <m>M</m> is finitely generated.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="prop-every-cyclic-r-module-cong-to-ri-for-some-i">
                <title><m>\prop</m> – Every Cyclic <m>R</m>-module <m>\cong</m> to <m>R/I</m> for some <m>I</m></title>
      
                <p>
                  Every cyclic <m>R</m>-module is isomorphic to <m>R/I</m>[^1] for some left ideal <m>I</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-79">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Say <m>M</m> is cyclic and <m>m \in M</m> is a generator of <m>M</m>, so that <m>M = \{rm \mid r \in R\}</m>. Define <m>f: R \to M</m> to be the unique <m>R</m>-map with <m>f(1) = m</m>. Here I am applying the UMP for bases, using that <m>\{1\}</m> is a basis of <m>R</m> as a left <m>R</m>-modules. More explicitly, <me>f(r) = f(r\cdot 1) = r f(1) = rm</me> for all <m>r \in R</m>. Then <m>f</m> is onto, since <m>m</m> generates <m>M</m>. Its kernel is a left ideal <m>I</m> of <m>R</m>, since submodules of <m>R</m> are the same thing as left ideals. By the FIT, there is an isomorphism <m>R/I \xra{\cong} M</m> sending <m>r + I</m> to <m>rm</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-18">
                <title>Problem</title>
      
                <p>
                  Let <m>R</m> be a ring. Recall that we proved in class that every cyclic <m>R</m>-module is isomorphic to <m>R/I</m> for some left ideal <m>I</m>. Prove the left ideal <m>I</m> occurring this statement is unique; that is, if a cyclic <m>R</m>-module is isomorphic to <m>R/I</m> and <m>R/J</m> for left ideals <m>I</m> and <m>J</m>, then <m>I = J</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-23">
                <title><m>\rem</m></title>
      
                <p>
                  More generally, the same argument shows that if <m>M</m> is a finitely generated <m>R</m> module, say generated by <m>n</m> elements, then <m>M \cong R^n/N</m> for some submodule <m>N</m> of <m>R^n</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-mim-is-an-ri-module">
                <title><m>\lem</m> – <m>M/IM</m> is an <m>R/I</m>-module</title>
      
                <p>
                  For a commutative ring <m>R</m>, module <m>M</m> and ideal <m>I</m>, the rules for addition in <m>M/IM</m>[^1] and scaling by <m>R/I</m> on <m>M/IM</m> introduced above make <m>M/IM</m> into an <m>R/I</m>-module.
                </p>
      
                <p>
                  Moreover, given another <m>R</m>-module <m>N</m> and an <m>R</m>-map <m>f: M \to N</m>, the function <m>\ov{f}: M/IM \to N/IN</m> given by <m>\ov{f}(m + IM) = f(m) + IN</m> is a well-defined <m>R/I</m>-module homomorphism.
                </p>
      
                <p>
                  Finally, if <m>g: N \to P</m> is yet another <m>R</m>-module homomorphism, then <m>\ov{g} \circ \ov{f} = \ov{g \circ f}</m>, and we also have <m>\ov{\id_M} = \id_{M/IM}</m> for any <m>R</m>-module <m>M</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-80">
                  <title><em>Proof.</em></title>
      
                  <p>
                    I leave some of the details as an exercise, but I will check a few of the necessary things:
                  </p>
      
                  <p>
                    We already showed that the rule for scaling is well defined, and we know from 817 that the rule for addition is well-defined and that <m>(M/IM, +)</m> is an abelian group. To show <m>M/IM</m> is an <m>R/I</m>-module, there remain four axioms to verify. For instance, <me>((r+I)(s+I))(m +IM) = (rs + I)(m + IM) = (rsm + IM) = (r+I)(sm +IM) = (r+I)((s+I)(m+IM)),</me>which verifies one of them; the other proofs are similar.
                  </p>
      
                  <p>
                    Next, let me verify that the function <m>\ov{f}</m> is a well-defined <m>R/I</m>-map: Let <m>g: M \to N/IN</m> be the composition of <m>R</m>-maps <m>M \xra{f} N \to N/IN</m> (the second one being the canonical one), so that <m>g(m) = f(m) + IN</m>. Since <m>f(\sum_i a_i m_i) = \sum a_i f(m_i) \in IN</m> for any <m>a_i</m>’s belonging to <m>I</m> and <m>m_i</m>’s belonging to <m>M</m>, we have <m>IM \subseteq \ker(g)</m>. By the UMP for quotient modules, there is an induced <m>R</m>-map <m>\ov{f}: M/IM \to N/IN</m> given by <m>\ov{f}(m + IM) = f(m) + IN</m>. The map <m>\ov{f}</m> is so far only known to be an <m>R</m>-map, but it is in fact an <m>R/I</m>-map since <me>\ov{f}((r + I)(m +IM)) =   \ov{f}(rm +IM) = f(rm) + IN = rf(m) + IN = (r+I)(f(m) + IN).</me> The final assertions are clear from the formula for <m>\ov{f}</m> for an <m>R</m>-map <m>f</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-11-modules-and-mleq-n">
                <title>Problem 11 – Modules and <m>m\leq n</m></title>
      
                <p>
                  Let <m>R</m> be a commutative ring with <m>1\neq 0</m>, and let <m>f : R^m\to R^n</m> be a surjective homomorphism of free <m>R</m>-modules. Prove that <m>m \leq n</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-81">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Let <m>R^n=N</m> and <m>R^m=M</m>.
                  </p>
      
                  <p>
                    Let <m>I</m> be a maximal ideal in <m>R</m>. Thus <m>R/I</m> is a field. Lemma 1.58 tells us that <m>M/IM</m> and <m>N/IN</m> are <m>R/I</m>-vector spaces. Additionally, this gives rise to <m>\overline{p}:M/IM\to N/IN</m>, which is a surjective <m>R/I</m>-module linear transformation.
                  </p>
      
                  <p>
                    Note that <m>M</m> is generated by <m>e_i+IM</m> for <m>i\leq m</m>. Let <m>a_i\in R</m> and consider <m>\sum a_ie_i=0</m>. For this to be <m>0</m> we need it to be in <m>IM</m>, and thus all <m>a_i\in I</m>. So the set of <m>e_i</m> is a basis for <m>M/IM</m> with <m>m</m> elements. Likewise <m>N/IN</m> has a basis with <m>n</m> elements. As we are surjective, Rank<m>=n</m>, <m>\dim=m</m>. So by Rank-Nullity <m>\dim(\ker)=m-n</m> which is only positive with <m>m\geq n</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-19">
                <title>Problem</title>
      
                <p>
                  Let <m>R</m> be a non-zero commutative ring and suppose <m>p: R^m \to R^n</m> is a surjective homomorphism of <m>R</m>-modules for some non-negative integers <m>m</m> and <m>n</m>. 1. Prove that if <m>R</m> is a field, then <m>m \geq n</m>. 2. Prove that if <m>R</m> is any non-zero commutative ring, then <m>m \geq n</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-5-showing-module-is-torsion-free">
                <title>Problem 5 – Showing Module is Torsion-Free</title>
      
                <p>
                  Recall that a <m>\Z</m>-module <m>M</m> is called <em>torsion-free</em> if its torsion submodule is <m>\Tor(M ) =\{0\},</m> where <m>\Tor(M ) = \{m \in M | rm = 0 \text{ for some }r \in \Z\setminus\{0\}\}.</m> Consider the <m>\Z</m>-module <me>M= \frac{\Z \oplus \Z}{\langle(7, 11)\rangle }\text{ where }\langle(7, 11)\rangle = \{(7k, 11k) | k \in \Z\}.</me> Show that <m>M</m> is torsion free.
                </p>
      
      
                <paragraphs xml:id="proof.-82">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Let <m>m=(a,b)\in\Tor(M)</m>. Thus <m>rm=0</m> for some nonzero <m>r\in\Z</m>. Then <m>(ra,rb)=0</m>, and so <m>ra=0</m> and <m>rb=0</m>. Then there exists <m>p,q\in\Z</m> such that <m>7p=ra</m> and <m>7q=rb</m> or <m>11p=ra</m> and and <m>11q=rb</m>. Notice that in <m>M</m> we have <m>7,11=0</m>, and thus neither <m>7</m> nor <m>11</m> can divide <m>r</m>. Suppose <m>7p=ra</m> and <m>7q=rb</m>. As <m>7</m> cannot divide <m>r</m> we see that <m>7</m> divides both <m>a</m> and <m>b</m>, placing them both <m>\langle(7, 11)\rangle</m>. Thus <m>m=0</m>. The same holds true if we use <m>11</m>. Thus <m>m=0</m> and <m>\Tor(M)=\{0\}</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-24">
                <title><m>\rem</m></title>
      
                <p>
                  The Lemma shows that the rules <m>M \mapsto M/IM</m> and <m>f \mapsto \ov{f}</m> determine what is known as a functor from the category of <m>R</m>-modules to the category of <m>R/I</m>-modules.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-maximal-ideals-and-vector-spaces">
                <title><m>\exe</m> – Maximal Ideals and Vector Spaces</title>
      
                <p>
                  Suppose <m>I</m> is a maximal ideal of a commutative ring <m>R</m>. Then <m>R/I</m>[^1] is a field,[^2] and given an <m>R</m>-module <m>M</m>, <m>M/IM</m> is a module over the field <m>R/I</m>; i.e., it is a vector space over this field. Moreover, if <m>f: M \to N</m> is an <m>R</m>-map then <m>\ov{f}: M/IM \to N/IN</m> is an <m>R/I</m>-linear transformation.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-20">
                <title>Problem</title>
      
                <p>
                  Let <m>R</m> be a ring, let <m>M</m> be a left <m>R</m>-module and let <m>N \subseteq M</m> be a left <m>R</m>-submodule. 1. Prove that if <m>M</m> is finitely generated (as an <m>R</m>-module), then so is <m>M/N</m>. 2. Prove that if <m>M/N</m> and <m>N</m> are both finitely generated, then so is <m>M</m>. 3. Prove the converse to the previous part is false, as follows: Let <m>F</m> be a field and <m>R = F[x_1, x_2, \dots]</m>, the ring of polynomials in the infinite list of variables <m>x_1, x_2, \dots</m>. (So, an element of <m>R</m> is a {} <m>F</m>-linear combination of monomials of the form <m>x_1^{e_1} \cdots x_n^{e_n}</m> for <m>n \geq 0</m> and <m>e_i \geq 1</m>. Note that each element of <m>R</m> involves only a finite number of variables, but there is no uniform bound on how many such variables can be involved in the elements of <m>R</m>.) You may assume without proof that <m>R</m> is a ring with the usual rules for adding and multiplying polynomials, which make sense since each element of <m>R</m> involves only a finite number of variables. Finally let <m>I</m> be the ideal of <m>R</m> generated by the variables <m>x_1, x_2, \dots</m>. Prove <m>R</m> is finitely generated as an <m>R</m>-module but the submodule <m>I</m> is not.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-21">
                <title>Problem</title>
      
                <p>
                  Let <m>R</m> be a ring, <m>M</m> and <m>N</m> left <m>R</m>-modules, and <m>p: M \to N</m> an <m>R</m>-module homomorphism. Assume that <m>p</m> is surjective. We say <m>p</m> is a {} if there exists an <m>R</m>-module homomorphism <m>j: N \to M</m> such that <m>p \circ j = id_N</m> (i.e., <m>p(j(n)) = n</m> for all <m>n \in N</m>). 1. Prove that if <m>N</m> is free, then every surjective <m>R</m>-module homomorphism of the form <m>p: M \to N</m> is a split surjection. 2. By giving an explicit example with justification, show the statement in part (a) would become false if <m>N</m> were not assumed to be free.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-22">
                <title>Problem</title>
      
                <p><ol>
                  <li>
                  Assume <m>R</m> is a ring, <m>W</m> an <m>R</m>-module, and <m>i: V \to W</m> an injective <m>R</m>-module homomorphism. We say <m>i</m> is a {} if there exists an <m>R</m>-module homomorphism <m>q: W \to V</m> such that <m>q \circ i = id_V</m> (i.e., <m>q(i(v)) = v</m> for all <m>v \in V</m>).
                  </li>
      
                  <li>
                  Prove that if <m>R</m> is a field, <m>V</m> is a subspace (i.e., submodule) of <m>W</m>, and <m>i</m> is the inclusion map, then <m>i</m> is a split injection. Note that since <m>i</m> is the inclusion map, what you need to prove is that there exists an <m>R</m>-module homomorphism <m>q: W \to V</m> such that <m>q(v) = v</m>. {}: Start by picking a basis <m>B</m> of <m>V</m> and use a theorem proven in class to show that <m>B</m> can be extended to a basis <m>C</m> of <m>W</m>. Use <m>C</m> to construct <m>q</m>. (I am assuming that <m>V</m> is a subspace of <m>W</m> and that <m>i</m> is the inclusion map just for simplicity — more generally, it is true that every injective <m>R</m>-module homomorphism is a split injection whenever <m>R</m> is a field.)
                  </li>
      
                  <li>
                  Assume <m>R</m> is a non-zero integral domain, but that it is not a field. Prove there exists an <m>R</m>-module homomorphism that is an injection but {} a split injection. {}: Pick <m>a \in R</m> such that <m>a \neq 0</m> and <m>a</m> is not a unit, let <m>I</m> be the proper ideal <m>I = R \cdot a</m> generated by <m>a</m> and show the inclusion map <m>i: I \to R</m> is not a split injection.
                  </li>
      
                </ol></p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-23">
                <title>Problem</title>
      
                <p>
                  Let <m>R</m> be a commutative integral domain and <m>M</m> an <m>R</m>-module. A subset <m>S</m> of <m>M</m> is called a <em>maximal linearly independent</em> subset of <m>M</m> if <m>S</m> is linearly independent and any subset of <m>M</m> properly containing <m>S</m> is linearly dependent. 1. Let <m>T</m> be a linearly independent subset of <m>M</m>. Prove that <m>T</m> is contained in some maximal linearly independent subset of <m>M</m>. 2. Let <m>T</m> be a linearly independent subset of <m>M</m> and let <m>N</m> be the <m>R</m>-submodule of <m>M</m> generated by <m>T</m>. Prove that <m>T</m> is a maximal linearly independent subset if and only if <m>M/N</m> is torsion. (Recall that an <m>R</m>-module <m>P</m> is called “torsion’’ if for each <m>p \in P</m>, there is a <m>r \in R</m> such that <m>r \ne 0</m> and <m>rp = 0</m>.)
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-24">
                <title>Problem –</title>
      
                <p>
                  Prove that if <m>R</m> is a commutative ring with <m>1\neq 0</m> then <m>R^m \cong R^n</m> as <m>R</m>-modules if and only if <m>m = n</m>. In order to do that, you will complete the following steps: 1. Show that if <m>I</m> is any ideal of <m>R</m> and <m>M</m> is any <m>R</m>-module, then <m>M/IM</m> is an <m>R/I</m>-module via <me>(r + I) \cdot (m + IM) = rm + IM.</me> 2. Show that if <m>I</m> is any ideal of <m>R</m>, then <m>R^n/IR^n \cong (R/I)^n</m> as <m>R/I</m>-modules. 3. Apply the previous part when <m>I = \fm</m> is a maximal ideal of <m>R</m>. You will need to use the following fact, which we shall prove in class very soon: if <m>F</m> is a field, then <m>F^n\cong F^m</m> as <m>F</m>-vector spaces if and only if <m>m=n</m>. #### Problem – Properties of Torsion Submodules Let <m>R</m> be a domain and let <m>M</m> be an <m>R</m>-module. The torsion submodule of <m>M</m> is <me>\Tor(M) = \{m \in M \mid rm = 0 \text{ for some } r \in R \textrm{ with } r \neq 0 \}.</me> Elements of <m>\Tor(M)</m> are called the torsion elements of <m>M</m>, and the module <m>M</m> is called {} if <m>\Tor(M) = 0</m>. You may take for granted that this is actually a submodule of <m>M</m> without proof. 1. Show that if <m>M</m> and <m>N</m> are <m>R</m>-modules, then <m>\Tor(M \oplus N) = \Tor(M) \oplus \Tor(N)</m>. 2. Show that if <m>M \cong N</m>, then <m>\Tor(M) \cong \Tor(N)</m>. 3. Show that if <m>M</m> is a free <m>R</m>-module then <m>\Tor(M)=0</m>. 4. Show that if <m>I</m> is an ideal of <m>R</m> that is not principal, then <m>I</m> is a torsion-free <m>R</m>-module that is not a free <m>R</m>-module. 5. Show that if <m>I\neq(0)</m> is an ideal of <m>R</m> then <m>\Tor(R/I)=R/I</m>. 6. Suppose that R is a PID, and that <m>M</m> is a finitely generated <m>R</m>-module. Show that <m>M</m> is a torsion-free <m>R</m>-module if and only if <m>M</m> is a free <m>R</m>-module.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-25">
                <title>Problem –</title>
      
                <p>
                  Let <m>R</m> be a commutative ring with <m>1 \neq 0</m>. Show that <me>\ann_R (M \oplus N) = \ann_R (M) \cap \ann_R (N)</me>
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-9-unfinished">
                <title>Problem 9 #unfinished</title>
      
                <p>
                  Let <m>I</m> be an ideal in a commutative ring <m>R</m>, let <m>M</m> and <m>N</m> be <m>R</m>-modules and let <m>f : M \to N</m> be an <m>R</m>-module homomorphism.
                </p>
      
                <p><ol>
                  <li>
                  Prove there is a unique <m>R</m>-module homomorphism <m>\ov{f} : M/IM \to N/IN</m> such that <m>\ov{f} \circ p = q \circ f</m>, where <m>p : M\to M/IM</m> and <m>q : N\to N/IN</m> are the canonical quotient maps
                  </li>
      
                  <li>
                  Prove that if <m>I^2 = 0</m> and <m>\ov{f}</m> is surjective, then so is <m>f</m> . (Recall that <m>I^2</m> is the ideal<!-- linebreak -->generated by all elements of the form <m>ab</m>, where <m>a, b \in I</m>.)
                  </li>
      
                </ol></p>
      
              </paragraphs>
            </paragraphs>
          </subsection>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

      </chapter>

      <chapter xml:id="ch-freemod">
        <title>Free Modules</title>

        <section xml:id="sec-linear">
          <title>Linear Independence</title>

          <subsection xml:id="generated-modules">
            <title>Generated Modules</title>
      
      
            <paragraphs xml:id="defn-linear-combination">
              <title><m>\defn</m> – Linear Combination</title>
      
              <p>
                Let <m>M</m> be an <m>R</m>-module and <m>m_1, \dots, m_n \in M</m>. An <m>R</m>-<em><m>\defnn{linear combination}</m></em> of <m>m_1, \dots, m_n</m> is an element of <m>M</m> of the form <m>r_1m_1 + \cdots + r_nm_n</m> for some <m>n \geq 0</m> and <m>r_1,\dots,r_n \in R</m>. (If <m>n = 0</m>, this gives the empty sum which is interpreted to give <m>0_M</m>.)
              </p>
      
      
              <paragraphs xml:id="rem-12">
                <title><m>\rem</m></title>
      
                <p>
                  If <m>M</m> is an <m>F</m>-vector space, one usually uses the term <q>spanned by’’ instead of the term</q>generated by’’.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-rcdot-a-smallest-submodule-of-m-containing-a">
                <title><m>\lem</m> – <m>R\cdot A</m> Smallest Submodule of <m>M</m> Containing <m>A</m></title>
      
                <p>
                  For any subset <m>A</m> of a <m>R</m>-module <m>M</m>, the subset <m>R \cdot A</m> is indeed a submodule of <m>M</m>, and it is the smallest submodule of <m>M</m> that contains <m>A</m> as a subset. In fact, we have <me>R \cdot A = \bigcap\limits_{A\subseteq N, N \text{ submodule of }M} N.</me>
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-finitely-generated-module">
                <title><m>\defn</m> – Finitely Generated (Module)</title>
      
                <p>
                  A module <m>M</m> is <em><m>\defnn{finitely generated}</m></em> if there exists some finite subset <m>A</m> of <m>M</m> such that <m>M = R \cdot A</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-13">
                <title><m>\rem</m></title>
      
                <p>
                  For any <m>R</m>-module <m>M</m>, we have <m>R \cdot \emptyset = \{0_M\}</m>. This is because the empty sum is interpreted as giving <m>0_M</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-cyclic-module">
                <title><m>\defn</m> – Cyclic Module</title>
      
                <p>
                  If <m>M = R \cdot a</m>[^1] for some single element <m>a \in M</m>, we say that <m>M</m> is <em><m>\defnn{cyclic}</m></em>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-cyclic-z-module">
                <title><m>\exe</m> – Cyclic <m>\Z</m>-module</title>
      
                <p>
                  If <m>R = \Z</m>, then (recalling that a <m>\Z</m>-module is the same thing as an abelian group) we see that <m>M</m> is a cyclic <m>\Z</m>-module if and only if <m>M</m> is a cyclic group.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-11">
                <title>Problem –</title>
      
                <p>
                  Show that the left <m>R</m>-module <m>M</m> is cyclic if and only if there exists a left ideal <m>I</m> of <m>R</m> such that <m>M \cong R/I</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-12">
                <title>Problem –</title>
      
                <p>
                  Let <m>R</m> be a commutative a ring. Let <m>M</m> be an <m>R</m>-module, <m>I</m> an ideal in <m>R</m>, and let <me>N = \{ m \in M \mid am = 0 \textrm{ for all } a \in I \},</me> which is an <m>R</m>-submodule of <m>M</m> by Problem 1. Show that <me>\Hom_R(R/I,M) \cong N.</me>
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-5-annihilators-and-generated-ideals">
                <title>Problem 5 – Annihilators and Generated Ideals</title>
      
                <p>
                  Let <m>R</m> be a (not-necessarily commutative) ring let <m>M</m> be a left <m>R</m>-module. The annihilator of <m>M</m> in <m>R</m> is defined to be <me>\ann_R(M) = \{ r \in R \mid rm = 0 \text{ for all $m \in M$}\}.</me>(a) Prove that <m>\ann_R(M)</m> is a <m>2</m>-sided ideal of <m>R</m>. (b) Suppose <m>M</m> is an abelian group (i.e., a <m>\Z</m>-module) such that <m>|M| = 400</m> and <m>\ann_\Z(M)</m> is the ideal generated by <m>20</m>. How many possibilities, up to isomorphism, are there for <m>M</m>?
                </p>
      
      
                <paragraphs xml:id="proof.-70">
                  <title><em>Proof</em>.</title>
      
      
                  <paragraphs xml:id="part-a-26">
                    <title>Part (a)</title>
      
                    <p>
                      Let <m>a,b\in \ann_R(M)</m>. Consider <m>(a+b)m=am+bm=0</m>. Thus <m>a+b\in \ann_R(M)</m>. Let <m>r\in\ann_R(M)</m> and consider <m>-r</m>. Let <m>m\in M</m> and suppose <m>-rm=n</m> for some <m>n\in R</m>. Add <m>rm</m> to both sides to see that <m>0=n+rm=n</m>. Thus <m>n=0</m> and <m>-rm=0</m> for all <m>m\in M</m>. So <m>-r\in\ann_R(M)</m>.
                    </p>
      
                    <p>
                      Note that as <m>0m=0=m0</m> for all <m>m\in M</m>, we know that <m>0\in\ann_R(M)</m>. As <m>rm=0</m> for all <m>m\in M</m>, we see that <m>\ann_R(M)</m> is a left sided ideal.
                    </p>
      
                    <p>
                      Suppose <m>mr=n</m> for some <m>m\in R</m>. This time we add <m>-rm</m> to both sides, but as <m>-r\in\ann_R(M)</m>, we once again find that <m>mr=0</m>. Notice that this means <m>rm=mr</m> for all <m>r\in \ann_R(M)</m> and <m>m\in M</m>, and thus that elements of <m>\ann_R(M)</m> commute with elements of <m>M</m>.
                    </p>
      
                    <p>
                      Let <m>a,b\in \ann_R(M)</m>. Consider <m>m(a+b)=ma+mb</m>. Luckily, we know <m>ma=am=0=bm=mb,</m> and thus that <m>ma+mb=0+0=0</m>. Hence <m>a+b\in \ann_R(M)</m>, making <m>\ann_R(M)</m> is a two sided ideal.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-b-24">
                    <title>Part (b)</title>
      
                    <p>
                      By the FTFGAG and Sunzi’s Remainder Theorem, there are only so many options we have for <m>M</m>: - <m>\Z_{16}\times \Z_{25}</m>, - <m>\Z_{8}\times \Z_{2}\times \Z_{25}</m>, - <m>\Z_{4}\times \Z_{4}\times \Z_{25}</m>, - <m>\Z_{4}\times \Z_{2}\times \Z_{2}\times \Z_{25}</m>, - <m>\Z_{2}\times\Z_{2}\times \Z_{2}\times \Z_{2}\times \Z_{25}</m>, - <m>\Z_{16}\times \Z_{5}\times \Z_{5}</m>, - <m>\Z_{8}\times \Z_{2}\times \Z_{5}\times \Z_{5}</m>, - <m>\Z_{4}\times \Z_{5}\times \Z_{20}</m> - <m>\Z_{4}\times \Z_{2}\times \Z_{2}\times \Z_{5}\times \Z_{5}</m>, and - <m>\Z_{2}\times\Z_{2}\times \Z_{5}\times \Z_{20}</m>. However, as ideals are additive subgroups, we know that <m>M</m> needs to contain a cyclic subgroup of order <m>20</m>. Thus we need only consider decompositions with a <m>\Z_{20}</m> in them, of which there are exactly two: 1. <m>\Z_{4}\times \Z_{5}\times \Z_{20}</m> 2. <m>\Z_{2}\times\Z_{2}\times \Z_{5}\times \Z_{20}</m>
                    </p>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="exe-standard-free-module-finitely-generated">
                <title><m>\exe</m> – Standard Free Module Finitely Generated</title>
      
                <p>
                  Let <m>R</m> be a ring. The standard free <m>R</m>-module of rank <m>n</m>, <m>R^n</m>, is finitely generated, since it is generated by <m>e_1, \dots, e_n</m> where <m>e_i := \begin{bmatrix} 0 \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}</m>, with a <m>1</m> in the <m>i</m>-th position. This holds since given any element <m>v = \begin{bmatrix} r_1 \\ \vdots \\ r_n\end{bmatrix}</m> of <m>R^n</m> we have <m>v = \sum_i r_i e_i</m>.
                </p>
      
                <p>
                  In particular, taking <m>n = 1</m>, <m>R</m> is cyclic as a module over itself, since <m>R = R \cdot 1</m>. More generally, for any (two sided) ideal <m>I</m>, <m>R/I</m> is a cyclic left <m>R</m>-module, generated by <m>\ov{1}</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-linearly-independent-module">
                <title><m>\defn</m> – Linearly Independent (Module)</title>
      
                <p>
                  Let <m>M</m> be an <m>R</m>-module and let <m>A</m> be a subset of <m>M</m>. The set <m>A</m> is <em><m>\defnn{linearly independent}</m></em> if whenever <m>r_1,\ldots,r_n \in R</m> and <m>a_1,\ldots ,a_n</m> are distinct elements of <m>A</m> satisfying <m>r_1a_1 + \dots + r_na_n = 0</m>, then <m>r_1 = \dots = r_n = 0</m>. Equivalently, <m>A</m> is linearly independent if and only if given any equation of the form <m>\sum_a r_a a = 0</m> where <m>r_a \in R</m> for all <m>a \in A</m> and <m>r_a = 0</m> for all but a finite number of <m>a'</m>s, we must have <m>r_a = 0</m> for all <m>a</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-linearly-dependent-module">
                <title><m>\defn</m> – Linearly Dependent (Module)</title>
      
                <p>
                  Let <m>M</m> be an <m>R</m>-module and let <m>A</m> be a subset of <m>M</m>. The set <m>A</m> is linearly dependent if and only if there is an equation of the form <m>\sum_a r_a a = 0</m> where <m>r_a \in R</m> for all <m>a \in A</m>, <m>r_a = 0</m> for all but a finite number of <m>a'</m>s, and <m>r_a \ne 0</m> for at least one <m>a</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-14">
                <title><m>\rem</m></title>
      
                <p>
                  The empty subset of any module is linearly independent (vacuously).
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-one-element-subsets-of-r-modules">
                <title><m>\exe</m> – One Element Subsets of <m>R</m>-Modules</title>
      
                <p>
                  A one element subset <m>\{m\}</m> of an <m>R</m>-module <m>M</m> is linearly independent if and only if whenever <m>rm =0</m>, we have <m>r = 0</m>.
                </p>
      
                <p>
                  But it is possible for one elements subsets to be linearly dependent: For example, let <m>R</m> be any ring and <m>I</m> and (two-sided) ideal such that <m>I \ne 0</m>. Then I claim that every non-empty subset of <m>M = R/I</m> is linearly dependent. For say <m>A</m> is a such a nonempty subset. For any <m>a \in A</m>, pick any <m>r \in I</m> such that <m>r \ne 0</m>. Then <m>ra = 0</m> (since <m>a = y + I</m> for some <m>y \in R</m> and hence <m>ra = ry + I = I = 0_M</m>) and this shows <m>A</m> is linearly dependent. In particular, even a one-element subset of <m>M</m> is linearly dependent.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-3-linearly-independent-in-z">
                <title><m>\exe</m> – <m>\{3\}</m> Linearly Independent in <m>\Z</m></title>
      
                <p>
                  The singleton <m>\{3\}</m> is a linearly independent subset of the <m>\Z</m>-module <m>M = \Z</m>. But it does not generate all of <m>M</m>. The subset <m>\{3, 5\}</m> does generate all of <m>M</m>, but it is not linearly independent, since <m>5 \cdot 3 + (-3) \cdot 5 = 0</m>. More on this later.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-basis-module">
                <title><m>\defn</m> – Basis (Module)</title>
      
                <p>
                  A subset <m>A</m> of an <m>R</m>-module <m>M</m> is a <em><m>\defnn{basis}</m></em> of <m>M</m>, if the set <m>A</m> generates <m>M</m> and is linearly independent.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-free-module">
                <title><m>\defn</m> – Free Module</title>
      
                <p>
                  An <m>R</m>-module M is a <em><m>\defnn{free}</m></em> <m>R</m>-module if <m>M</m> admits at least one a basis.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-rank-module">
                <title><m>\defn</m> – Rank (Module)</title>
      
                <p>
                  Let <m>R</m> be a non-zero commutative ring and let <m>M</m> be a free <m>R</m>-module. The cardinality of any basis of <m>M</m> is called the <em><m>\defnn{rank}</m></em> of <m>M</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-zero-module-is-free">
                <title><m>\exe</m> – Zero Module is Free</title>
      
                <p>
                  The zero module is free with <m>\emptyset</m> as (its only) basis. This holds since the empty set is vacuously linearly independent and it generates <m>\{0\}</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-ring-is-free-module-over-itself">
                <title><m>\exe</m> – Ring is Free Module over Itself</title>
      
                <p>
                  <m>R</m> is free since <m>\{1\}</m> is a basis for <m>R</m>. It clearly generates and if <m>r \cdot 1 = 0</m> then <m>r = 0</m>, so it is linearly independent.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-rn-is-free">
                <title><m>\exe</m> – <m>R^n</m> is Free</title>
      
                <p>
                  More generally, <m>R^n</m> is free since <m>e_1, \dots, e_n</m> is a basis. This is called the <m>\defnn{standard basis}</m> of <m>R^n</m>. We’ve already seen that <m>e_1, \dots, e_n</m> generates <m>R^n</m> as an <m>R</m>-module. Suppose <m>\sum r_i e_i = 0</m>. Then <m>\begin{bmatrix} r_1 \\ \vdots \\ r_n \end{bmatrix} = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix}</m> and hence <m>r_i = 0</m> for all <m>i</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-quotient-of-proper-nonzero-ideals-not-free">
                <title><m>\exe</m> – Quotient of Proper Nonzero Ideals Not Free</title>
      
                <p>
                  For any ring <m>R</m>, if <m>I</m> is a (two-sided) ideal such that <m>I \ne 0</m> and <m>I \ne R</m>, then <m>M = R/I</m> is not free. Since <m>I \ne R</m>, <m>R/I</m> is not the zero module and hence the empty set isn’t a basis. Let <m>A</m> be any non-empty subset. Then since <m>I \ne 0</m>, as shown above <m>A</m> is linearly dependent. We conclude that no subset of <m>M</m> is a basis.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-bases-are-not-unique">
                <title><m>\exe</m> – Bases are not Unique</title>
      
                <p>
                  As you likely know from a class in linear algebra, bases of free modules are rarely unique. Indeed, if <m>R</m> is any ring, then any single unit forms a basis for <m>R</m> as a module over itself. For another example, for any ring <m>R</m> and any fixed element <m>r \in R</m>, the set <me>\left\{ \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} r \\ 1 \end{bmatrix} \right \}</me> forms a basis for the free <m>R</m>-module <m>R^2</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-13">
                <title>Problem –</title>
      
                <p>
                  Let <m>R</m> be a commutative ring with <m>1 \neq 0</m>. Show that if every <m>R</m>-module is free then <m>R</m> is a field. #### Problem –
                </p>
      
                <p>
                  An abelian group <m>A</m> is called divisible if for each <m>a \in A</m> and <m>n \in \N</m>, there exists <m>b \in A</m> such that <m>a = nb</m>. Prove that if <m>A \neq\{0_A\}</m> is a divisible abelian group then <m>A</m> is not a free <m>\Z</m>-module. Deduce that <m>\Q</m> is not a free <m>\Z</m>-module.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-14">
                <title>Problem –</title>
      
                <p>
                  Let <m>R</m> be a commutative ring with <m>1 \neq 0</m>. 1. Show that if <m>M</m> is a free <m>R</m>-module, then <m>\ann(M) = 0</m>. 2. Give an example of a ring <m>R</m> an a nonzero module <m>M</m> such that <m>\ann(M) \neq 0</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-15">
                <title><m>\rem</m></title>
      
                <p>
                  A key difference between free modules over rings that are not fields and vector spaces is that not every linearly independent subset of a free module can be extended to a basis. For example, <m>\{3\}</m> is a linearly independent subset of <m>\Z</m>, but it cannot be extended to a basis. Indeed, any set of the form <m>\{3, n \}</m> with <m>n \ne 3</m> is linearly dependent since <m>n \cdot 3 + (-3) \cdot n = 0</m>.
                </p>
      
                <p>
                  Likewise, over arbitrary rings, not every subset that generates a free module necessarily contains a basis. For instance, <m>\{3,5\}</m> generated <m>\Z</m> as a module over itself, but no subset of it is a basis.
                </p>
      
                <p>
                  (Note that <m>\Z</m> has precisely two bases as a module over itself: <m>\{1\}</m> and <m>\{-1\}</m>. )
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-16">
                <title><m>\rem</m></title>
      
                <p>
                  If <m>R</m> is the zero ring, then there is (up to isomorphism) only one <m>R</m>-module, the zero module. (Proof: If <m>M</m> is any module over the <m>0</m> ring, then for each <m>m \in M</m> we have <m>m = 1_R \cdot m = 0_R \cdot m = 0_M</m>.) Checking the definition carefully, we see that both the empty set and the set <m>\{0\}</m> form bases for the zero module over the <m>0</m> ring. This gives an example of a module with two bases of different cardinalities. On the homework you will encounter another example of this sort.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-elements-uniquely-expressible-in-free-modules">
                <title><m>\lem</m> – Elements Uniquely Expressible in Free Modules</title>
      
                <p>
                  Suppose <m>M</m> is a free <m>R</m>-module and <m>A</m> is a basis of <m>M</m>. Then every element of <m>M</m> is uniquely expressible as an <m>R</m>-linear combination of elements of <m>A</m>.
                </p>
      
                <p>
                  More precisely, for each <m>m</m> there is unique family of elements <m>(r_a)_{a \in A}</m>, with <m>r_a \in R</m> for all <m>a</m>, such that <m>r_a = 0</m> for all but a finite number of indices <m>a \in A</m> and <m>m = \sum_{a \in A} r_a a</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-71">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Given <m>m \in M</m>, the fact that <me>m = \sum_a r_a a</me> for at least one family of elements <m>(r_a)_{a \in A}</m> with <m>r_a = 0</m> for all but a finite number of <m>a</m>’s is the definition of what it means for <m>A</m> to generate <m>M</m>.
                  </p>
      
                  <p>
                    Suppose <m>(s_a)_{a \in A}</m> is another such family with <m>m = \sum_a s_a a</m>. Then <me>0 = m-m = \sum_a r_a a - \sum_a s_a a = \sum_a (r_a - s_a) a.</me> Since <m>A</m> is linearly independent, by definition <m>r_a - s_a = 0</m> for all <m>a</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="exe-module-with-infinite-basis">
                <title><m>\exe</m> – Module with Infinite Basis</title>
      
                <p>
                  I have not yet given an example of a module with an infinite basis; here is one. Let <m>R</m> be any ring and <m>M = R[x]</m>[^1] (which, recall, is an <m>R</m>-module due to the evident ring map <m>R \to R[x]</m>). Then the countably infinite set <m>\{1,x,x^2,\dots, x^n,\dots\}</m> is a basis. The fact that this set is a basis is essentially part of the definition of <m>R[x]</m>. The Lemma says that every polynomial is uniquely expressible as an <m>R</m>-linear combination of (a finite subset of) <m>1,x,x^2,\dots</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="thm-ump-for-free-r-modules">
                <title><m>\thm</m> – UMP for Free <m>R</m>-Modules</title>
      
                <p>
                  Let <m>R</m> be a ring, let <m>M</m> be a free <m>R</m>-module with basis <m>B</m>, let <m>N</m> be an <m>R</m>-module, and let <m>j: B \to N</m> be any function. Then there is a unique <m>R</m>-module homomorphism <m>h: M \to N</m> such that <m>h(b) = j(b)</m> for all <m>b \in B</m>.
                </p>
      
                <p>
                  In other words, there is a bijection of sets <me>\Hom_R(M,N) \leftrightarrow\Fun(B, N)</me> given by sending a homomorphisms <m>f: M \to N</m> to its restriction <m>f|_B:B \to N</m> to <m>B</m>. (Here, <m>\Hom_R(M,N)</m> is the set of all <m>R</m>-module homomorphisms from <m>M</m> to <m>N</m> and <m>\Fun(B, N)</m> is the set of all functions from <m>B</m> to <m>N</m>.)
                </p>
      
      
                <paragraphs xml:id="proof.-72">
                  <title><em>Proof.</em></title>
      
                  <p>
                    {} Given a function <m>j: B \to N</m>, define <m>f: M\to N</m> as follows: Given <m>m \in M</m>, by Lemma  <m>m</m> can be written uniquely as a finite sum <m>m = \sum_{b \in B} r_b b</m>. We set <me>
      f(m) = \sum_{b \in B} r_b j(b).
      </me> Note that <m>f</m> is a well-defined function by the uniqueness of the equation <m>m = \sum_{b \in B} r_b b</m>.
                  </p>
      
                  <p>
                    We need to prove <m>f</m> is an <m>R</m>-module homomorphism. I’ll just show it preserves scaling — the proof for addition is similar. Given <m>x \in R</m> and <m>m \in M</m>, we have <m>m = \sum_{b \in B} r_b b</m> for some <m>r_b \in R</m>, and hence <m>xm = \sum_b (xr_b) b</m>. By definition of <m>f</m>, <me>
      f(xm) = \sum_b (xr_b) j(b) = x \sum_b r_b j(b) = x f(m).
      </me>
                  </p>
      
                  <p>
                    Finally, for any <m>y \in B</m> we have <m>y = \sum_{b \in B} r_b b</m> where <m>r_b = 1</m> if <m>b = y</m> and <m>r_b = 0</m> if <m>b \ne y</m>. So <m>f(y) = j(y)</m> by construction. This proves existence.
                  </p>
      
                  <p>
                    {} Let <m>g:M\to N</m> be another <m>R</m>-module homomorphism such that <m>g(b)=g(b)</m> for each <m>b \in B</m>. Given <m>m \in M</m> we have <m>m = \sum_{b \in B} r_b b</m> and hence <me>
      g(m) = \sum_b g(r_b (b)  = \sum_b r_b g(b)  = 
      \sum_b r_b j(b) = f(m).
      </me> and hence <m>g = f</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-17">
                <title><m>\rem</m></title>
      
                <p>
                  The uniqueness only uses that <m>B</m> generates <m>M</m> as an <m>R</m>-module.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="cor-free-modules-with-equal-basis-elements-isomorphic">
                <title><m>\cor</m> – Free Modules with Equal Basis Elements Isomorphic</title>
      
                <p>
                  If <m>M</m> and <m>N</m> are free <m>R</m>-modules having bases of the same cardinality, then <m>M</m> and <m>N</m> are isomorphic <m>R</m>-modules.
                </p>
      
                <p>
                  More precisely, if <m>A</m> is a basis of <m>M</m> and <m>B</m> is a basis of <m>N</m> and <m>j:A\to B</m> is a bijection of sets, then there is a unique <m>R</m>-module isomorphism <m>h: M \xra{\cong} N</m> such that <m>h|_A = j</m>[^1].
                </p>
      
      
                <paragraphs xml:id="proof.-73">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Let <m>h:M\to N</m> and <m>h':N\to M</m> be the <m>R</m>-module homomorphisms induced by the bijection <m>j:A\to B</m> and its inverse <m>j^{-1}:B\to A</m>, respectively, using the UMP for free modules. We’ll show that <m>h</m> and <m>h'</m> are mutual inverses. For this note that <m>h'\circ h:N\to N</m> is an <m>R</m>-module homomorphism and <m>(h'\circ h)(b)=h'(j(b))=j^{-1}(j(b))=b</m> for every <m>b\in B</m>. Since the identity map <m>\id_N</m> is also an <m>R</m>-module homomorphism such that <m>id_N(b)=b</m> for every <m>b\in B</m>, by the uniqueness clause in the UMP, we have <m>h'\circ h=\id_n</m>. Similarly <m>h\circ h'=\id_M</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="exe-mcong-rn-as-r-modules">
                <title><m>\exe</m> – <m>M\cong R^n</m> as <m>R</m>-modules</title>
      
                <p>
                  If <m>M</m> is a free <m>R</m>-module that has a basis of cardinality <m>n</m>, then <m>M \cong R^n</m> as <m>R</m>-modules. This holds since, as seen above, <m>R^n</m> has a basis of cardinality <m>n</m>, namely the standard basis <m>e_1, \dots, e_n</m>.
                </p>
      
                <p>
                  More precisely, as the proof makes clear, if <m>B = \{x_1, \dots, x_n\}</m> is an (ordered) basis of <m>M</m>, there is an isomorphism <m>\phi_B: R^n \cong M</m> that sends <m>\begin{bmatrix} r_1 \\ \vdots \\ r_n \end{bmatrix}</m> to <m>\sum_i r_i x_i</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-18">
                <title><m>\rem</m></title>
      
                <p>
                  Beware that the cardinality of a basis of a free modules is not an isomorphism invariant in general! There exist rings <m>S</m> such that <m>S^n</m> and <m>S^m</m> are isomorphic <m>S</m>-modules for all positive integers <m>n</m> and <m>m</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="thm-uniqueness-of-rank-over-commutative-rings">
                <title><m>\thm</m> – Uniqueness of Rank over Commutative Rings</title>
      
                <p>
                  Let <m>R</m> be a commutative ring such that <m>1 \ne 0</m> and let <m>M</m> be a free <m>R</m>-module with bases <m>A</m> and <m>B</m>. Then <m>A</m> and <m>B</m> have the same rank, i.e. there exists a (non unique) bijection of sets joining them.
                </p>
      
      
                <paragraphs xml:id="proof.-74">
                  <title><em>Proof.</em></title>
      
      Let <m>R</m> be a non-zero commutative ring and let <m>M</m> be a free <m>R</m>-module with two {} bases, <m>A</m> and <m>B</m>. We need to show <m>A</m> and <m>B</m> have the same cardinality. Since <m>A</m> and <m>B</m> are finite, by Example  the assertion is equivalent to the following statement:
      
      <cd>
      \begin{quote} For a non-zero commutative ring $R$,
      if there is an isomorphism $R^n \cong R^m$ of $R$-modules for some integers $n$ and $m$, then $n = m$.
      \end{quote}
      </cd>
      
                  <p>
                    I will prove this statement by taking it as already known that it holds in the special case when <m>R</m> is a field. (We will prove it for fields later.)
                  </p>
      
                  <p>
                    Since <m>R</m> is not the zero ring, it contains at least one maximal ideal <m>I</m>. (This is proven using Zorn’s Lemma.) Recall that <m>R/I</m> is a field.
                  </p>
      
                  <p>
                    Given an isomorphism <m>f: R^n \xra{\cong} R^m</m> of <m>R</m>-modules, by Lemma  we have an induced homomorphism of <m>R/I</m>-module <m>\ov{f}: R^n/IR^n \to R^m /IR^m</m>. Likewise, the inverse map <m>f^{-1}: R^m \xra{\cong} R^n</m> induces a map <m>\ov{f^{-1}}: R^m/IR^m \to R^n /IR^n</m>. Also by that Lemma we have <m>\ov{f} \circ \ov{f^{-1}} = \ov{f \circ f^{-1}} = \ov{\id} = \id</m> and similarly <m>\ov{f^{-1}} \circ \ov{f}</m> is the identity. That is, we have an isomorphism <m>R^n/IR^n \cong R^m /IR^m</m> of <m>R/I</m>-modules.
                  </p>
      
                  <p>
                    Next, I claim that there is an isomorphism <me>
      R^n/IR^n \cong (R/I)^n
      </me> of <m>R/I</m>-modules. Define <m>R^n \to (R/I)^n</m> in the evident way (modding out by <m>I</m> entry-wise). It is a surjective map of <m>R</m>-modules with kernel <m>I R^n</m> and thus, by the First Isomorphism Theorem, it induces an isomorphism <me>
      g: R^n/I R^n \xra{\cong} (R/I)^n
      </me> given by <m>g((r_1, \dots, r_n)^\tau + IR^n) = (r_1 + I, \dots, r_n + I)^\tau</m> (where <m>\tau</m> denotes taking the transpose).<!-- linebreak -->Now, what we have said so far only shows that <m>g</m> is isomorphism of <m>R</m>-modules, but it is easy to see that <m>g</m> is in fact <m>R/I</m>-linear (I’ll leave that to you) and thus it is an isomorphism of <m>R/I</m>-modules.
                  </p>
      
                  <p>
                    Putting the results proven so far together, we conclude that <m>(R/I)^n</m> and <m>(R/I)^m</m> are isomorphic as <m>R/I</m>-modules. Since <m>R/I</m> is a field and since we are assuming the result holds for fields, we deduce that <m>n = m</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="prop-bijection-of-matrices-and-hom-functor">
                <title><m>\prop</m> – Bijection of Matrices and Hom Functor</title>
      
                <p>
                  Suppose <m>R</m> is a ring and <m>F</m> and <m>E</m> are free <m>R</m>-modules with ordered bases <m>X = \{x_1, \dots, x_m\}</m> and <m>Y = \{y_1, \dots, y_n\}</m>, respectively. There is a bijective correspondence <me>\Mat_{n \times m}(R) \to \Hom_R(F, E)</me> that sends a matrix <m>A = (a_{i,j})_{1 \leq i \leq n, 1 \leq j \leq m}</m> to the unique <m>R</m>-module homomorphism <m>T_A: F \to E</m> satisfying <m>T_A(x_j)=\sum_{i= 1}^n a_{i,j} y_i</m> for each <m>1 \leq j \leq m</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-19">
                <title><m>\rem</m></title>
      
                <p>
                  It is of paramount importance to realize that the bijection between maps and matrices {}.
                </p>
      
                <p>
                  In particular, the <m>R</m>-map written as <m>T_A</m> that we associate to the matrix <m>S</m> in this proposition depends on the basis <m>X</m> and <m>Y</m>, and so it should really be written as something like <m>T_A^{X,Y}</m>. I will occasionally do so.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-20">
                <title><m>\rem</m></title>
      
                <p>
                  Let us rephrase the Proposition in terms of usual matrix multiplication in the case when <m>R</m> is {}.
                </p>
      
                <p>
                  Given an <m>R</m>-module homomorphism <m>f: F \to E</m> with <m>F</m> and <m>E</m> as in the proposition, let <m>A</m> be the matrix attached to it using the bases <m>X</m> and <m>Y</m> as described above. Then the diagram #empty
                </p>
      
                <p>
                  commutes, where <m>\phi^F_X</m> and <m>\phi^E_Y</m> are defined in  and  and by <m>A: R^m \to R^n</m> we mean the map <m>v \mapsto Av</m> where <m>Av</m> denotes the usual rule for matrix multiplication. (Recall elements of <m>R^m</m> are represented as column vectors.)
                </p>
      
                <p>
                  Conversely, given a matrix <m>A</m>, the corresponding map <m>T_A^{X,Y}: F \to E</m> is the unique one causing this square to commute; i.e. <m>T_A^{X,Y} = \phi_Y^E \circ A \circ (\phi^F_X)^{-1}</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-21">
                <title><m>\rem</m></title>
      
                <p>
                  For non-commutative rings, there is a version of the previous remark that remains true: Replace the vertical map on the left in diagram  with the map given by <m>v \mapsto (v^\tau A^\tau)^\tau</m> where the exponent <m>\tau</m> denotes taking the transpose of a matrix. (If <m>R</m> is commutative, we have <m>(AB)^\tau = (BA)^\tau</m> for all matrices <m>A</m> and <m>B</m> of an appropriate size, and thus <m>(v^\tau A^\tau)^\tau = (A^\tau)^\tau (v^\tau)^\tau = Av</m>. When <m>R</m> is not commutative, it is {} true in general that <m>(AB)^\tau = (BA)^\tau</m>.)
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-matrix-multiplication-is-associative">
                <title><m>\lem</m> – Matrix Multiplication is Associative</title>
      
                <p>
                  Let <m>R</m> be a commutative ring, let <m>F, E, G</m> be free <m>R</m>-modules having finite bases <m>X = \{x_1, \dots, x_m\}, Y = \{y_1, \dots, y_n\}, Z = \{z_1, \dots, z_p\}</m> of size <m>m,n,p</m>, respectively. Given a <m>n \times m</m> matrix <m>A</m> and a <m>p \times n</m> matrix <m>B</m>, let <m>AB</m> denote the <m>p \times m</m> matrix obtained by the usual formula for matrix multiplication, and let <m>T_A^{X,Y}: F \to E</m>, <m>T_B^{Y,Z}: E \to G</m> and <m>T_{BA}^{X,Z}: F \to G</m> be the maps associated to these matrices relative to the specified bases, as given in Proposition . Then <me>T^{Y,Z}_B \circ T_A^{X,Y} = T_{BA}^{X,Z}.</me> In particular, matrix multiplication is associative. ###### <em>Proof.</em> For the first assertion we just need to check the two maps agree on <m>x_j</m> for all <m>j</m>: We have <me>
      T_B(T_A(x_j)) = T_B(\sum_i a_{i,j} y_i) = \sum_i a_{i,j} T_B(y_i) 
      = \sum_i \sum _l a_{i, j} b_{k, i} z_k
      </me> (with the second equation using that <m>T_B</m> is an <m>R</m>-map) and <me>
      T_{BA}(x_j) = \sum_k (BA)_{k,j} z_k = \sum_k \left(\sum _i b_{k, i} a_{i, j}\right) z_k.
      </me> (I have left off the superscripts <m>(X,Y)</m> etc. for the sake of my sanity.) These two expressions agree since <m>R</m> is commutative.
                </p>
      
                <p>
                  The second assertion holds since composition of functions is associative.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-22">
                <title><m>\rem</m></title>
      
                <p>
                  The correct version of this for <m>R</m>-non commutative is that <m>T_B \circ T_A = T_{(A^\tau B^\tau)^\tau}</m> (again, with superscripts omitted).
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-15">
                <title>Problem</title>
      
                <p>
                  Bases of <m>R</m>. 1. Let <m>R</m> be a non-zero, commutative ring. Prove that a subset <m>A</m> of <m>R</m> is a basis of <m>R</m> as a module over itself if and only if <m>A =\{u\}</m> for some unit <m>u</m> of <m>R</m>. 2. Let <m>R</m> be the ring of column-finite, infinite-by-infinite matrices with entries in a field <m>F</m>, as in exercise #3. Prove that for each positive integer <m>n</m>, there is a subset of <m>R</m> of cardinality <m>n</m> that forms as a basis for <m>R</m> as a module over itself.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-16">
                <title>Problem</title>
      
                <p>
                  Bases of ideals in commutative rings 1. Assume <m>R</m> is a non-zero, commutative ring and <m>I</m> is a non-zero ideal. Prove <m>I</m> is free as an <m>R</m>-module if and only if <m>I = R \cdot a</m> for a non-zero-divisor <m>a</m>. (Recall that an element <m>a</m> is a non-zero-divisor in <m>R</m> provided <m>a \ne 0</m> and <m>xa = 0</m> implies <m>x = 0</m> for all <m>x \in R</m>. ) 2. Let <m>F</m> be a field and <m>R = F[x,y]</m>. Let <m>I = R \cdot \{x, y\}</m>, the ideal consisting of all polynomials with <m>0</m> constant term. Prove <m>I</m> is not free as an <m>R</m>-module.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-5-maximal-linearly-independent">
                <title>Problem 5 – Maximal Linearly Independent</title>
      
                <p>
                  Let <m>R</m> be a commutative integral domain and <m>M</m> an <m>R</m>-module. Recall that a subset <m>S</m> of <m>M</m> is called a <em>maximal linearly independent</em> set of <m>M</m> if <m>S</m> is linearly independent and any subset of <m>M</m> properly containing <m>S</m> is linearly dependent.
                </p>
      
                <p><ol>
                  <li>
                  Let <m>T</m> be a linearly independent subset of <m>M</m>. Prove that <m>T</m> is contained in some maximal linearly independent subset of <m>M</m>.
                  </li>
      
                  <li>
                  Let <m>T</m> be a linearly independent subset of <m>M</m> and <m>N</m> the <m>R</m>-submodule of <m>M</m> generated by <m>T</m>. Prove that <m>T</m> is a maximal linearly independent subset if and only if <m>M/N</m> is torsion. (Recall that an <m>R</m>-module <m>P</m> is called “torsion’’ if for each <m>p \in P</m>, there is a <m>r \in R</m> such that <m>r \ne 0</m> and <m>rp = 0</m>.)
                  </li>
      
                </ol></p>
      
      
                <paragraphs xml:id="proof.-75">
                  <title><em>Proof</em>.</title>
      
      
                  <paragraphs xml:id="part-a-27">
                    <title>Part (a)</title>
      
                    <p>
                      Let <m>A</m> be the set of all linearly independent subsets of <m>M</m> that contain <m>T</m>. We can order <m>A</m> with respect to inclusion. Let <m>X</m> be a totally ordered subset of <m>A</m>, and let <m>U</m> be the union of all elements in <m>X</m>. Let <m>\{u_i\}</m> be a set of elements in <m>U</m> such that <m>\sum_{i=0}^nr_iu_i=0</m> for some <m>r_i\in R</m>, where <m>i\leq n</m> for some <m>n\in\N</m>. As <m>U</m> is the union of all elements in <m>X</m>, there exists some <m>B_1</m> such that <m>u_1\in B_1</m>. However, as <m>X</m> is totally ordered, there exists some <m>B_2</m> such that <m>B_2</m> contains <m>B_1</m> and <m>u_2\in B_2</m>. Continuing in this way, we see that there exists some <m>B_n\in X</m> such that <m>\{u_i\}\subseteq B_n</m>. As <m>B_n</m> is linearly independent, we know that <m>\sum_{i=0}^nr_iu_i=0</m> means that <m>r_i=0</m> for all <m>i</m>. Thus <m>U</m> is indeed linearly independent, making it an upper bound for <m>X</m>. Thus by Zorn’s Lemma there exists a maximal element of <m>A</m>, which we denote <m>S</m>. Thus <m>S</m> is linearly independent, contains <m>T</m>, and is maximal.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-b-25">
                    <title>Part (b)</title>
      
                    <p>
                      <m>(\Rightarrow)</m> Suppose <m>T</m> is maximal linearly independent, and suppose by way of contradiction that <m>M/N</m> is not torsion. Thus there exists some <m>xN\in M/N</m> such that for all <m>r\in R</m>, we see that <m>rx\neq0</m>. However, as <m>x\neq 0</m> and <m>x\not\in N</m>, this means that <m>x\not\in T</m>. Consider <m>T\cup\{x\}</m>. This set is linearly independent, contradicting the assumption that <m>T</m> was maximal.
                    </p>
      
                    <p>
                      <m>(\Leftarrow)</m> Suppose <m>M/N</m> is torsion. Let <m>x\neq0\in M</m> and consider <m>T\cup\{x\}</m>. Consider <m>x+N\in M/N</m>. As <m>M/N</m> is torsion, there exists an <m>r\neq0\in R</m> such that <m>rxN=0</m>. Thus <m>rx\in N</m>. (Note, if <m>x\in N</m>, then <m>r=1</m>). As <m>rx\in N</m> and <m>N</m> is generated by <m>T</m> (<m>N=RT</m>), <m>rx=\sum r_it_i</m>. Subtracting over we see that <m>rx-\sum r_it_i=0</m>. But as <m>r\neq 0</m>, we see that each <m>t_i</m> and <m>x</m> are in <m>T\cup\{x\}</m>, but the sum is 0. Thus <m>T\cup\{x\}</m> is linearly dependent.
                    </p>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-5-unfinished-3">
                <title>Problem 5 #unfinished</title>
      
                <p>
                  Let <m>R</m> be an integral domain and let <m>M</m> be an <m>R</m>-module. Recall that a subset <m>S</m> of <m>M</m><!-- linebreak -->is <em>linearly independent</em> if whenever <m>∑_i r_is_i = 0</m> for ring elements <m>r_i</m> and elements <m>s_i ∈ S</m>, we must have <m>r_i = 0</m> for all <m>i</m>. We say <m>S</m> is <em>maximally linearly independent</em> if it is linearly independent and it is not properly contained in a linear independent subset of <m>M</m> . Finally, recall that we say an <m>R</m>-module <m>P</m> is torsion if for all <m>p ∈ P , rp = 0</m> for some non-zero <m>r\in R</m>.
                </p>
      
                <p><ol>
                  <li>
                  Suppose <m>S</m> is a linearly independent subset of <m>M</m> and let <m>N</m> be the submodule<!-- linebreak -->of <m>M</m> generated by <m>S</m>. Prove it is maximally linearly independent if and only if <m>M/N</m><!-- linebreak -->is torsion.
                  </li>
      
                  <li>
                  Prove that if for every module <m>M</m> every maximally independent subset of <m>M</m> generates <m>M</m> as an <m>R</m>-module, then <m>R</m> must be a field.
                  </li>
      
                </ol></p>
      
              </paragraphs>
            </paragraphs>
          </subsection>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>
        
        <section xml:id="sec-modgen">
          <title>Generated Modules</title>

          <p></p>


          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>

        </section>

        <section xml:id="sec-bases">
          <title>Bases</title>

          <p></p>


          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
            
        </section>

      </chapter>

      <chapter xml:id="ch-vectors">
        <title>Vector Spaces</title>

        <section xml:id="sec-vecbasis">
          <title>Existence of Bases</title>

          <subsection xml:id="bases-of-vector-spaces-and-dimension">
            <title>Bases of Vector Spaces and Dimension</title>
      
      
            <paragraphs xml:id="rem-span">
              <title><m>\rem</m> – Span</title>
      
              <p>
                In keeping with standard convention, we use the term <q>span’’ instead of</q>submodule generated by’’, but they mean exactly the same thing: for a subset <m>A</m> of an <m>F</m>-vector space <m>V</m>, the <em>span</em> of <m>A</m> is <me>\Span(A)=\{\sum_{i=1}^n c_ia_i \mid n\geq 0, c_i\in F, a_{i\in}A\}.</me>As with modules over general rings, the span of a subset of a vector space is a sub-vector space (i.e., sub-module).
              </p>
      
      
              <paragraphs xml:id="lem-linear-independence-and-span">
                <title><m>\lem</m> – Linear Independence and Span</title>
      
                <p>
                  Suppose <m>I</m> is a linearly independent subset of an <m>F</m>-vector space <m>V</m> and <m>v \in V \setminus \Span(I)</m>[^1], then <m>I \cup \{v\}</m> is also linearly independent.
                </p>
      
      
                <paragraphs xml:id="proof.-83">
                  <title><em>Proof.</em></title>
      
                  <p>
                    We need to prove that every finite subset of <m>I \cup \{v\}</m> is linearly independent. Let <m>w_1, \dots, w_n</m> be a list of distinct elements of <m>I \cup \{v\}</m> and suppose <m>\sum_i c_i w_i = 0</m> for some <m>c_i \in F</m>. If <m>v \ne w_i</m> for all <m>i</m>, then <m>c_i = 0</m> for all <m>i</m> since <m>I</m> is linearly independent. Without loss, say <m>w_1 = v</m>. If <m>c_1 \ne 0</m>, then <m>v = \sum_{i \geq 2} c_1^{-1} c_i w_i \in \Span(I)</m>, contrary to the assumption. So we must have <m>c_1 =0</m>. But then <m>c_i = 0</m> for all <m>i \geq 2</m> by the same reasoning as in the first case.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-25">
                <title><m>\rem</m></title>
      
                <p>
                  The only place where the fact that the ring of scalars is a field is to know that <m>c_1</m> has a multiplicative inverse when <m>c_1 \ne 0</m>. In particular, the Lemma holds when <m>F</m> is a division ring too.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="thm-finding-intermediate-bases">
                <title><m>\thm</m> – Finding Intermediate Bases</title>
      
                <p>
                  Let <m>V</m> be an <m>F</m>-vector space and assume <m>I \subseteq S \subseteq V</m> are subsets such that <m>I</m> is linearly independent and <m>S</m> spans <m>V</m>. Then there is a subset <m>B</m> such that <m>I \subseteq B \subseteq S</m> and <m>B</m> is a basis of <m>V.</m>
                </p>
      
      
                <paragraphs xml:id="proof.-84">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Let <m>\cP</m> denote the collection of all subsets <m>X</m> of <m>V</m> such that <m>I\subseteq X \subseteq S</m> and <m>X</m> is linearly independent. We make <m>\cP</m> into a poset by the order relation <m>\subseteq</m>, set containment.
                  </p>
      
                  <p>
                    We note that <m>I \in \cP</m>.
                  </p>
      
                  <p>
                    Let <m>\cT</m> be any totally ordered subset of <m>\cP</m>. If <m>\cT</m> is empty, then <m>\cT</m> is (vacuously) bounded above by <m>I</m>.
                  </p>
      
                  <p>
                    Assume <m>\cT</m> is non-empty. Let <m>Z = \bigcup_{Y \in \cT} Y</m>. I claim <m>Z \in \cP</m>. Given <m>z_1, \dots, z_m \in Z</m>, for each <m>i</m> we have <m>z_i \in Y_i</m> for some <m>Y_{i} \in \cT</m>. Since <m>\cT</m> is totally ordered, one of <m>Y_1, \dots, Y_m</m> contains all the others and hence it contains all the <m>z_i</m>’s. Since each <m>Y_i</m> is linearly independent, this shows <m>z_1, \dots, z_m</m> are linearly independent. We have shown that every finite subset of <m>Z</m> is linearly independent, and hence <m>Z</m> is linearly independent. Since <m>\cT</m> is non-empty, <m>Z \supseteq I</m>. Since each member of <m>\cT</m> is contained in <m>S</m>, <m>Z \subseteq S</m>. Thus, <m>Z \in \cP</m>, and it is clearly an upper bound for <m>\cT</m>.
                  </p>
      
                  <p>
                    We may thus apply Zorn’s Lemma to conclude that <m>\cP</m> has at least one maximal element, <m>B</m>. I claim <m>B</m> is a basis of <m>V</m>. Note that <m>B</m> is linearly independent and <m>I \subseteq B \subseteq S</m> by construction. We need to show that it spans <m>V</m>. Suppose not. Since <m>S</m> spans <m>V</m>, if <m>S \subseteq \Span(B)</m>, then <m>\Span(B)</m> would have to be all of <m>V</m>. (For note that if <m>\Span(S) = V</m> and <m>S \subseteq \Span(B)</m>, then for any <m>v \in V</m> we may write <m>v = \sum_i c_i s_i</m> for <m>s_i \in S</m> and <m>s_i = \sum_j a_{i,j} b_{i,j}</m> with <m>b_{i,j} \in B</m> and hence <m>v= \sum_{i,j} c_i a_{i,j} b_{i,j}</m>, which implies <m>\Span(B) = V</m>.) Since we are assuming <m>\Span(B) \ne V</m>, there must be at least one <m>v \in S</m> such that <m>v \notin \Span(B)</m>. Set <m>X := B \cup \{v\}</m>. Clearly, <m>I \subset X \subseteq S</m> and, by Lemma , <m>X</m> is linearly independent. This shows that <m>X</m> is an element of <m>\cP</m> that is strictly bigger than <m>B</m>, contrary to the maximality of <m>B</m>. So, <m>B</m> must span <m>V</m> and hence it is a basis.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="cor-every-vector-space-has-a-basis">
                <title><m>\cor</m> – Every Vector Space has a Basis</title>
      
                <p>
                  Every <m>F</m>-vector space <m>V</m> has a basis. Moreover, every linearly independent subset of <m>V</m> is contained in some basis, and every set of vectors that spans <m>V</m> contains some basis.
                </p>
      
      
                <paragraphs xml:id="proof.-85">
                  <title><em>Proof.</em></title>
      
                  <p>
                    For this first part, apply the theorem with <m>I = \emptyset</m> and <m>S = V</m>. For the second and third, use <m>I</m> arbitrary and <m>S = V</m> and <m>I = \emptyset</m> and <m>S</m> arbitrary, respectively.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-6-linearly-independent-set-contained-in-basis">
                <title>Problem 6 – Linearly Independent Set Contained in Basis</title>
      
                <p>
                  Let <m>F</m> be a field and <m>V</m> a vector space (not necessarily finite-dimensional) over <m>F</m> . Prove that every linearly independent subset of <m>V</m> is contained in a basis for <m>V</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-86">
                  <title><em>Proof</em>.</title>
      
                  <p>
                    Let <m>I</m> be linearly independent in <m>V</m>. Add elements to it until it spans <m>V</m>, and denote this new set <m>S</m>. Let <m>\cP</m> denote the collection of all subsets <m>X</m> of <m>V</m> such that <m>I\subseteq X \subseteq S</m> and <m>X</m> is linearly independent. We make <m>\cP</m> into a poset by the order relation <m>\subseteq</m>, set containment.
                  </p>
      
                  <p>
                    We note that <m>I \in \cP</m>.
                  </p>
      
                  <p>
                    Let <m>\cT</m> be any totally ordered subset of <m>\cP</m>. If <m>\cT</m> is empty, then <m>\cT</m> is (vacuously) bounded above by <m>I</m>. Assume <m>\cT</m> is non-empty. Let <m>Z = \bigcup_{Y \in \cT} Y</m>. I claim <m>Z \in \cP</m>.
                  </p>
      
                  <p>
                    Given <m>z_1, \dots, z_m \in Z</m>, for each <m>i</m> we have <m>z_i \in Y_i</m> for some <m>Y_i \in\cT</m>. Since <m>\cT</m> is totally ordered, one of <m>Y_1, \dots, Y_m</m> contains all the others and hence it contains all the <m>z_i</m>’s. Since each <m>Y_i</m> is linearly independent, this shows <m>z_1, \dots, z_m</m> are linearly independent. We have shown that every finite subset of <m>Z</m> is linearly independent, and hence <m>Z</m> is linearly independent. Since <m>\cT</m> is non-empty, <m>Z \supseteq I</m>. Since each member of <m>\cT</m> is contained in <m>S</m>, <m>Z \subseteq S</m>. Thus, <m>Z \in \cP</m>, and it is clearly an upper bound for <m>\cT</m>.
                  </p>
      
                  <p>
                    We may thus apply Zorn’s Lemma to conclude that <m>\cP</m> has at least one maximal element, <m>B</m>. I claim <m>B</m> is a basis of <m>V</m>.
                  </p>
      
                  <p>
                    Note that <m>B</m> is linearly independent and <m>I \subseteq B \subseteq S</m> by construction. We need to show that it spans <m>V</m>. Suppose not. Since <m>S</m> spans <m>V</m>, if <m>S \subseteq \Span(B)</m>, then <m>\Span(B)</m> would have to be all of <m>V</m>. (For note that if <m>\Span(S) = V</m> and <m>S \subseteq \Span(B)</m>, then for any <m>v \in V</m> we may write <m>v = \sum_i c_i s_i</m> for <m>s_i \in S</m> and <m>s_i = \sum_j a_{i,j} b_{i,j}</m> with <m>b_{i,j} \in B</m> and hence <m>v= \sum_{i,j} c_i a_{i,j} b_{i,j}</m>, which implies <m>\Span(B) = V</m>.)
                  </p>
      
                  <p>
                    Since we are assuming <m>\Span(B) \ne V</m>, there must be at least one <m>v \in S</m> such that <m>v \notin \Span(B)</m>. Set <m>X := B \cup \{v\}</m>. Clearly, <m>I \subset X \subseteq S</m> and, by Lemma , <m>X</m> is linearly independent. This shows that <m>X</m> is an element of <m>\cP</m> that is strictly bigger than <m>B</m>, contrary to the maximality of <m>B</m>. So, <m>B</m> must span <m>V</m> and hence it is a basis.
                  </p>
      
                  <p>
                    Thus <m>I</m> is contained in the basis <m>B</m>. #### <m>\exe</m> – <m>\R</m> has a basis as a <m>\Q</m>-Vector Space <m>\R</m> has a basis as a <m>\Q</m>-vector space. Just don’t ask me what it looks like.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="cor-basis-of-subspaces-extent">
                <title><m>\cor</m> – Basis of Subspaces Extent</title>
      
                <p>
                  Suppose <m>F</m> is a field an <m>W</m> is a subspace (i.e., submodule) of the <m>F</m>-vector space (i.e., <m>F</m>-module) <m>V</m>. Then every basis of <m>W</m> extends to a basis of <m>V</m> – that is, if <m>B</m> is a basis of <m>W</m> then there exists a basis <m>\tilde{B}</m> of <m>V</m> such that <m>B</m> is a subset of <m>\tilde{B}</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-87">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Just apply the Theorem with <m>B = I</m> and <m>S = V</m>. (Since <m>B</m> is a basis of <m>W</m>, it is linearly independent, and observe that <m>B</m> remains linearly independent when regarded as a subset of <m>V</m>.)
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-26">
                <title><m>\rem</m></title>
      
                <p>
                  It is <em>not</em> true that, with the notation of the previous Corollary, if <m>\tilde{B}</m> is a basis of <m>V</m> then there exists a basis <m>B</m> of <m>W</m> such that <m>B</m> is a subset of <m>\tilde{B}</m>. For instance, take <m>F = \R</m>, <m>V = \R^2</m>, <m>\tilde{B} = \{(1,0), (0,1)\}</m> and <m>W</m> the subspace spanned by <m>(1,1)</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-exchange-lemma">
                <title><m>\lem</m> – Exchange Lemma</title>
      
                <p>
                  Let <m>F</m> be a field, let <m>B</m> be a basis of an <m>F</m>-vector space <m>V</m>, and let <m>C</m> be any finite set of <m>m</m> linearly independent vectors in <m>V</m>. Then there are distinct vectors <m>v_1, \dots, v_m</m> in <m>B</m>, such that <m>(B \setminus \{v_1, \dots, v_m\}) \cup C</m> is also a basis <m>V</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-88">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Let <m>C = \{w_1, \dots, w_m\}</m>. As noted, the sublemma establishes the case <m>w = 1</m> of the Exchange Lemma. The general case proceeds recursively:
                  </p>
      
                  <p>
                    Suppose that for some <m>0 \leq k &lt; m</m>, we have found <m>v_1, \dots, v_k \in B</m> such that <m>B'' := (B \setminus \{v_1, \dots, v_k\}) \cup \{w_1, \dots, w_k\}</m> is a basis for some <m>k &lt; m</m>. We need to show we can “swap out one more’’; that is, we need to prove there is a <m>v_{k+1} \in(B \setminus \{v_1, \dots, v_k\})</m> such that <m>(B'' \setminus \{v_{k+1}\}) \cup \{w_{k+1}\}</m> is also a basis.
                  </p>
      
                  <p>
                    Since <m>B''</m> is a basis, there is a (unique) equation of the form <me>
      w_{k+1} = \sum_{i=1}^n a_i u_i
      </me> with <m>u_i \in B''</m> and <m>a_i \ne 0</m>. Now, there must be at least one <m>u_i</m> that is not in <m>\{w_1, \dots, w_k\}</m>, for otherwise we would have <m>w_{k+1} \in \Span( w_1, \dots, w_k)</m>, contrary to <m>C</m> being linearly independent. Let <m>v_{k+1} = u_i</m> for such an <m>i</m>. Then by the sublemma <me>
      (B'' \setminus \{v_{k+1}\}) \cup \{w_{k+1}\}
      </me> is a basis of <m>V</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="thm-dimension-theorem">
                <title><m>\thm</m> – Dimension Theorem</title>
      
                <p>
                  Any two bases of the same vector space have the same dimension.
                </p>
      
      
                <paragraphs xml:id="proof.-89">
                  <title><em>Proof.</em></title>
      
                  <p>
                    We will only prove this in the case of finite dimensional vector spaces, but it is indeed true in general.
                  </p>
      
                  <p>
                    Suppose <m>F</m> is a field and <m>V</m> is a finite dimensional <m>F</m>-vector space. Then it has a finite basis <m>B</m>. Let <m>B'</m> be any other basis. (Note that we cannot assume <m>B'</m> is necessarily finite.) For any non-negative integer <m>m</m>, suppose <m>C</m> is any <m>m</m>-element subset of <m>B'</m>. Then <m>C</m> is linearly independent and so, by the Exchange Lemma , there is an <m>m</m>-element subset <m>D</m> of <m>B</m> such that <m>(B \setminus D) \cup C</m> is also a basis of <m>V</m>. In particular, <m>m \leq \# B</m>. Since this holds for all <m>m</m>, we conclude <m>|B|' \leq |B|</m>. By symmetry, <m>|B| \leq |B|'</m> and hence <m>\# B= \# B'</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="defn-dimension">
                <title><m>\defn</m> – Dimension</title>
      
                <p>
                  The <em><m>\defnn{dimension}</m></em> of a vector space <m>V</m>, denoted <m>\dim_F(V)</m> or <m>\dim(V)</m>, is the cardinality of any of its bases. This is the same as the rank of <m>V</m> as an <m>F</m>-module.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-dimension-of-fn-in-f">
                <title><m>\exe</m> – Dimension of <m>F^n</m> in <m>F</m></title>
      
                <p>
                  <m>\dim_F(F^n)=|\{e_1,e_2,\dots,e_n\}|=n.</m>[^1]
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-sublemma">
                <title><m>\lem</m> – SubLemma</title>
      
                <p>
                  Let <m>F</m> be a field. Given a basis <m>B</m> of an <m>F</m>-vector space <m>V</m> and vector <m>w \in V</m>, let <me>w = a_1 v_1 + \cdots + a_n v_n,</me>for <m>n \geq 0</m>, <m>0 \ne a_i \in F</m> and <m>v_i \in B</m> for all <m>i</m>, be the unique way of writing <m>w</m> as a linear combination of some of the members of <m>B</m> using non-zero coefficients. Then for any <m>1 \leq i \leq n</m>, we have that <me>B' := (B \setminus \{v_i\}) \cup \{w\}</me> is also a basis of <m>V</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-90">
                  <title><em>Proof.</em></title>
      
                  <p>
                    If <m>w = 0</m> the sublemma is vacously true since <m>n = 0</m> in that case.
                  </p>
      
                  <p>
                    For <m>w \ne 0</m>, we may as well assume (to simplify the notation) that <m>i =1</m>. We need to show <m>B' = (B \setminus \{v_1\}) \cup \{w\}</m> is linearly independent and spans.
                  </p>
      
                  <p>
                    By solving for <m>v_1</m> (as we may since <m>a_1 \ne 0</m>) we obtain <me>
      v_1 = b_1 w + \sum_{j \geq 2} b_j v_j 
      </me> for some <m>b_j \in F</m> and hence <m>v_1 \in \Span(B')</m>. Given any <m>u \in V</m>, since <m>B</m> spans, we have <m>u = \sum_i c_i u_i</m> for some <m>c_i \in F, u_i \in B</m>. If one of <m>u_i</m>’s is equal to <m>v_1</m>, then we may use the previous equation to replace the term <m>c_i u_i</m> with a linear combination of members of <m>B'</m>. This proves that <m>u \in \Span(B')</m>. So <m>B'</m> spans <m>V</m>.
                  </p>
      
                  <p>
                    To show <m>B'</m> is linearly indepedent, suppose <me>
      cw + \sum_{i=1}^p c_i u_i= 0
      </me> for some <m>c, c_i \in F</m> and some <m>u_i \in (B \setminus \{v_1\})</m>. If <m>c \ne 0</m>, then we have <me>
      w = \sum_i d_i u_i
      </me> for some <m>d_i</m>. But this contradicts the uniqueness of <m>w = a_1 v_1 + \cdots + a_n v_n</m> (for note that <m>u_i \ne v_1</m> for all <m>i</m>). So we must have <m>c = 0</m>. But then we must also have <m>c_i = 0</m> for all <m>i</m> since <m>B</m> is linearly independent. This proves <m>B'</m> is linearly independent. This completes the proof of the sublemma.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="exe-sublemma-and-r3">
                <title><m>\exe</m> – SubLemma and <m>\R^3</m></title>
      
                <p>
                  For a visual interpretation of the sublemma, suppose <m>V = \R^3</m> and <m>B =\{i,j,k\}</m> is its standard basis. Given <m>w \in\R^3</m>, if <m>w</m> is a non-zero multiple of one of the members of <m>B</m>, say <m>w = r i</m> for <m>r \ne 0</m>, then <m>\{w, j , k\}</m> is also basis of <m>\R^3</m>. If <m>w</m> lies one of the coordinate planes but is not on a axis, say <m>w = ri + s j</m> for <m>r, s \ne 0</m>, then both <m>\{w,j,k\}</m> and <m>\{i,w,k\}</m> are bases. If <m>w</m> lies on none of the coordinate planes, then <m>w = ri + s j + tk</m> for <m>r, s, t \ne 0</m> and each of <m>\{w,j,k\}</m>, <m>\{i,w,k\}</m> and <m>\{i,j,w\}</m> is a basis. So, the sublemma is saying that we can swap in <m>w</m> for any of the basis elements that occur with a non-zero coefficient in the unique expression of <m>w</m> as a linear combination of the basis. (This includes the case when <m>w = 0</m>, since no such basis vectors occur.)
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-27">
                <title><m>\rem</m></title>
      
                <p>
                  This completes the proof that, for a commutative ring <m>R</m> with <m>1 \ne 0</m>, any two finite bases of a free <m>R</m>-module have the same cardinality. (In the proof of this fact given above, we had assumed it held for fields.)
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-28">
                <title><m>\rem</m></title>
      
                <p>
                  The Exchange Lemma also holds for any division ring (using the exact same proof).
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-rn">
                <title><m>\exe</m> – <m>\R^\N</m></title>
      
                <p>
                  Consider <m>V = \R^\N=\R\times \R\times \R \times \dots</m>, and define rules for addition and scaling degree-wise in the evident way.
                </p>
      
                <p>
                  It is not hard to see <m>V</m> is a <m>\R</m>-vector space. It can be identified with the collection of all sequences <m>\{a_n\}</m> of real numbers. One might be interested in a basis for this vector space. At first glance the most obvious choice would be <m>E=\{e_1,e_2,\ldots\}</m>, where <m>e_i</m> is the sequence with a <m>1</m> in the <m>i</m>-th position and <m>0</m>’s everywhere else.
                </p>
      
                <p>
                  However, this set does not span <m>V</m> as <m>v = (1,1,\ldots)</m> can not be represented as a finite linear combination of these elements. (It turns out that <m>E</m> is the basis for the direct sum <m>\bigoplus_{i\in \N}\R</m>, which may be identified with all sequences having only a finite number of non-zero terms.)
                </p>
      
                <p>
                  Now, since we know since <m>v</m> is not in <m>\Span(E)</m>, we have that <m>E \cup \{v\}</m> is a linearly independent set. However, this does not span either as <m>(1, 2, 3, 4, \dots)</m> is not in the span of this set. We know that <m>V</m> has a basis, but it can be shown that no countable collection of vectors forms a basis for this space, in fact <m>\dim_\R \R^\N =|\R|</m>. An explicit basis of this vector space is impossible to describe.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="thm-dimension-and-subspaces">
                <title><m>\thm</m> – Dimension and Subspaces</title>
      
                <p>
                  Let <m>F</m> be a field and let <m>W</m> be a subspace of a finite dimensional <m>F</m>-vector space <m>V</m>. Then[^1] <me>\dim(V) = \dim(W) + \dim(V/W).</me>
                </p>
      
      
                <paragraphs xml:id="proof.-91">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Pick a basis <m>X</m> of <m>W</m>. Regarded as a subset of <m>V</m>, <m>X</m> remains linearly independent and thus it may be extended to a basis of <m>V</m> by Corollary . Let us write this basis of <m>V</m> as <m>X \cup Y</m> with <m>X \cap Y = \emptyset</m>.
                  </p>
      
                  <p>
                    Let <m>Z := \{y + W \mid y \in Y\} \subseteq V/W</m>. I claim that <m>Z</m> is a basis of <m>V/W</m>.
                  </p>
      
                  <p>
                    Given <m>v + W</m> we have <m>v = \sum_i a_i x_i + \sum_j b_j y_j</m> for some <m>x_i \in X, y_j \in Y</m> and scalars <m>a_i, b_j</m>. Since <m>x + V = 0</m> for all <m>x \in X</m>, we have <m>v + W = \sum_j b_j z_j</m>. This proves <m>Z</m> spans. Say <m>\sum_j c_j y_j + W = 0</m> for some <m>y_1, \dots, y_m \in Y</m>. Then <m>\sum_j c_j y_j \in W</m> and hence <m>\sum_j c_j y_j = \sum_i a_i x_i</m>, whence <m>\sum_j c_j y_j + \sum_i - a_i x_i = 0</m>. Since <m>X \cup Y</m> is linearly independent, <m>c_j = 0</m> and <m>a_i = 0</m> for all <m>j, i</m>. This proves <m>Z</m> is linearly independent.
                  </p>
      
                  <p>
                    We have <me>\dim(V) = |X \cup Y| = |X| + |Y| = |X|+ |Z| = \dim(W) + \dim(V/W).</me>with the second equality holding since <m>X</m> and <m>Y</m> are disjoint.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-29">
                <title><m>\rem</m></title>
      
                <p>
                  Suitably interpreted, this is valid even if <m>V</m> is infinite dimensional, as the proof will show.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-nullspace">
                <title><m>\defn</m> – Nullspace</title>
      
                <p>
                  Let <m>f: V \to W</m> be a linear transformation. The <em><m>\defnn{nullspace}</m></em> of <m>f</m> is <m>\ker(f)</m>[^1].
                </p>
      
                <p>
                  The <em><m>\defnn{nullity}</m></em> of <m>f</m> is <m>\dim(\ker(f))</m>[^2].
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="cor-rank-nullity-theorem">
                <title><m>\cor</m> – Rank Nullity Theorem</title>
      
                <p>
                  Let <m>F</m> be a field and <m>f: V \to W</m> an <m>F</m>-linear transformation between <m>F</m>-vector spaces <m>V</m> and <m>W</m>, and assume <m>V</m> is finite dimensional. Then <me>\dim(V) = \rank(f) + \null(f)</me> or equivalently <me>\dim(V) = \dim(\ker(f)) + \dim(\im(f)).</me>
                </p>
      
      
                <paragraphs xml:id="proof.-92">
                  <title><em>Proof.</em></title>
      
                  <p>
                    By the first isomorphism theorem for modules we have <m>V/\ker(f)\cong\im(f)</m>, thus <m>\dim\left(V/\ker(f)\right)=\dim(\im(f))</m>. By the previous theorem we have <me> \dim(V)=\dim(\ker(f)) \dim\left(V/\ker(f)\right) =\dim(\ker(V))+\dim(\im(f)).</me>
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-30">
                <title><m>\rem</m></title>
      
                <p>
                  Suitably interpreted, this is valid even if <m>V</m> is infinite dimensional.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-9-vector-spaces-and-complements">
                <title>Problem 9 – Vector Spaces and Complements</title>
      
                <p>
                  Let <m>F</m> be a field, <m>V</m> an <m>F</m>-vector space, and <m>W</m> a subspace of <m>V</m>. A subspace <m>U</m> of <m>V</m> is called a <em>complement</em> of <m>W</m> in <m>V</m> if <m>V</m> is the internal direct sum of <m>W</m> and <m>U</m> ; that is, <m>V=W\oplus U</m>.
                </p>
      
                <p><ol>
                  <li>
                  Prove that for every <m>V</m> and <m>W</m> as above, <m>W</m> has at least one complement in <m>V</m>.
                  </li>
      
                  <li>
                  Prove that if <m>U</m> is a complement of <m>W</m> in <m>V</m> and <m>V</m> is finite dimensional, then <m>\dim_F (V ) = \dim_F (W ) + \dim_F (U )</m> (where <m>\dim_F</m> denotes the dimension of an <m>F</m>-vector space).
                  </li>
      
                </ol></p>
      
      
                <paragraphs xml:id="proof.-93">
                  <title><em>Proof</em>.</title>
      
                  <p>
                    Let <m>F</m> be a field, <m>V</m> an <m>F</m>-vector space, and <m>W</m> a subspace of <m>V</m>.
                  </p>
      
      
                  <paragraphs xml:id="part-a-28">
                    <title>Part (a)</title>
      
                    <p>
                      Let <m>A</m> denote the set of all subspaces of <m>V</m> such that <m>A\cap W=\{0\}</m>. We can order <m>A</m> with respect to inclusion. Let <m>X</m> be a totally ordered subset of <m>A</m>, and let <m>B</m> be the union of all the elements in <m>X</m>. Unions of subspaces are subspaces, and by DeMorgan’s Laws we see that <m>B\cap W=\{0\}</m>. Thus by Zorn’s Lemma there exists a maximal element of <m>A,</m> which we denote <m>U</m>. So <m>U\cap W=\{0\}</m> by definition.
                    </p>
      
                    <p>
                      Suppose by way of contradiction there exists some <m>x\in V</m> such that <m>x\not\in U+W</m>.
                    </p>
      
                    <p>
                      Consider <m>U+Vx</m>. As <m>x\not\in W</m>, <m>vx\not\in W</m> for all <m>v\in V</m>, as we could just multiply by <m>v\inv</m>. Thus <m>W\cap U+Vx=\{0\}</m> and <m>U\subsetneq U+Vx</m>, a contradiction, as <m>U</m> was maximal. Thus <m>U</m> is a complement of <m>W</m>.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-b-26">
                    <title>Part (b)</title>
      
                    <p>
                      The Second Isomorphism Theorem tells us that <m>U+W/W\cong U/(U\cap W)</m>. We also know that <me>\dim(U)-\dim(U\cap W)=\dim(U/U\cap W)=\dim(U+W/W)=\dim(U+W)-\dim(W).</me> Put succinctly, <me>\dim(U)-\dim(U\cap W)=\dim(U+W)-\dim(W),</me> and thus <me>\dim(U)+\dim(W)=\dim(U+W)+\dim(U\cap W).</me> As <m>U\cap W=\{0\}</m>, we see that <m>\dim_F(V)=\dim_F(W)+\dim_F(U)</m>.
                    </p>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-4-integral-domain-with-field-subring">
                <title>Problem 4 – Integral Domain with Field Subring</title>
      
                <p><ol>
                  <li>
                  Suppose that <m>A</m> is an integral domain that contains a field <m>F</m> as a subring, and that moreover <m>A</m> is finite dimensional as an <m>F</m>-vector space. Prove that <m>A</m> is a field.
                  </li>
      
                  <li>
                  Give an example to show that the finite dimension condition is necessary.
                  </li>
      
                </ol></p>
      
      
                <paragraphs xml:id="proof.-94">
                  <title><em>Proof.</em></title>
      
      
                  <paragraphs xml:id="part-a-29">
                    <title>Part (a)</title>
      
                    <p>
                      Let <m>A</m> be an integral domain that contains a field <m>F</m> as a subring, and that moreover <m>A</m> is finite dimensional as an <m>F</m>-vector space.
                    </p>
      
                    <p>
                      Say 𝐴 has dimension <m>n</m> over <m>K</m>. Let <m>x\in A,x\neq0</m>. Consider elements <m>1,x,x^2,x^{3,\dots,}</m> which cannot be independent because of the finite dimension. So we may choose <m>m</m> so that <m>1,x,\dots,x^m</m> is linearly dependent over <m>K</m> and is as small as possible. This means that we may find <m>c_0,c_1,\dots,c^{m}\in K</m>, not all equal to <m>0</m>, such that <m>c_0+c_1x+\dots+c_mx^m=0</m>. Note that <m>c_0\neq0</m>, as this would contradict the minimality of <m>m</m>. Then <m>x(c_1+\dots+c)_mx^{m-1})</m> is invertible, so 𝑥 is invertible as well, making <m>A</m> a field.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-b-27">
                    <title>Part (b)</title>
      
                    <p>
                      Consider <m>\Q[x]</m>, which contains the subring <m>\Q</m>. However, this is not a finite dimensional vector space over <m>F</m>, and <m>\Q[x]</m> is not a field, as <m>x</m> has no inverse.
                    </p>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-26">
                <title>Problem</title>
      
                <p>
                  Let <m>F</m> be a field and let <m>V</m> and <m>W</m> be finite dimensional <m>F</m>-vector spaces. 1. Let <m>\a: V \to W</m> be an <m>F</m>-linear transformation. Prove <m>\dim(\coker(\a)) - \dim(\ker(\a)) = \dim(W) - \dim(V)</m>, where by definition <m>\coker(\a) := W/\im(\a)</m>. 2. Let <m>\phi: V \to V</m> be an <m>F</m>-linear transformation from <m>V</m> to itself. Prove that if <m>\phi \circ \phi = 0</m>, then <m>\rank(\phi) \leq \frac12 \dim_F(V)</m>. (Note: Recall that, by definition, <m>\rank(\phi) := \dim_F(\im(\phi))</m>.) 3. For extra credit: Let <m>\phi: V \to V</m> be an <m>F</m>-linear transformation from <m>V</m> to itself. Prove that if <m>\overbrace{\phi \circ \cdots \circ \phi}^{\text{n times}} = 0</m> for some <m>n \geq 1</m> then <m>\rank(\phi) \leq \frac{n-1}{n} \dim_F(V)</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-27">
                <title>Problem</title>
      
                <p>
                  Suppose <m>A \in \Mat_{n \times n}(F)</m> where <m>F</m> is a field and <m>A^k = 0</m> for some integer <m>k \geq 1</m>. Prove <m>A^n = 0</m>. {}: Note that for any <m>m \geq 1</m>, <m>\im(T_A^{m+1}) \subseteq \im(T_A^{m})</m>. Show that if equality holds then <m>A^m = 0</m>. #### Problem Recall that a <m>\Z</m>-module <m>M</m> is called torsion-free if the only element <m>m \in M</m> such that <m>r \cdot m = 0</m>for some non-zero integer <m>r</m> is the element <m>m = 0</m>. Let <m>M = \coker(T_A)</m> where <m>T_A: \Z \to \Z^2</m> is the homomorphism <m>T_A(v) = Av</m> for the matrix <m>A = \begin{bmatrix} 7 \\ 11 \\ \end{bmatrix}</m>. Prove <m>M</m> is torsion-free. #### Problem Let <m>V</m> be a finite dimensional vector space over a field <m>F</m> and <m>f: V \to V</m> an <m>F</m>-linear transformation. Prove the following assertions. 1. There exists an integer <m>s \geq 0</m> such that for <m>n \geq s</m> there are equalities <m>\ker(f^n) = \ker(f^s)</m> and <m>\im(f^n) = \im(f^s)</m>. (Here <m>f^m</m> denotes <m>f</m> composed with itself <m>m</m> times.) 2. <m>\ker(f^s) \cap \im(f^s) = \{0\}</m> for any <m>s</m> as above. 3. <m>V = \ker(f^s) \oplus \im(f^s)</m> for any s as above. (Recall <m>V = U \oplus W</m> for subspaces <m>U</m> and <m>W</m> if <m>U + W = V</m> and <m>U\cap W = \{0\}</m>. )
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-28">
                <title>Problem</title>
      
                <p>
                  Let <m>F</m> be a field, <m>V</m> an <m>F</m>-vector space, and <m>W</m> a subspace of <m>V</m>. A subspace <m>U</m> of <m>V</m> is called a {} of <m>W</m> in <m>V</m> if <m>U \cap W = \{0\}</m> and <m>U + W = V</m>. 1. Prove that for every <m>V</m> and <m>W</m> as above, <m>W</m> has at least one complement in <m>V</m>. 2. Prove that if <m>U</m> is a complement of <m>W</m> in <m>V</m> and <m>V</m> is finite dimensional, then <m>\dim_F(V)=\dim_F(W)+\dim_F(U)</m>. 3. Prove that if <m>T</m> is a subspace of <m>V</m> with <m>V</m> finite dimensional and <m>\dim_F(T) + \dim_F(W) &gt; \dim_F(V)</m>, then <m>T \cap W</m> is non-zero.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-29">
                <title>Problem</title>
      
                <p>
                  Let <m>V</m> be the set of all <m>r \times s</m> matrices over a field <m>F</m>, let <m>G</m> denote the group <m>GL_r(F) \times GL_s(F)</m> (where, recall, <m>GL_m(F)</m> is the group of <m>m \times m</m> invertible matrices with entries in <m>F</m>) and set <m>(A,B)\cdot M=AMB^{-1}</m> for all <m>M \in V</m> and <m>(A,B) \in G</m>. 1. Prove that the formula above defines a group action. 2. Prove that each orbit contains a matrix <m>M = (m_{i,j})</m> such that <m>m_{i,j} = 0</m> for all <m>i \ne j</m> and <m>m_{i,i} \in \{0,1\}</m> for all <m>i</m> 3. How many orbits are there?
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-30">
                <title>Problem –</title>
      
                <p>
                  Let <m>W</m> be a subspace of a vector space <m>V</m>. Show that <m>\dim(V) = \dim(W) + \dim(V/W)</m>. #### Problem – Let <m>F</m> be a field, <m>f\!: V \to W</m> be an <m>F</m>-linear transformation, and <m>\coker(f):=W/\im(f)</m>. Prove that <me>\dim(\coker(f)) + \dim(V) = \dim(W) + \dim(\ker(f)).</me> #### Problem – Let <m>\phi: V \to V</m> be an <m>F</m>-linear transformation. Prove that if <m>\phi \circ \phi = 0</m>, then <me>\dim(\im(\phi))) \leq \frac{1}{2} \dim(V).</me> #### Problem 4 #unfinished Let <m>V</m> be a subspace of a finite-dimensional vector space, <m>W</m>. Recall that a subspace <m>U</m> of <m>W</m> is called a <em>complement</em> of <m>V</m> if <m>U\oplus V = W</m>. Prove the following statements.
                </p>
      
                <p><ol>
                  <li>
                  Every complement of <m>V</m> has dimension <m>\dim W-\dim V</m> .
                  </li>
      
                  <li>
                  If <m>V</m> is not <m>0</m> or <m>W</m>, then <m>V</m> has more than one complement.
                  </li>
      
                  <li>
                  If <m>T</m> is a subspace of <m>W</m> with <m>\dim T + \dim V &gt; \dim W</m> , then <m>T\cap V</m> is non-zero.
                  </li>
      
                </ol></p>
      
              </paragraphs>
            </paragraphs>
          </subsection>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

        <section xml:id="sec-ranknul">
          <title>Rank-Nullity</title>

          <p></p>


          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

      </chapter>

      <chapter xml:id="sec-transforms">
        <title>Linear Transformations</title>

        <section xml:id="sec-basics">
          <title>Basics</title>

          <subsection xml:id="linear-transformations">
            <title>Linear Transformations</title>
      
      
            <paragraphs xml:id="rem-31">
              <title><m>\rem</m></title>
      
              <p>
                Recall that when <m>F</m> is a field an <m>F</m>-module homomorphism is called a <m>F</m>-linear transformation. Since every <m>F</m>-vector space has a basis, every linear transformation between finite dimensional vector spaces may be represented by a matrix, as we noted before.
              </p>
      
      
              <paragraphs xml:id="defn-matrix-of-free-module-homomorphism">
                <title><m>\defn</m> – Matrix of Free Module Homomorphism</title>
      
                <p>
                  Let <m>R</m> be a non-zero commutative ring and let <m>V,W</m>, be <m>R</m>-modules of finite rank <m>n</m> and <m>m</m>, respectively. (For instance, <m>R</m> could be a field and <m>V</m> and <m>W</m> could be arbitrary finite dimensional vector spaces.) Let <m>B=\{b_1, \dots, b_n\}</m> and <m>C=\{c_1, \dots, c_m\}</m> be <em>ordered</em> bases of <m>V</m> and <m>W</m>. 1. For each <m>v \in V</m>, let <m>[v]^B</m> denote the unique column vector <m>\begin{bmatrix} r_1 \\ \vdots \\ r_n \end{bmatrix}</m> such that <m>v = \sum_i r_i b_i</m>. Define <m>[w]^C</m> for <m>w \in W</m> similarly. 2. If <m>f: V \to W</m> is an <m>R</m>-module homomorphism then we define elements <m>a_{ij}\in R</m> for <m>1\leq i\leq m</m> and <m>1\leq j\leq n</m> by the formulas <m>\begin{equation} f(b_i) = \sum_{j=1}^m a_{j,i} c_j. \end{equation}</m> Define <m>[f]_B^C</m> to be the matrix
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-r-vector-space-of-polynomials-p_n">
                <title><m>\exe</m> – <m>\R</m>-Vector Space of Polynomials (<m>P_n</m>)</title>
      
                <p>
                  Let <m>P_n</m> denote the the <m>\R</m>-vector space of polynomials of degree at most <m>n</m> (including the zero polynomial) and consider the linear transformation <m>\phi :P_3\to P_2</m> given by <m>\phi(f)=f'</m>, i.e. taking the derivative. Take <m>B=\{1,x,x^2,x^3\}</m> for an ordered basis of <m>P_3</m> and <m>C = \{1,x,x^2\}</m> for an ordered basis of <m>P_2</m>. Then for <m>p(x) = 5 + 3x + 2x^2 - 7x^3</m> we have <me>
      [p(x)]^B = \begin{bmatrix} 5 \\ 3 \\ 2 \\ -7\end{bmatrix}
      </me> and <me>
      [\phi]_B^C= 
      \begin{bmatrix}
      0 &amp; 1 &amp;0 &amp; 0 \\
      0 &amp;0 &amp; 2 &amp; 0 \\
      0&amp; 0&amp; 0&amp; 3 \\
      \end{bmatrix}.
      </me> We have <me>
      [\phi]_B^C \cdot [p(x)]^B =  
      \begin{bmatrix} 3 \\ 4 \\ -21 \end{bmatrix} = [p'(x)]^C = [\phi(p(x))]^C
      </me> confirming in this example one of the assertions of the Lemma.
                </p>
      
                <p>
                  Let <m>\psi: P_2 \to P_3</m> be the linear map <m>\psi(q(x)) = \int_0^x q(t) \, dt</m>. Then <me>
      [\psi]_C^B= 
      \begin{bmatrix}
      0 &amp; 0 &amp;0  \\
      1 &amp; 0 &amp; 0 \\
      0&amp; 1/2&amp;  0 \\
      0 &amp; 0  &amp; 1/3 \\
      \end{bmatrix}.
      </me> Note that <me>
      [\phi]_B^C \cdot [\psi]_C^B = I_3  = [\phi \circ \psi]_C^C
      </me> and <me>
      [\psi]_C^B \cdot [\phi]_B^C =
      \begin{bmatrix}
      0 &amp; 0 &amp;0 &amp;0 \\
      0 &amp; 1 &amp; 0 &amp;0\\
      0&amp; 0&amp; 1&amp; 0 \\
      0 &amp; 0  &amp; 0 &amp; 1\\
      \end{bmatrix} = [\psi \circ \phi]_B^B
      </me> as predicted by .
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-identity-automorphism-of-free-r-module">
                <title><m>\exe</m> – Identity Automorphism of Free <m>R</m>-Module</title>
      
                <p>
                  If <m>\id_V: V \to V</m> is the identity automorphism of an <m>n</m>-dimensional free <m>R</m>-module <m>V</m>, then for any basis <m>B</m> of <m>V</m> we have <m>\id_V(b_i) = b_i</m> for all <m>i</m> and hence[^1] <me>
      [\id_V]^B_B = I_n.
      </me>
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="prop-when-fmh-matrix-is-invertible">
                <title><m>\prop</m> – When FMH Matrix is Invertible</title>
      
                <p>
                  If <m>f: V \to W</m> is an isomorphism of free modules of finite rank, <m>B</m> is an ordered basis of <m>V</m> and <m>C</m> is an ordered basis of <m>W</m>, then <m>[f]_B^C</m>[^1] is an invertible matrix and <me>[f^{-1}]_C^B = \left([f]_B^C\right)^{-1}.</me>
                </p>
      
              </paragraphs>
            </paragraphs>
          </subsection>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

        <section xml:id="sec-cob">
          <title>Change of Basis</title>

          <subsection xml:id="change-of-basis">
            <title>Change of Basis</title>
      
      
            <paragraphs xml:id="defn-change-of-basis-matrix">
              <title><m>\defn</m> – Change of Basis Matrix</title>
      
              <p>
                Let <m>V</m> be a finite rank free module over a commutative ring <m>R</m>, and let <m>B</m> and <m>B'</m> be bases of <m>V</m>. Let <m>\id_V</m> be the identity map on <m>V</m>. Then <m>[\id_V]_B^{B'}</m> is called the <em><m>\defnn{change of basis matrix}</m></em> from <m>B</m> to <m>B'.</m>
              </p>
      
      
              <paragraphs xml:id="rem-32">
                <title><m>\rem</m></title>
      
                <p>
                  By Proposition, <m>[\id_V]_B^{B'}</m> is an invertible matrix and its inverse <me>\begin{equation}
      \left([\id_V]_B^{B'}\right)^{-1}=[\id_V]_{B'}^B.
      \end{equation}</me> It follows from  that <me>[\id_V]_B^{B'} \cdot [v]^B = [v]^{B'};</me>that is, multiplication by <m>[\id_V]_B^{B'}</m> really does change the representation of vectors by column vectors from one basis to another.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-cob-matrix-and-standard-free-module">
                <title><m>\exe</m> – CoB Matrix and Standard Free Module</title>
      
                <p>
                  Say <m>V = R^n</m>, <m>B</m> is the standard basis (so <m>b_i</m> is a column vector with a <m>1</m> in the <m>i</m>-th position and <m>0</m>’s elsewhere). Note that <m>[v]^B = v</m> for all <m>v</m>.
                </p>
      
                <p>
                  Let <m>B' = \{b'_1, \dots, b'_b\}</m> be any other basis of <m>V</m>. Then <m>[b'_i]^B = b_i</m> and so the change of basis matrix <m>[\id_V]_{B'}^{B}</m> satisfies, on the one hand, <me>[\id_V]_{B'}^{B} [b'_i]^{B'}=[\id_V]_{B'}^{B} b_i = \text{the $i$-th column of $[\id_V]_{B'}^{B}$}</me> and, on the other hand, <me>[\id_V]_{B'}^{B} [b'_i]^{B'}=[b'_i]^B  = b'_i.</me> That is, <m>[\id_V]_{B'}^B</m> is the matrix whose columns are <m>b_1', \dots, b_n'</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-cob-and-p_n">
                <title><m>\exe</m> – CoB and <m>P_n</m></title>
      
                <p>
                  Consider <m>V = P_2</m>, let <m>B = \{1, x, x^2\}</m> and <m>B' = \{1,x-2,(x-2)^2\}</m> be bases of <m>V</m>. We calculate the change of basis matrix. We have <me>\begin{aligned}\id_V(1) &amp;=1 ,\\
      \id_V(x) &amp;=2\cdot1+1\cdot(x-2), \\
      \id_V(x^2) &amp;=4\cdot1 +4\cdot(x-2)+1\cdot(x-2)^2.
      \end{aligned}
      </me> Thus, the change of basis matrix is given by <me>[\id_V]_B^{B'} = \begin{bmatrix}
      1 &amp; 2 &amp; 4\\
      0 &amp; 1 &amp; 4\\
      0 &amp; 0 &amp; 1
      \end{bmatrix}.</me> If we wish to represent <m>f(x) = 1 + x + x^2</m> in the basis <m>B'</m>, we start by noting <m>[f(x)]^B = (1,1,1)^\tau</m> (where recall <m>\tau</m> means to take the transpose) and compute <m>[\id_V]_B^{B'} \cdot (1,1,1)^\tau</m> to get <m>(7, 5, 1)^\tau</m>. This tells us that <me>
      1 + x + x^2 = 7 + 5(x-2) + (x-2)^2.
      </me>
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="prop-cob-and-identity">
                <title><m>\prop</m> – CoB and Identity</title>
      
                <p>
                  Let <m>V, W</m> be finitely generated free modules over a commutative ring <m>R</m>, let <m>B</m> and <m>B'</m> be bases of <m>V</m>, let <m>C</m> and <m>C'</m> be bases of <m>W</m>, and let <m>f: V \to W</m> be a homomorphism. Then[^1] <me>\begin{equation}[f]_{B'}^{C'} = [\id_W]_C^{C'} [f]_B^C [\id_V]_{B'}^{B}.\end{equation}</me>
                </p>
      
      
                <paragraphs xml:id="proof.-95">
                  <title><em>Proof.</em></title>
      
                  <p>
                    This is seen to hold by applying  twice (since <m>\id_W \circ f \circ \id_V = f</m>).
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="defn-equivalent-matrices">
                <title><m>\defn</m> – Equivalent Matrices</title>
      
                <p>
                  Two <m>m \times n</m> matrices <m>X</m> and <m>Y</m> are <em><m>\defnn{equivalent}</m></em> if there exists invertible matrices <m>P</m> and <m>Q</m> (of the appropriate sizes) so that <m>Y = PXQ</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-33">
                <title><m>\rem</m></title>
      
                <p>
                  With this bit of terminology, we can say that two matrices representing the same linear transformation with respect to a different pair of bases are equivalent. The converse is also true: If two matrices are equivalent, then they represent the same linear transformation with respect to different bases.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="thm-finding-equivalent-matrices">
                <title><m>\thm</m> – Finding Equivalent Matrices</title>
      
                <p>
                  Let <m>F</m> be a field. Given an <m>F</m>-linear transformation <m>g: V \to U</m> between finite dimensional <m>F</m>-vector spaces <m>V</m> and <m>U</m>, there are bases <m>B</m> and <m>C</m> of <m>V</m> and <m>U</m>, respectively, such that[^1] <me>[g]_B^C = 
      \begin{bmatrix}
      I_r &amp; 0 \\
      0 &amp; 0 \\
      \end{bmatrix}</me> where <m>r = \rank(g)</m>[^2] and the <m>0</m>’s denote appropriately sized matrices with all <m>0</m> entries.
                </p>
      
      
                <paragraphs xml:id="proof.-96">
                  <title><em>Proof.</em></title>
      
                  <p>
                    We form <m>B</m> and <m>C</m> in steps.
                  </p>
      
                  <p>
                    Start by picking an ordered basis <m>C' = \{w_1, \dots, w_r\}</m> of the image of <m>g</m>. For each <m>i</m> pick <m>v_i \in V</m> such that <m>g(v_i) = w_i</m> and set <m>B' = \{v_1, \dots, v_r\}</m>. Then pick a basis <m>B''</m> of the kernel of <m>g</m>. Let us list the elements of <m>B''</m> as <m>\{v_{r+1}, \dots, v_n\}</m>.
                  </p>
      
                  <p>
                    I claim that <m>B := B' \cup B'' = \{v_1, \dots, v_n\}</m> is a basis of <m>V</m>. (Note that <m>B' \cap B'' = \emptyset</m> since <m>g(v_i) = w_i \ne 0</m> for all <m>i</m> with <m>1 \leq i \leq r</m> and <m>g(v_i) = 0</m> for all <m>i &gt; r</m>.) Pick <m>v \in V</m>. Then, since <m>C'</m> spans the image of <m>g</m>, we have <m>g(v) = \sum_{i=1}^r a_i w_i</m> for some scalars <m>a_i \in F</m>. It follows that <m>v - \sum_{i=1}^r a_i v_i \in \ker(g)</m> and hence, since the kernel is spanned by <m>B''</m>, we have<me>v - \sum_{i=1}^r a_i v_i = \sum_{i=r+1}^n a_i v_i,</me>for some <m>a_{r+1}, \dots, a_n \in F</m>. We rearranging this equation, we conclude <m>v \in \Span(B)</m> and thus <m>B</m> spans <m>V</m>.
                  </p>
      
                  <p>
                    Now say <m>\sum_{i=1}^n c_i v_i = 0</m> for some <m>c_i</m>’s in <m>F</m>. Since <m>g(v_i) = 0</m> for all <m>i &gt; r</m> and <m>g(v_i) = w_i</m> for all <m>i \leq r</m>, this gives that <m>0 = g(0) = g(\sum_{i=1}^n c_i v_i) = \sum_{i=1}^n c_i g(v_i) =\sum_{i=1}^r c_i w_i = 0</m>. Since <m>C'</m> is linearly independent, we have <m>c_i = 0</m> for <m>1 \leq i \leq r</m>. Going back to the original equation
                  </p>
      
                  <p>
                    <m>\sum_{i=1}^n c_i v_i = 0</m>, we see that <m>\sum_{i=1}^r c_i v_i = 0</m> and hence <m>c_i=0</m> for all <m>i</m>, since <m>B''</m> is linearly independent.
                  </p>
      
                  <p>
                    Finally we extend <m>C'</m> to an ordered basis <m>C = \{w_1, \dots, w_r, w_{r+1}, \dots, w_m\}</m> of <m>W</m> arbitrarily.
                  </p>
      
                  <p>
                    By our construction, for any <m>v_i \in B</m> we have <me>
        g(v_i) = \begin{cases}
          w_i &amp; \text{for $1 \leq i \leq r$ and} \\
          0 &amp; \text{for $r+1 \leq i \leq n$} \\
        \end{cases}
        </me> and hence <me>
      [g]_B^C = 
      \begin{bmatrix}
      I_r &amp; 0 \\
      0 &amp; 0 \\
      \end{bmatrix}.
      </me> Note that, in our construction, <m>r = \dim_F(\im(g)) = \rank(g)</m>.[^3]
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="cor-every-mtimes-n-matrix-equivalent-to-unique-matrix">
                <title><m>\cor</m> – Every <m>m\times n</m> Matrix Equivalent to Unique Matrix</title>
      
                <p>
                  Every <m>m \times n</m> matrix <m>A</m> with entries in a field is equivalent to a unique matrix of the form<me>\begin{bmatrix}
      I_r &amp; 0 \\
      0 &amp; 0 \\
      \end{bmatrix}</me> where <m>r</m> is the rank of <m>A</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-97">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Let <m>V = F^n</m> and <m>W = F^m</m> and <m>g: V \to W</m> be the linear transformation given by <m>g(v) = A \cdot v</m>, matrix multiplication. (I have called this map <m>T_A</m> in the past.) If <m>B'</m> and <m>C'</m> are the standard basis of <m>V</m> and <m>W</m> then <m>[g]_{B'}^{C'} = A</m>.
                  </p>
      
                  <p>
                    The Theorem gives that there are bases <m>B</m> and <m>C</m> of <m>V</m> and <m>W</m> such that<me>[g]_B^C = 
       \begin{bmatrix}
      I_r &amp; 0 \\
      0 &amp; 0 \\
      \end{bmatrix}.</me>So, by Proposition , we have <me>A = [g]_{B'}^{C'} = [\id_W]_C^{C'}[g]_B^C [\id_V]_{B'}^{B}=P
      \begin{bmatrix}I_r &amp; 0 \\0 &amp; 0 \\\end{bmatrix}Q</me> with <m>P := [\id_W]_C^{C'}</m> and <m>Q := [\id_V]_{B'}^{B}</m>. Note that <m>P, Q</m> are invertible since they are change of basis matrices (see ).
                  </p>
      
                  <p>
                    The uniqueness of <m>r</m> follows from the fact that <m>r = \rank_F(g)</m>, which does not depend on a choice of bases.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="prop-finding-unique-equivalent-matrix">
                <title><m>\prop</m> – Finding Unique Equivalent Matrix</title>
      
                <p>
                  Let <m>R</m> be a non-zero commutative ring, let <m>V</m> be a free <m>R</m>-module of dimension <m>n</m>, and let <m>B = \{b_1, \dots, b_n\}</m> be any ordered basis of <m>V</m>. If <m>P</m> is any <m>n \times n</m> matrix with entries in <m>R</m> that is invertible (i.e., there is another <m>n \times n</m> matrix <m>P^{-1}</m> such that <m>PP^{-1} = I_n = P^{-1}P</m>), then <m>P = [\id_V]_B^C</m>[^1] for a unique basis <m>C</m> of <m>V</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-98">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Each of the maps<me>
        V \xra{v \mapsto [v]^B} R^n \xra{w \mapsto P^{-1} \cdot w} R^n \xra{(r_1, \dots, r_n)^\tau \mapsto \sum_i r_i b_i} V
        </me> is an isomorphism. (The one on the far right is the inverse of the one on the far left. The middle one is since <m>u \mapsto P \cdot u</m> is a two-sided inverse.) So the composition of all three of these maps, let us call it <m>g</m>, is also an isomorphism. It follows that <m>C = \{g(b_1), \dots, g(b_n)\}</m> is a basis of <m>V</m>. I leave it as an exercise for you to check that <m>P = [\id_V]_B^C</m>.
                  </p>
      
                  <p>
                    To show the uniqueness, say <m>C = \{c_1, \dots, c_n\}</m>, say <m>C' = \{c'_1, \dots, c'_n\}</m> are ordered bases such that <m>[\id_V]_B^{C'} = P = [\id_V]_B^C</m>. Then <me>
        [\id_V]_C^{C'} = [\id_V]_B^{C'} [\id_V]_C^B =  [\id_V]_B^{C'}  \left( [\id_V]_B^{C} \right)^{-1} = PP^{-1}  = I_n.
        </me> For each <m>i</m> we have<me>
       [c_i]^C  =  (0, \dots, 0, 1, 0, \dots, 0)^\tau 
       </me> and<me>
       [c_i]^C  =  I_n [c_i]^C =  [\id_V]_C^{C'} [c_i]^C =  [c_i]^{C'}
      </me>so that<me>
      [c_i]^{C'} =  (0, \dots, 0, 1, 0, \dots, 0)^\tau
      </me>which gives <m>c_i = c_i'</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-5-unfinished-4">
                <title>Problem 5 #unfinished</title>
      
                <p>
                  Let F be a field, <m>V</m> and <m>W</m> finite dimensional <m>F</m>-vector spaces, and <m>g : V\to W</m> an<!-- linebreak --><m>F</m>-linear transformation.
                </p>
      
                <p><ol>
                  <li>
                  Prove that there exists bases of <m>V</m> and <m>W</m> such that the matrix representing <m>g</m> with respect to these bases has the form <me>\begin{bmatrix} I_{r\times r} &amp; 0 \\ 0 &amp; 0 \end{bmatrix},</me> where <m>I_{r\times r}</m> is the <m>r\times r</m> identity matrix and all the <m>0</m>’s denote zero matrices of the appropriate size.
                  </li>
      
                  <li>
                  Prove that the number <m>r</m> appearing in part (a) is independent of choice of bases; that is, if, for another pair of bases of <m>V</m> and <m>W</m>, the matrix representing <m>g</m> has the form <me>\begin{bmatrix} I_{r'\times r'} &amp; 0 \\ 0 &amp; 0 \end{bmatrix}</me>then <m>r'=r</m>.
                  </li>
      
                </ol></p>
      
      
                <paragraphs xml:id="proof.-99">
                  <title><em>Proof</em>.</title>
      
                  <p>
                    Let F be a field, <m>V</m> and <m>W</m> finite dimensional <m>F</m>-vector spaces, and <m>g : V\to W</m> an<!-- linebreak --><m>F</m>-linear transformation.
                  </p>
      
      
                  <paragraphs xml:id="part-a-30">
                    <title>Part (a)</title>
      
                    <p>
                      We form <m>B</m> and <m>C</m> in steps.
                    </p>
      
                    <p>
                      Start by picking an ordered basis <m>C' = \{w_1, \dots, w_r\}</m> of the image of <m>g</m>. For each <m>i</m> pick <m>v_i \in V</m> such that <m>g(v_i) = w_i</m> and set <m>B' = \{v_1, \dots, v_r\}</m>. Then pick a basis <m>B''</m> of the kernel of <m>g</m>. Let us list the elements of <m>B''</m> as <m>\{v_{r+1}, \dots, v_n\}</m>.
                    </p>
      
                    <p>
                      I claim that <m>B := B' \cup B'' = \{v_1, \dots, v_n\}</m> is a basis of <m>V</m>. (Note that <m>B' \cap B'' = \emptyset</m> since <m>g(v_i) = w_i \ne 0</m> for all <m>i</m> with <m>1 \leq i \leq r</m> and <m>g(v_i) = 0</m> for all <m>i &gt; r</m>.) Pick <m>v \in V</m>. Then, since <m>C'</m> spans the image of <m>g</m>, we have <m>g(v) = \sum_{i=1}^r a_i w_i</m> for some scalars <m>a_i \in F</m>. It follows that <m>v - \sum_{i=1}^r a_i v_i \in \ker(g)</m> and hence, since the kernel is spanned by <m>B''</m>, we have<me>v - \sum_{i=1}^r a_i v_i = \sum_{i=r+1}^n a_i v_i,</me>for some <m>a_{r+1}, \dots, a_n \in F</m>. We rearranging this equation, we conclude <m>v \in \Span(B)</m> and thus <m>B</m> spans <m>V</m>.
                    </p>
      
                    <p>
                      Now say <m>\sum_{i=1}^n c_i v_i = 0</m> for some <m>c_i</m>’s in <m>F</m>. Since <m>g(v_i) = 0</m> for all <m>i &gt; r</m> and <m>g(v_i) = w_i</m> for all <m>i \leq r</m>, this gives that <m>0 = g(0) = g(\sum_{i=1}^n c_i v_i) = \sum_{i=1}^n c_i g(v_i) =\sum_{i=1}^r c_i w_i = 0</m>. Since <m>C'</m> is linearly independent, we have <m>c_i = 0</m> for <m>1 \leq i \leq r</m>. Going back to the original equation
                    </p>
      
                    <p>
                      <m>\sum_{i=1}^n c_i v_i = 0</m>, we see that <m>\sum_{i=1}^r c_i v_i = 0</m> and hence <m>c_i=0</m> for all <m>i</m>, since <m>B''</m> is linearly independent.
                    </p>
      
                    <p>
                      Finally we extend <m>C'</m> to an ordered basis <m>C = \{w_1, \dots, w_r, w_{r+1}, \dots, w_m\}</m> of <m>W</m> arbitrarily.
                    </p>
      
                    <p>
                      By our construction, for any <m>v_i \in B</m> we have <me>
      g(v_i) = \begin{cases}
      w_i &amp; \text{for $1 \leq i \leq r$ and} \\
      0 &amp; \text{for $r+1 \leq i \leq n$} \\
      \end{cases}</me>and hence <me>
      [g]_B^C = 
      \begin{bmatrix}
      I_r &amp; 0 \\
      0 &amp; 0 \\
      \end{bmatrix}.
      </me>
                    </p>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
            </paragraphs>
          </subsection>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

        <section xml:id="sec-elematix">
          <title>Elementary Matrices</title>

          <subsection xml:id="elementary-matrices">
            <title>Elementary Matrices</title>
      
      
            <paragraphs xml:id="defn-elementary-basis-change-operations">
              <title><m>\defn</m> – Elementary Basis Change Operations</title>
      
              <p>
                Let <m>R</m> be a commutative ring with <m>1 \neq 0</m>, let <m>M</m> be a free <m>R</m>-module of finite rank <m>n</m>, and let <m>B = \{b_1,\dots ,b_n\}</m> be an ordered basis for <m>M</m>. An <em><m>\defnn{elementary basis change operation}</m></em> on the basis <m>B</m> is one of the following three types of operations: 1. (Type I) Replacing <m>b_i</m> by <m>b_i + rb_j</m> for some <m>i \neq j</m> and some <m>r\in R</m>. 2. (Type II) Replacing <m>b_i</m> by <m>ub_i</m> for some <m>i</m> and some unit <m>u</m> of <m>R</m>, 3. (Type III) Swapping the positions of <m>b_i</m> and <m>b_j</m> for some <m>i \neq j</m>.
              </p>
      
      
              <paragraphs xml:id="defn-elementary-row-operations">
                <title><m>\defn</m> – Elementary Row Operations</title>
      
                <p>
                  Let <m>R</m> be a commutative ring with <m>1 \neq 0</m>. An <em><m>\defnn{elementary row (column) operation}</m></em> on a matrix <m>A \in \Mat_{m,n}(R)</m> is one of the following three types of operations: 1. (Type I) Adding an element of <m>R</m> times a row (column) of <m>A</m> to a different row column of <m>A</m>. 2. (Type II) Multiplying a row (column) of <m>A</m> by a unit of <m>R</m>. 3. (Type III) Interchanging two rows (columns) of <m>A</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-elementary-matrix">
                <title><m>\defn</m> – Elementary Matrix</title>
      
                <p>
                  Let <m>R</m> be a commutative ring with <m>1 \neq 0</m>. An <em><m>\defnn{elementary matrix}</m></em> over <m>R</m> is an <m>n \times n</m> matrix obtained from <m>I_n</m> by applying a single elementary column operation (or, equivalently, a single elementary column operation). In more detail: 1. (Type I) For <m>r \in R</m> and <m>1 \leq i,j \leq n</m> with <m>i \neq j</m>, let <m>E_{i,j}(r)</m> be the type I elementary matrix with <m>1</m>’s on the diagonal, <m>r</m> in the <m>(i,j)</m> position, and <m>0</m> everywhere else. 2. (Type II) For <m>u \in R^\times</m> and <m>1\leq i \leq n</m> let <m>E_i(u)</m> be the type II elementary matrix with <m>(i,i)</m> entry <m>u</m>, <m>(j,j)</m> entry <m>1</m> for all <m>j \neq i</m>, and <m>0</m> everywhere else. 3. Type III) For <m>1 \leq i,j \leq n</m> with <m>i \neq j</m>, let <m>E_{(i,j)}</m> be the type III elementary matrix with <m>1</m> in the <m>(i,j)</m> and <m>(j,i)</m> positions and in the <m>(l,l)</m> positions for all <m>l\not \in \{i,j\}</m>, and 0 in all other entries.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-34">
                <title><m>\rem</m></title>
      
                <p>
                  In particular, the Lemma and the fact that elementary matrices are invertible shows that if <m>A</m> is an <m>n \times n</m> invertible matrix, then the result of performing elementary row or column operations on <m>A</m> results in another invertible matrix.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-properties-of-elementary-matrices">
                <title><m>\lem</m> – Properties of Elementary Matrices</title>
      
                <p>
                  Let <m>E</m> be an <m>n \times n</m> elementary matrix. - For a free <m>R</m>-module <m>V</m> with basis <m>B</m> such that <m>|B| = n</m>, <m>E</m> is the change of basis matrix <m>[\id_V]_{B'}^{B}</m>, where <m>B'</m> is the basis obtained from <m>B</m> by the corresponding elementary basis change operation. - If <m>A \in \Mat_{n,q}(R)</m>, then the product matrix <m>E \cdot A</m> is the result of performing the corresponding elementary column operation on <m>A</m>. - If <m>B \in \Mat_{m,n}(R)</m>, then the product matrix <m>B \cdot E</m> is the result of performing the corresponding elementary column operation on <m>B</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-35">
                <title><m>\rem</m></title>
      
                <p>
                  This is true for commutative rings in general, but the proof is much harder.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-matrix-invertible-iff-columns-span-fn">
                <title><m>\lem</m> – Matrix Invertible iff Columns Span <m>F^n</m></title>
      
                <p>
                  Let <m>F</m> be a field and let <m>A</m> be an <m>n \times n</m> matrix for <m>n \geq 1</m>. <m>A</m> is invertible if and only if its columns span <m>F^n</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-100">
                  <title><em>Proof.</em></title>
      
                  <p>
                    <m>A</m> is invertible if and only if the associated linear map <m>g: F^n \to F^n</m> given by <m>g(v) = Av</m> is an isomorphism. By the rank-nullity Theorem, <m>\rank(g) = n</m> if and only if <m>\ker(g) = \{0\}</m> if and only if <m>g</m> is an isomorphism. The result follows, since <m>\rank(g) = \dim_F \im(g)</m> and <m>\im(g)</m> is the span of the columns of <m>A</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-36">
                <title><m>\rem</m></title>
      
                <p>
                  The previous proof used the following fact implicitly:
                </p>
      
                <blockquote>
                            <p>
                  If <m>W</m> is a subspace of a finite dimensional <m>F</m>-vector space <m>V</m> (where <m>F</m> is a field) and <m>\dim_F(W) = \dim_F(V)</m>, then <m>W = V</m>.
                </p>
                </blockquote>
      
                <p>
                  This holds since any basis of <m>W</m> can be extended to a basis of <m>V</m> and hence if <m>W</m> is properly contained in <m>V</m> we must have <m>\dim_F(W) &lt; \dim_F(V)</m>.
                </p>
      
                <p>
                  This fact fails for commutative rings in general. For instance, let <m>V = \Z</m> and <m>W = \Z \cdot 2</m>. Then <m>V</m> and <m>W</m> are free of rank one and <m>W</m> is a submodule of <m>V</m>, but <m>W \ne V</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="thm-gauss">
                <title><m>\thm</m> – Gauss</title>
      
                <p>
                  Let <m>F</m> be a field and let <m>A</m> be an <m>n \times n</m> invertible matrix for <m>n \geq 1</m>. Then <m>A</m> can be transformed to a matrix of the form <me> \begin{bmatrix}
           u &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
           0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\
           0 &amp; 0 &amp; \ddots &amp; \cdots &amp; 0 \\
           \vdots &amp; &amp; \ddots &amp; \ddots &amp; \vdots  \\
           0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1 \\
      \end{bmatrix}</me> for some <m>u \ne 0</m> via elementary column and column operations of type I.
                </p>
      
      
                <paragraphs xml:id="proof.-101">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Proceed by induction on <m>n</m>. If <m>n = 1</m>, there is nothing to prove (since <m>A</m> cannot be the <m>0</m> matrix.) Assume <m>n &gt; 1</m> and that the result holds for <m>n-1</m> by <m>n-1</m> invertible matrices.
                  </p>
      
                  <p>
                    Let <m>A = (a_{i,j})</m>. Since <m>A</m> is invertible, its column space must have dimension <m>n</m>, by the previous Lemma, and in particular none of its columns can be <m>0</m> (since otherwise the column space would be spanned by <m>n-1</m> vectors and would thus have dimension at most <m>n-1</m>). In particular, its second-to-last column has at least one non-zero entry. By either doing nothing or by adding a suitable row to the last row, we can assume <m>a_{n,n-1} \ne 0</m>. By adding <m>(1 -a_{n,n}) a_{n,n-1}^{-1}</m> times column <m>n-1</m> to column <m>n</m>, we obtain <m>a_{n,n} = 1</m>. Next do suitable column operations of type I to arrive at <m>a_{n,j} = 0</m> for all <m>j &lt; n</m>, and finally do row operations of type I to also get <m>a_{i,n} = 0</m> for all <m>i &lt; n</m>. We have thus transformed <m>A</m> to a matrix of the form <m>\begin{bmatrix} A' &amp; 0 \\ 0 &amp; 1 \end{bmatrix}</m>. Note that this matrix remains invertible (see Remark ) and so the columns of this matrix span <m>F^n</m> by the previous Lemma. Since <m>a_{n,j} = 0</m> for all <m>j &lt; n</m>, the columns of <m>A'</m> must span <m>F^{n-1}</m>. Thus <m>A'</m> is invertible by the previous Lemma. By induction, we can transform <m>A'</m> via type I row and column operations to a matrix of the form as in the statement. Notice that since the <m>a_{n,j} = 0</m> for all <m>j &lt; n</m> and <m>a_{i,n} = 0</m> for all <m>i &lt; n</m>, such operations will also put <m>A</m> into this form.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="cor-invertible-matrices-and-identity-matrix">
                <title><m>\cor</m> – Invertible Matrices and Identity Matrix</title>
      
                <p>
                  If <m>A</m> is invertible then it can be transformed into <m>I_n</m> via elementary column and column operations of type I and II.
                </p>
      
      
                <paragraphs xml:id="proof.-102">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Apply the Theorem and then use a single type II operator; i.e., multiply the first row by <m>u^{-1}</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="cor-square-matrix-invertible-iff-pi-of-elementary-matrices">
                <title><m>\cor</m> – Square Matrix Invertible iff <m>\Pi</m> of Elementary Matrices</title>
      
                <p>
                  If <m>F</m> is a field, then a square matrix is invertible if and only if it is a product of elementary matrices.
                </p>
      
      
                <paragraphs xml:id="proof.-103">
                  <title><em>Proof.</em></title>
      
                  <p>
                    (<m>\Leftarrow</m>) Every elementary matrix is invertible and a product of invertible matrices is again invertible.
                  </p>
      
                  <p>
                    (<m>\Rightarrow</m>) If <m>A</m> is invertible, then by the previous Corollary and Lemma  there exists elementary matrices <m>E_1, \dots, E_s</m>, <m>F_1, \dots, F_t</m> of type I or II such that <me>E_s \cdots E_1 \cdot A \cdot F_1 \cdots F_t= I_n</me> and hence <m>A = E_1^{-1} \cdots E_s^{-1}F_t^{-1} \cdots F^{-1}_1</m>. This proves the statement since the inverse of an elementary matrix is an elementary matrix.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="cor-matrices-equivalent-iff-path-with-operations">
                <title><m>\cor</m> – Matrices Equivalent iff Path with Operations</title>
      
                <p>
                  Given two <m>m \times n</m> matrices <m>A</m> and <m>B</m> with entries in a field, <m>A</m> and <m>B</m> are equivalent if and only if each can be transformed to the other via elementary column and column operations.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-37">
                <title><m>\rem</m></title>
      
                <p>
                  For a ring <m>R</m>, recall that <m>\GL_n(R)</m> is the group of invertible <m>n \times n</m> matrices with entries in <m>R</m>. Let <m>E_n(R)</m> be the subset of <m>\GL_n(R)</m> consisting of all products of elementary matrices of type I. Since the inverse of an elementary matrix of type I is again an elementary matrix of type I, <m>E_n(R)</m> is a subgroup of <m>\GL_n(R)</m>. In fact it is a normal subgroup, at least when <m>n \geq 3</m>, and hence the quotient group <m>\GL_n(R)/E_n(R)</m> is defined in this case.
                </p>
      
                <p>
                  As a consequence of the Theorem above, if <m>R = F</m> is a field then there is an isomorphism of groups <m>\GL_n(F)/E_n(F) \cong F^\times</m> where <m>F^\times = (F \setminus \{0\}, \cdot)</m> is the group of units in <m>F</m>.
                </p>
      
                <p>
                  For a general ring, the extent to which <m>\GL_n(R)/E_n(R)</m> is “larger’’ than <m>R^\times</m> measure the extent to which Theorem fails.
                </p>
      
                <p>
                  The algebraic <m>K</m>-group <m>K_1(R)</m> is defined to be <m>\GL_\infty(R)/E_\infty(R)</m> (and it is usually isomorphic to <m>\GL_n(R)/E_n(R)</m> for <m>n \gg 0</m>). Moreover, <m>E_\infty(R)</m> is the derived subgroup of <m>\GL_\infty(R)</m>, and hence <m>K_1(R)</m> is the abelianization of <m>\GL_\infty(R)</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-9-unfinished-1">
                <title>Problem 9 #unfinished</title>
      
                <p>
                  Let <m>A</m> be a square matrix over the field <m>\C</m> of complex numbers.
                </p>
      
                <p><ol>
                  <li>
                  Suppose <m>A</m> is invertible. Prove that there is a square matrix <m>B</m> over <m>\C</m> such that <m>B^2=A</m>. (Hint: Reduce to the case that <m>A = I + N</m> where <m>I</m> is the identity matrix and <m>N</m> is a nilpotent matrix.)
                  </li>
      
                  <li>
                  Show by example that (a) can fail if <m>A</m> is not assumed to be invertible.
                  </li>
      
                </ol></p>
      
              </paragraphs>
            </paragraphs>
          </subsection>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

        <section xml:id="sec-operator">
          <title>Linear Operators on Free Modules</title>

          <subsection xml:id="linear-operators-on-free-modules">
            <title>Linear Operators on Free Modules</title>
      
      
            <paragraphs xml:id="defn-linear-operator">
              <title><m>\defn</m> – Linear Operator</title>
      
              <p>
                By a <em><m>\defnn{linear operator}</m></em> (or <m>R</m>-linear operator) on <m>V</m> we mean an <m>R</m>-module endomorphism <m>g: V \to V</m> of <m>V</m>.
              </p>
      
      
              <paragraphs xml:id="rem-38">
                <title><m>\rem</m></title>
      
                <p>
                  Let <m>R</m> be a commutative ring (with <m>1 \ne 0</m>) and assume <m>V</m> is a free <m>R</m>-module of finite rank <m>n</m>. Upon choosing an ordered basis <m>B = \{b_1, \dots, b_n\}</m> of <m>V</m>, we may represent <m>g</m> as a <m>n \times n</m> matrix <m>X = [g]_B^B</m> — a key point here is that for operators we usually use the same basis for both the source and target of the map.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-representing-bases">
                <title><m>\exe</m> – Representing Bases</title>
      
                <p>
                  Here is a concrete example. Let <m>R = \R</m>, <m>V</m> the collection of polynomials in <m>x</m> with coefficients in <m>\R</m> of degree at most <m>m</m>. Let <m>D: V \to V</m> be the map sending a polynomial to its derivative. The most obvious choice for a bases of <m>V</m> is <m>1, x, \dots, x^m</m>. With respect to this basis, the map <m>D</m> is represented by the <m>n \times n</m> matrix <me>
      \begin{bmatrix}
      0 &amp; 1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
      0 &amp; 0 &amp; 2 &amp; 0 &amp; \cdots &amp; 0 \\
      0 &amp; 0 &amp; 0 &amp; 3 &amp; \cdots &amp; 0 \\
      0 &amp; 0 &amp; 0 &amp; 0 &amp; \ddots &amp; 0 \\
      0 &amp; 0 &amp; 0 &amp; 0 &amp;  \cdots &amp; m \\
      0 &amp; 0 &amp; 0 &amp; 0 &amp;  \cdots &amp; 0 \\
      \end{bmatrix}.
      </me>
                </p>
      
                <p>
                  If, when <m>m = 2</m>, we instead used <m>1, x^2, 3x-7</m> as a basis, then the matrix for <m>D</m> would be <me>\begin{bmatrix}
      0 &amp;  \frac{14}{3} &amp; 3 \\
      0 &amp;  0 &amp; 0 \\
      0 &amp;  \frac23   &amp; 0\\ 
      \end{bmatrix}.</me>
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-39">
                <title><m>\rem</m></title>
      
                <p>
                  Say <m>X = [g]_B^B</m> is the square matrix representing <m>g</m> with respect to <m>B</m>. If <m>B'</m> is another basis of <m>V</m> and <m>Y = [g]_{B'}^{B'}</m> is the matrix representing <m>g</m> with respect to <m>B'</m> (used for both the source and target of the map), then we have <me>
      Y = P X P^{-1}</me>where <m>P = [\id_V]_{B'}^{B}</m> is the change of basis matrix. This holds since <me>PXP^{-1} = [\id_V]_{B'}^{B} [g]_B^B [\id_V]_{B}^{B'} = [g]_{B'}^{B'} = Y.</me>
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-similar-matrices">
                <title><m>\defn</m> – Similar Matrices</title>
      
                <p>
                  Two <m>n \times n</m> matrices <m>X</m> and <m>Y</m> with entries in a commutative ring <m>R</m> are called <em><m>\defnn{similar}</m></em> if <m>Y = PXP^{-1}</m> for some invertible <m>n \times n</m> matrix <m>P</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-40">
                <title><m>\rem</m></title>
      
                <p>
                  It is easy to decide if two matrices with entries in a field are equivalent — just row reduce them to see if they have the same rank.
                </p>
      
                <p>
                  But it is in general quite difficult to decide if two square matrices with entries in a field are similar. The operation <m>X \mapsto PXP^{-1}</m> can in principle be broken down into a sequence of steps by factoring <m>P</m> into elementary matrices. That is, two square matrices <m>X</m> and <m>Y</m> of the same size are similar if and only if one can be obtained from the other by a sequence of operations of the form <m>Z \mapsto EZE^{-1}</m> where <m>E</m> is an elementary matrix.
                </p>
      
                <p>
                  But such a stap amounts to doing a row operation and simultaneously doing the inverse column operation. There is no simple algorithm, such as Gaussian reduction, to determine whether matrices are similar.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="prop-linear-operators-and-bases">
                <title><m>\prop</m> – Linear Operators and Bases</title>
      
                <p>
                  Let <m>R</m> be a non-zero commutative ring. Given a linear operator <m>g: V \to V</m> on a free <m>R</m>-module <m>V</m> of finite rank <m>n</m>, we have: 1. The matrices representing <m>g</m> with respect to any two choices of bases of <m>V</m> are similar 2. If <m>X</m> represents <m>g</m> with respect to a basis <m>B</m> of <m>V</m> and if <m>Y</m> is similar to <m>X</m>, then there is a basis <m>C</m> of <m>V</m> such that the matrix representing <m>g</m> with respect to <m>C</m> is <m>Y</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-104">
                  <title><em>Proof.</em></title>
      
                  <p>
                    We proved the first assertion above.
                  </p>
      
                  <p>
                    Say <m>X = [g]_B^B</m> and <m>Y = PXP^{-1}</m> for some invertible matrix <m>P</m>. By Proposition  above, <m>P = [\id_V]_B^C</m> for a (unique) new basis <m>C</m>. So <m>Y = [\id]^C_{B} [g]_B^B [\id_V]_C^{B} = [g]_{C}^C</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-5-eigenvalues-and-linear-maps">
                <title>Problem 5 – Eigenvalues and Linear Maps</title>
      
                <p>
                  Suppose that <m>T : V\to V</m> is a linear map, where <m>V</m> is a finite dimensional <m>\C</m>-vector space. Fix a polynomial <m>p\in \C[x]</m>[^1].
                </p>
      
                <p><ol>
                  <li>
                  Prove that if <m>\lambda</m> is an eigenvalue of <m>T</m> then <m>p(\lambda)</m> is an eigenvalue of <m>p(T)</m>.
                  </li>
      
                  <li>
                  Prove conversely that if <m>\mu</m> is an eigenvalue of <m>p(T)</m> then there exists an eigenvalue <m>\l</m> of <m>T</m> such that <m>\mu = p(\l)</m>.
                  </li>
      
                </ol></p>
      
      
                <paragraphs xml:id="proof.-105">
                  <title><em>Proof</em>.</title>
      
      
                  <paragraphs xml:id="part-a-31">
                    <title>Part (a)</title>
      
                    <p>
                      Let <m>\l</m> be an eigenvalue of <m>T</m>. Thus there exists some <m>v</m> such that <m>Tv=\l v</m>. Notice that <m>p(T)v=a_nT^nv+\dots+a_0Iv=0</m> and <m>p(\l)=a_n\l^n+\dots+a_0</m>. Then <me>\begin{align}(p(T)-p(\l I))v&amp;=p(T)v-p(\l I)v\\&amp;=(a_nT^nv+\dots+a_0Iv)-(a_n\l^nv+\dots+a_0v)\\&amp;=a_n(T^n-\l^n)v+\dots a_0(I-I)v\\&amp;=0,\end{align}</me>making <m>p(\l)</m> an eigenvalue of <m>p(T)</m>.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-b-28">
                    <title>Part (b)</title>
      
                    <p>
                      First note that if <m>T</m> is a scalar of the identity matrix then its only eigenvalue is <m>0</m>. Let <m>\mu</m> be an eigenvalue of <m>p(T)</m>. Thus <m>(p(T)-\mu I)v=0</m> for some <m>v</m>. As <m>\C</m> is algebraically closed we can factor the polynomial <m>p(x)-\mu</m> into linear terms: <me>p(x)-\mu=c(x-r_1)\dots(x-r_n)</me>for roots <m>r_i</m>. Note that <m>p(r_i)-\mu=0</m> for all <m>r_i</m>. Observe <me>\begin{align}(p(T)-\mu I)v&amp;=p(T)v-(\mu I)v\\&amp;=c(T-r_nI)\dots(T-r_1I)v\\&amp;=0.\end{align}</me>Thus one of these terms must be sent to zero. Note that if <m>T-r_iI=0</m> for any <m>I</m> this would make <m>T</m> a scaler of the identity matrix. Thus there exists some <m>i</m> such that <m>(T-r_iI)v=0</m>, making <m>r_i</m> an eigenvalue of <m>T</m>. Notice that as <m>p(r_i)-\mu=0</m> we have <m>p(r_i)=\mu</m>, completing the proof.
                    </p>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-31">
                <title>Problem</title>
      
                <p>
                  Let <m>F</m> be a field and consider a monic polynomial <m>f(x)=x^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0</m> in <m>F[x]</m> with <m>n \geqslant 1</m>. 1. Show that the principal ideal <m>(f(x))</m> is a subspace of the <m>F</m>-vector space <m>F[x]</m>. 2. Show that the set <m>B=\{\ov{1},\ov{x},\ldots,\ov{x^{n-1}}\}</m>, where <m>\ov{x^i}=x^i+(f(x))</m>, is a basis for the quotient <m>F</m>-vector space <m>F[x]/(f(x))</m>. 3. Consider the linear transformation <m>\l_x : F [x]/(f (x)) \to F[x]/(f(x))</m> defined by <m>\l_x(v)=\ov{x}v</m> for any <m>v\in F [x]/(f (x))</m>. Find the matrix representing <m>\l_x</m> in the basis <m>B</m> from part b).
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-32">
                <title>Problem</title>
      
                <p>
                  Let <m>F</m> be a field, let <m>V</m> and <m>W</m> be vector spaces over <m>F</m>, let <m>a\!: V \to V</m> and <m>b\!:W \to W</m> be linear transformations and let <m>V_a</m> and <m>W_b</m> be the <m>F[x]</m>-modules they determine. 1. Show that a function <m>g: V_a \to W_b</m> is an <m>F[x]</m>-module homomorphism if and only if 1. <m>g: V \to W</m> is a linear transformation and 2. <m>g \circ a = b \circ g</m>. 2. Suppose that <m>V = F^m = W</m>, and let <m>A, B \in \M_m(F)</m> be the matrices representing the linear transformations <m>a</m> and <m>b</m>, respectively, in the standard basis of <m>F^m</m>. Show that there is an <m>F[x]</m>-module isomorphism <m>V_a \cong W_b</m> if and only if the matrices <m>A</m> and <m>B</m> are similar.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-33">
                <title>Problem</title>
      
                <p>
                  Determine, with justification, if the following two matrices with complex entries are similar.
                </p>
      
                <pre>$$A = \begin{bmatrix}
          0 &amp; -4 &amp; 0 &amp; 0 \\
          1 &amp; 4 &amp; 0 &amp; 0 \\
          0 &amp; 0 &amp; 0 &amp; -4 \\
          0 &amp; 0 &amp; 1 &amp; 4
      \end{bmatrix} \textrm{ and } B = \begin{bmatrix}
          2 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 2
      \end{bmatrix}.$$</pre>
      
              </paragraphs>
            </paragraphs>
          </subsection>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

        <section xml:id="sec-det">
          <title>Determinants</title>

          <subsection xml:id="determinants">
            <title>Determinants</title>
      
      
            <paragraphs xml:id="defn-multilinear-and-alternating">
              <title><m>\defn</m> – Multilinear and Alternating</title>
      
              <p>
                For any commutative ring <m>R</m> and <m>R</m>-module <m>V</m>, given a function of the form <me>\phi: \overbrace{V \times \cdots \times V}^n \to R</me> we say: 1. <m>\phi</m> is <m>R</m>-<em><m>\defnn{mutli-linear}</m></em> if, for each <m>i</m>, we have <me>\phi(v_1, \dots, v_{i-1}, v_i + v'_i, v_{i+1}, \dots, v_n) = \phi(v_1, \dots, v_{i-1}, v_i , v_{i+1}, \dots, v_n) +\phi(v_1, \dots, v_{i-1},  v_i', v_{i+1}, \dots, v_n)</me>and <me>\phi(v_1, \dots, v_{i-1}, r v_i, v_{i+1}, \dots, v_n) = r \phi(v_1, \dots, v_{i-1},  v_i, v_{i+1}, \dots, v_n) </me>for all elements <m>v_1, \dots, v_n, v_i'</m> in <m>V</m> and all <m>r \in R</m>. 2. <m>\phi</m> is <em>alternating</em> if <m>\phi(v_1, \dots, v_n) = 0</m> if <m>v_i = v_j</m> for some <m>i \ne j</m>.
              </p>
      
      
              <paragraphs xml:id="exe-multilinear-maps-when-n1">
                <title><m>\exe</m> – Multilinear Maps when <m>n=1</m></title>
      
                <p>
                  When <m>n=1</m>, a multi-linear map is the same thing as an <m>R</m>-module homomorphism.(The alternating condition is vacuous in this case.)
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-r-bilinear-maps-n2">
                <title><m>\exe</m> – <m>R</m>-Bilinear Maps (<m>n=2</m>)</title>
      
                <p>
                  When <m>n= 2</m>, instead of <q><m>R</m>-multilinear’’ we say</q><m>R</m>-bilinear’<sq>. If <m>\phi: V \times V \to R</m> is <m>R</m>-bilinear then <me>\phi(v+w, u+m) = \phi(v,u) + \phi(v,m) + \phi(w,u)+\phi(w,m).</me> Also <m>\phi(rv,w) = r\phi(v,w) = \phi(v, rw)</m>. For any example of this, take <m>V = R^m</m>. Then the “dot product</sq>’ <m>\phi:V \times V \to R</m> defined by <m>\phi((a_1, \dots, a_m)^\tau, (b_1, \dots, b_m)^\tau) = \sum_i a_i b_i</m> is <m>R</m>-bilinear. But the dot product isn’t alternating.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-determinant-formula-using-bilinear-map">
                <title><m>\exe</m> – Determinant Formula Using Bilinear Map</title>
      
                <p>
                  For <m>n =2</m> and <m>V = R^2</m>, <m>\phi: V \times V \to R</m> defined by <me>\phi(\begin{bmatrix} x \\ y \end{bmatrix}, \begin{bmatrix} z \\ w \end{bmatrix}) = xw - yz</me>is both <m>R</m>-bilinear and alternating. This is of course the familiar determinant formula.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-41">
                <title><m>\rem</m></title>
      
                <p>
                  The multi-linearity property of a function <m>\phi: V^{\times n} \to R</m> can equivalently be stated as follows: If we fixed <m>n-1</m> of the <m>n</m> inputs and let the remaining input vary, then the resulting function is an <m>R</m>-module homomorphism. More precisely: <m>\phi</m> is <m>R</m>-multi-linear if and only if for each <m>i</m> and for each choice of elements <m>v_1, \dots, v_{i-1}, v_{i+1}, \dots, v_n</m> in <m>V</m>, the function <me>
      g: V \to R
      </me> defined by <m>g(w) = \phi(v_1, \dots, v_{i-1}, w, v_{i+1}, \dots, v_n)</m> is a <m>R</m>-module homomorphism.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-multilinear-maps-and-permutations">
                <title><m>\lem</m> – Multilinear Maps and Permutations</title>
      
                <p>
                  Assume <m>\phi: V^{\times n} \to R</m> is <m>R</m>-multilinear and alternating. Then for any permutation <m>\tau \in S_n</m> and <m>v_1, \dots, v_n \in V</m>, we have <me>\phi(v_1, \dots, v_n) = \sign(\tau) \phi(v_{\tau(1)}, \dots, v_{\tau(n)})</me>where <m>\sign(\tau)</m> is the sign of the permutation <m>\tau</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-106">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Let us first prove this in the case when <m>n = 2</m> and <m>\tau</m> is the only non-trivial element of <m>S_2</m>. Say<me>
        \psi: V^{\times 2} \to R
      </me>is <m>R</m>-multi-linear (<m>R</m>-bilinear) and alternating. The goal is to prove<me>
      \psi(v,w) = -\psi(w,v)</me>for any <m>v,w</m>.
                  </p>
      
                  <p>
                    We have <me>\psi(v+w, v+w) = 0</me>on the one hand and <me>\psi(v+w, v+w) = \psi(v, v+w) + \psi(w, v+w) = \psi(v,v) + \psi(v, w) + \psi(w,v) + \psi(w,w) = \psi(v, w) + \psi(w,v)  </me>on the other hand. It follows that <me>
        \psi(v, w) + \psi(w,v)  = 0
        </me>which proves the Lemma in this case.
                  </p>
      
                  <p>
                    Now let <m>n</m> be arbitrary and <m>\tau = (i \, j)</m> for some <m>1 \leq i &lt; j \leq n</m>. In this case we need to show<me>
        \phi(v_1, \dots, v_n) = - \phi(v_1, \dots, v_{i-1}, v_{j}, v_{i+1}, v_{i+2}, \dots, v_{j-1}, v_i, v_{j+1}, \dots, v_n)
        </me>for all <m>v_1, \dots, v_n \in V</m>. Fix <m>n-2</m> elements <m>v_1, \dots, v_{i-1}, v_{i+1}, \dots, v_{j-1}, v_{j+1}, \dots, v_n</m> and define <me>
        \psi: V^{\times 2} \to R
        </me>by<me>
      \psi(v,w) = \phi(v_1, \dots, v_{i-1}, v, v_{i+1}, \dots, v_{j-1}, w,  v_{j+1}, \dots, v_n)
        </me>for any <m>v,w \in V</m>. With this notation the goal is to show <me>\psi(v,w) = -\psi(w,v)</me>Since <m>\phi</m> is <m>R</m>-multi-linear and alternating, <m>\psi: V \times V\to R</m> is <m>R</m>-bilinear and alternating, and so the case already proven gives the result.
                  </p>
      
                  <p>
                    Finally let <m>n</m> and <m>\tau</m> be arbitrary. Then <m>\tau</m> is a product of transpositions — say <m>\tau = \tau_r \cdots \tau_1</m> with each <m>\tau_i</m> a transposition. Proceed by induction on <m>r</m>. The previous case establishes the result when <m>r = 1</m>. If <m>r \geq 2</m>, then using the previous case gives <me>
      \psi(v_\tau(1), \dots, v_\tau(n)) = -\psi(v_\tau'(1), \dots, v_\tau'(n))
      </me> where <m>\tau' = \tau_{r-1} \cdots \tau_1</m> and by induction <m>\psi(v_\tau'(1), \dots, v_\tau'(n)) = (-1)^{r-1}\psi(v_1, \dots, v_n)</m>. Hence <me>
      \psi(v_\tau(1), \dots, v_\tau(n)) = (-1)^{r}\psi(v_1, \dots, v_n)
      </me> which proves the result.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="defn-determinant-function">
                <title><m>\defn</m> – Determinant Function</title>
      
                <p>
                  Define the function <me>\det: \Mat_{n \times n} (R) \to R</me> by <me>\det(A) = \sum_{\s \in S_n} \sign(\s) \prod_{i=1}^n a_{\s(i), i}</me>where <m>S_n</m> is the group of all permutations of <m>\{1, \dots, n\}</m> and <m>\sign(\s)</m> refers to the sign of a permutation.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-determinant-when-n2">
                <title><m>\exe</m> – Determinant when <m>n=2</m></title>
      
                <p>
                  For <m>n = 2</m> we have[^1] <me>\det(\begin{bmatrix} a_{1,1} &amp; a_{1,2} \\ a_{2,1} &amp; a_{2,2} \end{bmatrix}) = a_{1,1} a_{2,2} - a_{2,1} a_{1,2}.</me>
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-determinant-of-upper-triangular-matrix">
                <title><m>\exe</m> – Determinant of Upper Triangular Matrix</title>
      
                <p>
                  If <m>A</m> is upper triangular, then <m>\det(A) = \prod_i a_{i,i}</m>[^1]. For note that for such a matrix the only non-zero terms in the formula for <m>\det(A)</m> occur when <m>\s(i) \leq i</m> for all <m>i</m>, and this can only happen when <m>\s = \id</m>. Similarly, if <m>A</m> is lower triangular, then <m>\det(A) = \prod_i a_{i,i}</m>.
                </p>
      
                <p>
                  In particular, if <m>E</m> is a Type I elementary matrix, then <m>\det(E) = 1</m> and if <m>E</m> is a type II elementary matrix with scalar <m>u \ne 0</m>, then <m>\det(E) = u</m>. We’ll look at the determinants of Type III elementary matrices later.
                </p>
      
                <p>
                  An even more special case is <m>\det(I_n) = 1</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-transpose-preserves-det">
                <title><m>\exe</m> – Transpose Preserves Det</title>
      
                <p>
                  Prove <m>\det(A) = \det(A^\tau)</m>[^1] where <m>\tau</m> denotes transpose.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-42">
                <title><m>\rem</m></title>
      
                <p>
                  Let us write elements of <m>\Mat_{n \times n}(R)</m> as <m>n</m>-tuples of columns vectors; that is, we identify <m>\Mat_{n \times n}(R)</m> with <m>\overbrace{R^n \times \cdots \times R^n}^n</m> by identifying a matrix with the <m>n</m>-tuple of its columns. Then <m>\det</m> is a function of the form <me>
      \det: \overbrace{R^n \times \cdots \times R^n}^n \to R
      </me> where for <m>v_1, \dots, v_n \in R^n</m>, we let <m>\det(v_1, \dots, v_n)</m> be the determinant of the <m>n \times n</m> matrix whose columns are <m>v_1, \dots, v_n</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="thm-uniqueness-of-determinant">
                <title><m>\thm</m> – Uniqueness of Determinant</title>
      
                <p>
                  For each fix positive integer <m>n</m> and non-zero commutative ring <m>R</m>, the determinant function is the unique function of the form <me>\phi: \overbrace{R^n \times \cdots \times R^n}^n \to R</me>such that 1. <m>\phi</m> is <m>R</m>-multilinear, 2. <m>\phi</m> is alternating, and 3. <m>\phi(I_n) =1</m> (i.e., <m>\phi(e_1, \dots, e_n) = 1</m> where <m>e_i</m> is the <m>i</m>-th standard column vector).
                </p>
      
      
                <paragraphs xml:id="proof.-107">
                  <title><em>Proof.</em></title>
      
                  <p>
                    We start by proving that <m>\det</m> has these properties.
                  </p>
      
                  <p>
                    Let <m>v_1, \dots, v_n</m> form the columns of a matrix <m>A</m>. (I.e, the <m>s</m>-th entry of <m>v_t</m> is <m>a_{s,t}</m>.) We have <me>
      \begin{aligned}
      \det(v_1, \dots, v_{i-1}, r v_i + v'_i, v_{i+1}, \dots, v_n) 
      &amp; =  \sum_{\s} \sign(\s) \prod_{j\ne i} a_{\s(j), j} \cdot (r a_{\s(i), i} + a'_{\s(i), i})\\
      &amp; =  r \sum_{\s} \sign(\s) \prod_{j\ne i} a_{\s(j), j} \cdot (a_{\s(i), i})  \\
      &amp; + \sum_{\s} \sign(\s) \prod_{j\ne i} a_{\s(j), j} \cdot (a'_{\s(i), i})  \\
      &amp; = r \det(v_1, \dots, v_{i-1}, v_i , v_{i+1}, \dots, v_n) +
      \det(v_1, \dots, v_{i-1},  v_i', v_{i+1}, \dots, v_n) \\
      \end{aligned}
      </me> and this proves <m>R</m>-multi-linearity.
                  </p>
      
                  <p>
                    For the alternating property, for notational simplicity, let us assume that <m>v_1 = v_2</m> (i.e., the first two columns of the matrix are equal). (The proof of the general case is essentially the same.) Note that<me>
      \det(A) = \sum_{(i,j, g)} \sign(i,j,g) a_{i,1} a_{j,2} a_{g(3), 3} a_{g(4), 4} \cdots a_{g(n), n}
      </me>where the sum ranges over triples <m>(i,j,g)</m> where <m>i \ne j</m> and <m>g</m> is an injective function from <m>\{3, \dots, n\}</m> to <m>\{1, \dots, n\} \setminus \{i,j\}</m> and by <m>\sign(i,j,g)</m> we mean the sign of the permutation sending <m>1</m> to <m>i</m>, <m>2</m> to <m>j</m> and <m>s</m> to <m>g(s)</m> for all other <m>s</m>. For each <m>(i,j,g)</m>, we have <m>\sign(i,j,g) = -\sign(j,i,g)</m>, since the corresponding permutations differ by a transposition. Since <m>a_{i,1} = a_{i,2}</m> and <m>a_{j,1} = a_{j,2}</m>, it follows that the terms in the formula for <m>\det(A)</m> cancel in pairs to give <m>0</m>.
                  </p>
      
                  <p>
                    The property <m>\det(I_n) = 1</m> follows from the formula. (See Example .)
                  </p>
      
                  <p>
                    Let us now prove the uniqueness part of the Theorem:
                  </p>
      
                  <p>
                    Let <m>\phi</m> be any function with the three properties stated in the Theorem. Let <m>A = (a_{i,j})</m> be any square matrix with columns <m>v_1, \dots, v_n</m>. Note that <m>v_j = \sum_i a_{i,j} e_i</m> where <m>e_i</m> is the <m>i</m>-th standard basis (column) vector. Using the <m>R</m>-multi-linearity property repeatedly we have <me>
      \begin{aligned}
      \phi(A) &amp; = \phi(\sum_{i_1} a_{i_1,1} e_{i_1},  \sum_{i_2} a_{i_2,2} e_{i_2}, \cdots, \sum_{i_n} a_{i_n,n} e_{i_n}) \\
      &amp; = \sum_{i_1} a_{i_1,1} \phi(e_{i_1},  \sum_{i_2} a_{i_2,2} e_{i_2}, \cdots, \sum_{i_n} a_{i_n,n} e_{i_n}) \\
      &amp; = \sum_{i_1}a_{i_1,1} \sum_{i_2} a_{i_2,2} \phi(e_{i_1},   e_{i_2}, \cdots, \sum_{i_n} a_{i_n,n} e_{i_n}) \\
      &amp; = \vdots \\
      &amp; = \sum_{ i_1, \dots, i_n } a_{i_1, 1} \cdots a_{i_n,n} \phi(e_{i_1}, \dots, e_{i_n}) \\
      \end{aligned}
      </me> where each of <m>i_1, \dots, i_n</m> ranges from <m>1</m> to <m>n</m>. By the alternating property <m>\phi(e_{i_1}, \dots, e_{i_n}) = 0</m> unless all the <m>i_1, \dots, i_n</m> are distinct. If they are distinct, then by Lemma  we have <me>
      \phi(e_{i_1}, \dots, e_{i_n}) = \sign(\s) \phi(e_1, \dots, e_n)
      </me> where <m>\s</m> is the permutation defined as <m>\s(t) = i_t</m>. Finally, recall <m>\phi(e_1, \dots, e_n) = 1</m> by assumption. Putting all of this together gives<me>
      \phi(A) = \sum_{\text{ distinct  $i_1, \dots, i_n$}} \sign(\text{the permutation $t \mapsto i_t$}) a_{i_1, 1} \cdots a_{i_n,n}.
      </me>This is just a re-writing of the formula for <m>\det</m>; so <m>\phi = \det</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="prop-computing-determinant-of-square-matrix">
                <title><m>\prop</m> – Computing Determinant of Square Matrix</title>
      
                <p>
                  Let <m>R</m> be any non-zero commutative ring. Let <m>A</m> be a square matrix and let <m>B</m> be a matrix obtained form <m>A</m> by a single elementary column operation: #fix 1. If the operation is of type I, <m>\det(B) = \det(A)</m>[^1]. 2. If the operation is of type II, given by multiplying a column of <m>A</m> by a unit <m>u</m>, then <m>\det(B)=u\det(A)</m>. 3. If the operation is of type III, <m>\det(B)=-\det(A)</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-108">
                  <title><em>Proof.</em></title>
      
                  <p>
                    The first claim follows from multi-linearity and alternating properties: For notational simplicity say <m>A = (v_1, v_2, \dots)</m> and <m>B = (v_1 + rv_2, v_2, \dots)</m>. Then<me>
        \det(B) = \det(v_1, v_2, \dots) + r \det(v_2, v_2, \dots) = \det(A) + r \cdot 0 = \det(A)
        </me>The second is immediate from (the second part of) <m>R</m>-multi-linearity. The last is a special case of Lemma .
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-43">
                <title><m>\rem</m></title>
      
                <p>
                  The previous Lemma gives an efficient method of computing the determinant of a square matrix: Apply Gaussian reduction to transform such a matrix <m>A</m> to <m>I_n</m>, keeping track of the operations used. Since <m>\det(I_n) = 1</m>, we can deduce what the <m>\det(A)</m> is in this way, using the Lemma.
                </p>
      
                <p>
                  The number of type I (and type II) operations needed to transform <m>A</m> to <m>I_n</m> is at most about <me>2(n-1) + 2(n-2) + \cdots \approx n^2</me>Each operation involves at most at most <m>n-1</m> multiplications and additions. All total, the number of operations needed to compute the determinant of an <m>n \times n</m> matrix is a polynomial function of <m>n</m> (I think it is cubic, in fact). Observe that the determinant formula involves <m>n!</m> terms each with <m>n</m> products, which is certainly exponential in <m>n</m>.
                </p>
      
                <p>
                  Thus, Gaussian elimination, among other things, gives a polynomial time solution to a computation that at first blush involves an exponential number of calculations.
                </p>
      
                <p>
                  The “permanant’’ of a square matrix is given by the same formula but without the signs. It too involves an exponential (in <m>n</m>) number of calculations. Unlike the determinant, there is no known polynomial time algorithm to compute it. Indeed, if one exists then <m>P = NP</m>! This is a Theorem of Leslie Valiant.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="cor-det-splits-across-elementary-multiplication">
                <title><m>\cor</m> – Det Splits Across Elementary Multiplication</title>
      
                <p>
                  For any commutative ring <m>R</m>, if <m>A</m> is an <m>n \times n</m> matrix and <m>E</m> is an <m>n \times n</m> elementary matrix, then[^1] <me>\det(AE) = \det(A) \det(E)</me>
                </p>
      
      
                <paragraphs xml:id="proof.-109">
                  <title><em>Proof.</em></title>
      
                  <p>
                    This follows from the Proposition and the fact that <m>AE</m> is the result of doing the corresponding column operation on <m>A</m> (see Lemma ), since <me>
        \det(E)  = \begin{cases}
          1 &amp; \text{if $E$ is elementary of type I} \\
          u &amp; \text{if $E$ is elementary of type II with a unit $u$ on the diagonal} \\
          -1 &amp; \text{if $E$ is elementary of type III.} \\
        \end{cases}</me>These formulas follow from the formula for the determinant of an upper or lower triangular matrix — see Example .
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="cor-det-not-0-iff-matrix-invertible">
                <title><m>\cor</m> – Det not <m>0</m> iff Matrix Invertible</title>
      
                <p>
                  For <m>R = F</m> a field, we have <m>\det(A) \ne 0</m>[^1] if and only if <m>A</m> is invertible.
                </p>
      
      
                <paragraphs xml:id="proof.-110">
                  <title><em>Proof.</em></title>
      
                  <p>
                    If <m>A</m> is not invertible, then the column space of <m>A</m> is a proper subspace of <m>F^n</m> and hence the columns of <m>A</m> must be linearly dependent. Say the <m>i\th</m> column is a linear combination of the rest: <m>v_i = \sum_{j \ne i} c_j v_j</m>. Then <me>
      \det(v_1, \dots, v_n) = \sum_{j \ne i} c_j \det(\text{a matrix with the $i$-th and $j$-th columns equal}) = 0.
      </me>If <m>A</m> is invertible, then by Corollary  <m>A</m> can be obtained from <m>I_n</m> via a sequence of elementary column operations. The result thus follows from Proposition  and the fact that <m>\det(I_n) = 1</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-44">
                <title><m>\rem</m></title>
      
                <p>
                  For an arbitrary commutative ring <m>R</m>, the rule is: a square matrix <m>A</m> is invertible if and only if <m>\det(A)</m> is a unit of <m>R</m>. We’ll give a proof later.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="thm-det-splits-across-multiplication-fields">
                <title><m>\thm</m> – Det Splits Across Multiplication: Fields</title>
      
                <p>
                  For a field <m>F</m> and matrices <m>A, B \in \Mat_{n \times n}(F)</m> we have[^1] <me>\det(AB) = \det(A)\det(B).</me> ###### <em>Proof.</em> If <m>A</m> is not invertible, neither is <m>AB</m>, since <m>\im(AB) \subseteq \im(A)</m>, and if <m>B</m> is not invertible, neither is is <m>AB</m>, since <m>\ker(AB) \supseteq \ker(A)</m>. So, by Corollary , if either <m>A</m> or <m>B</m> is not invertible, both sides of the equation are <m>0</m>.
                </p>
      
                <p>
                  Assume now that <m>A</m> and <m>B</m> are both invertible. Then by Corollary  we have <me>
      A = E_1 \cdots E_n</me>and<me>
      B = F_1 \cdots F_m
      </me>and hence <me>
      AB = E_1 \cdots E_n F_1 \cdots F_m
      </me>where the <m>E_i</m>’s and <m>F_j</m>’s are elementary matrices. Applying Corollary  repeatedly gives <me>\det(AB) = \det(E_1 \cdots E_n F_1 \cdots F_{m-1}) \det(F_m) = \cdots= \det(E_1) \cdots \det(E_n) \det(F_1) \cdots \det(F_m)</me> and similarly <me>
      \det(A) \det(B) = \left(\det(E_1) \cdots \det(E_n)\right) \left( \det(F_1) \cdots \det(F_m)\right).
      </me>
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-det-function-is-natural">
                <title><m>\lem</m> – Det Function is Natural</title>
      
                <p>
                  The determinant function <m>\det</m> is “natural’’: If <m>g: R \to S</m> is a homomorphism of non-zero commutative rings and <m>A \in \Mat_{n \times n}(R)</m>, then <me>g \det(A) = \det(g(A))</me> where <m>g(A)</m> denotes the matrix obtained by applying <m>g</m> to the entries of <m>A</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-111">
                  <title><em>Proof.</em></title>
      
                  <p>
                    This follows from the formula for <m>\det</m>, since <m>g</m> commutes with sums and products.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="thm-det-splits-across-multiplication-commutative-ring">
                <title><m>\thm</m> – Det Splits Across Multiplication: Commutative Ring</title>
      
                <p>
                  For any non-zero commutative ring <m>R</m> and matrices <m>A, B \in \Mat_{n \times n}(R)</m>, we have[^1] <me>\det(AB) = \det(A) \det(B) \in R. </me>
                </p>
      
      
                <paragraphs xml:id="proof.-112">
                  <title><em>Proof.</em></title>
      
                  <p>
                    We have already proven that this holds when <m>R</m> is a field.
                  </p>
      
                  <p>
                    We next show that it holds whenever <m>R</m> is an integral domain. In this case, <m>R</m> is a subring of a field <m>F</m> (namely, the field of fractions of <m>R</m>). So, we know that the equation <m>\det(AB) = \det(A) \det(B)</m> holds in <m>F</m> if we interpret <m>A</m> and <m>B</m> as belonging to <m>\Mat_{n \times n}(F)</m>. But the value of <m>\det</m> is the same if we interpret these matrices as having entries in <m>R</m> or in <m>F</m>. So <m>\det(AB) = \det(A) \det(B)</m> holds in <m>R</m>.
                  </p>
      
                  <p>
                    We finally prove that the Theorem for any non-zero commutative ring <m>R</m> by building on the fact that it holds for domains. We do so by contructing a ring homomorphism <m>g: S \to R</m> and matrices <m>X</m> and <m>Y</m> in <m>\Mat_{n \times n}(S)</m> such that <m>S</m> is an integral domain, <m>g(X) = A</m> and <m>g(Y) = B</m>. Granting such a <m>S, g, X</m> and <m>Y</m> exist, the result follows from the naturality of <m>\det</m> (Lemma ). In detail, we know <m>\det(XY) = \det(X) \det(Y)</m>, since <m>S</m> is a domain. Since the rule for multiplying matrices involves only sums and products of ring elements, we have <m>g(XY) = g(X)g(Y)</m>. So<me>
      \begin{aligned}
        \det(AB) &amp; = \det(g(X) g(Y)) 
         = \det(g(XY))  = g \det(XY) \\ 
         &amp; = g(\det(X) \det(Y)) = g(\det(X)) g(\det(Y)) = \det(g(X)) \det(g(Y)) = \det(A) \det(B).
       \end{aligned}</me>It remains to prove such a <m>S</m>, <m>g</m>, <m>X</m> and <m>Y</m> exists. Suppose <m>A = (a_{i,j})</m> and <m>B = (b_{i,j})</m>. Form the polynomial ring<me>
        S := \Z[\{x_{i,j} \mid 1 \leq i \leq n, 1 \leq j \leq n\} \cup \{y_{i,j}\mid 1 \leq i \leq n, 1 \leq j \leq n\}]
        </me>of <m>2 n^2</m> variables with <m>\Z</m>-coefficients. By the UMP for polynomial rings with integer coefficients, since <m>R</m> is commutative, there is a (unique) ring map <m>g: S \to R</m> such that <m>g(x_{i,j}) = a_{i,j}</m> and <m>g(y_{i,j}) = b_{i,j}</m> for all <m>i,j</m>. That is, <m>g</m> is the evaluation map given by setting <m>x_{i,j} = a_{i,j}</m> and <m>y_{i,j} = b_{i,j}</m> for all <m>i,j</m>, and interpreting integers as elements of <m>R</m>. Let <me>X = (x_{i,j}), Y = (y_{i,j})  \in \Mat_{n \times n}(S)</me> be the evident matrices of indeterminants.Then <m>S</m> is a domain, <m>g: S \to R</m> is a ring homomorphism, <m>g(X) = A</m> and <m>g(Y) = B</m>, as desired.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-45">
                <title><m>\rem</m></title>
      
                <p>
                  Another way to deduce the Theorem for arbitrary commutative rings from the case of a domain is to use the following fact: If <m>R</m> is a non-zero commutative ring, there there exists a surjective ring homomorphism of the form <m>g: S \onto R</m> where <m>S</m> is a domain. So see this, let <m>X</m> be a (possibly very large) set of indeterminants such that there is a bijection of sets <m>\rho: X \to R</m>. Let <m>S = \Z[X]</m>, the polynomial ring with integer coefficients in the variables <m>X</m>. So, a typical element of <m>S</m> is a polynomial the form <m>f(x_1, \dots,x_n)</m> for some finite subset <m>\{x_1, \dots, x_n\}</m> of <m>X</m>. By the UMP for polynomial rings, there is a unique ring map <m>g: S \to R</m> such that <m>g(x) = \rho(x)</m> for all <m>x \in X</m>. That is, <m>g</m> sends <m>f(x_1, \dots, x_n)</m> as above to <m>f(\rho(x_1), \dots, \rho(x_n))</m>. The ring map <m>g</m> is clearly onto since for each <m>r \in R</m>, there is an <m>x \in X</m> with <m>\rho(x) = r</m> and hence <m>g(x) = r</m>. Finally, <m>S</m> is an integral domain.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-46">
                <title><m>\rem</m></title>
      
                <p>
                  The key idea in the proof of the Theorem can be used to deduce many other identities over commutative rings from known identities over fields. E.g., it can be used to show that <me>
      A A^{adj} = \det(A) I_n = A^{adj} A
      </me> holds for any commutative ring <m>R</m> and for any <m>A \in \Mat_{n \times n}(R)</m>, where <m>A^{adj}</m> is the “adjugate’’ of <m>A</m>, granting that this equation holds when <m>R</m> is a field. Recall <m>A^{adj}</m> is the <m>n \times n</m> matrix whose <m>(i,j)</m> entry is <m>(-1)^{i+j} \det(A_{i,j})</m> where <m>A_{i,j}</m> is the <m>(n-1) \times (n-1)</m> matrix obtained by deleting the <m>i</m>-th column and <m>j</m>-th row of <m>A</m>. I will likely have you do this as an exercise.
                </p>
      
                <p>
                  Granting <m>A A^{adj} = \det(A) I_n = A^{adj} A</m> holds for any non-zero commutative ring <m>R,</m> we can prove the following:
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-determinantal-technique">
                <title>Problem – Determinantal Technique</title>
      
                <p>
                  Let <m>R</m> be a non-zero commutative ring and <m>A \in \Mat_{n \times n}(R)</m>. Let <m>A^{adj}</m> denote the classical adjugate of <m>A</m> — the <m>(i,j)</m> entry of <m>A^{adj}</m> is defined to be <m>(-1)^{i+j} \det(A_{j,i})</m> where <m>A_{j,i}</m> is <m>(n-1) \times (n-1)</m> matrix obtained by deleting the <m>j</m>-th row and <m>i</m>-th column of <m>A</m>. You may assume without proof that when <m>R</m> is a field, we have <m>A A^{adj} = \det(A) I_n = A^{adj} A</m>. Prove <m>A A^{adj} = \det(A) I_n = A^{adj} A</m> holds for any non-zero commutative ring <m>R</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="prop-matrix-invertible-iff-det-is-a-unit">
                <title><m>\prop</m> – Matrix Invertible iff Det is a Unit</title>
      
                <p>
                  For any non-zero commutative ring <m>R</m> and any <m>A \in \Mat_{n \times n}(R)</m>, <m>A</m> is invertible if and only if <m>\det(A)</m>[^1] is a unit of <m>R</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-113">
                  <title><em>Proof.</em></title>
      
                  <p>
                    If <m>A</m> is invertible then <m>1 = \det(I_n) = \det(AA^{-1}) = \det(A) \det(A^{-1})</m> by the Theorem, and hence <m>\det(A)</m> is a unit.
                  </p>
      
                  <p>
                    If <m>\det(A)</m> is a unit, then, using the equation above, we have <me>
      A (A^{adj} \cdot \frac{1}{\det(A)}) =I_n = (A^{adj} \cdot \frac{1}{\det(A)}) A
      </me> which proves <m>A</m> has a two-sided inverse (namely, <m>A^{adj} \frac{1}{\det(A)}</m>).
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="defn-determinant">
                <title><m>\defn</m> – Determinant</title>
      
                <p>
                  Let <m>R</m> be a commutative ring and <m>V</m> a free <m>R</m>-module of finite rank. Given an endomorphism <m>\a: V \to V</m>, we define its <em><m>\defnn{determinant}</m></em> to be[^1] <me>
      \det(\a) = \det([\a]_B^B)
      </me> where <m>B</m> is any basis of <m>V</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-det-independent-of-basis-choice">
                <title><m>\lem</m> – Det Independent of Basis Choice</title>
      
                <p>
                  The definition of <m>\det(\a)</m> is independent of choice of <m>B</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-114">
                  <title><em>Proof.</em></title>
      
                  <p>
                    If <m>C</m> is another basis, then <me>
      [\a]_C^C = P [\a]_B^B P^{-1}
      </me> for an invertible matrix <m>P</m> (specifically, <m>P = [\id]_B^C</m>). Then <me>
      \det([\a]_C^C)  = \det(P [\a]_B^B P^{-1}) = \det(P) \det([\a]_B^B) \det(P^{-1}) =\det(P) \det(P^{-1}) \det([\a]_B^B) 
      </me> using the Theorem. Also by the Theorem <m>\det(P) \det(P^{-1}) = \det(PP^{-1}) = \det(I_n) = 1</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="exe-companion-matrix">
                <title><m>\exe</m> – Companion Matrix</title>
      
                <p>
                  Let <m>V = F[x]/I</m> where <m>F</m> is a field, <m>I = F[x] \cdot f(x)</m> with <m>f</m> a monic polynomial. Say <m>f(x) = x^n + a_{n-1} x^{n-1} + \cdots + a_1 x + a_0</m>.<!-- linebreak -->Recall that every element of <m>V</m> is uniquely represented by a coset of the form <me>
      \ov{p(x)} := p(x) + I
      </me> where <m>p(x)</m> is a polynomial of degree at most <m>n-1</m>.
                </p>
      
                <p>
                  We will regard <m>V</m> as an <m>F</m>-vector space (via restriction of scalars along <m>F \into F[x]</m>). Then <m>V</m> is finite dimensional — for instance, a basis of <m>V</m> is given by <m>\ov{1}, \ov{x}, \dots, \ov{x^{n-1}}</m>.
                </p>
      
                <p>
                  Let <m>g: V \to V</m> be the function given as multiplication by <m>x</m>. Then <m>g</m> is an <m>F</m>-linear operator, since <m>g(v+v') = x(v+v') = xv + xv' = g(v) + g(v')</m> and <m>g(cv) = x(cv) = c (xv) = c g(v)</m>. (In fact, <m>g</m> is <m>F[x]</m>-linear, but we won’t use that fact.) Relative to the ordered basis <m>\ov{1}, \ov{x}, \dots, \ov{x^{n-1}}</m>, the matrix of <m>g</m> is <me>
      C(f) :=
      \begin{bmatrix}
        0 &amp; 0 &amp; \cdots &amp; 0 &amp; -a_0 \\
        1 &amp; 0 &amp; \cdots &amp; 0 &amp; -a_1 \\
         0 &amp; 1 &amp; \cdots &amp; 0 &amp; -a_2 \\
         \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
        0 &amp; 0 &amp; \cdots &amp; 1 &amp; -a_{n-1} \\   
      \end{bmatrix}
      </me> The right-most column is due to the fact that, since <m>\overline{f(x)} = 0</m> in <m>V</m>, we have <me>
      x \cdot \overline{x^{n-1}} =
      \overline{x^n} = -a_{n-1} \overline{x^{n-1}} -a_{n-2} \overline{x^{n-2}} - \cdots - a_1 \overline{x} - a_0 \overline{1}.
      </me> The matrix <m>C(f)</m> is known as the <m>\defnn{companion matrix}</m> of <m>f(x)</m>} – it is defined for any monic polynomials with entries in a field.
                </p>
      
                <p>
                  We have <m>\det(C(f)) = (-1)^n a_0</m>, since the only permutation that gives a non-zero term in the formula for <m>\det</m> is the <m>n</m>-cycle <m>\s = (2 \, 3 \, \dots \, n \, 1)</m>, and its has sign is <m>(-1)^{n-1}</m>. So <m>\det(g) = (-1)^n a_0</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-47">
                <title><m>\rem</m></title>
      
                <p>
                  As we showed above, the determinant of linear operator on a free module of finite rank is an “invariant’’ of the operator, in the sense that it is independent of how we represent the operator as a matrix. Here is another such invariant:
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-trace">
                <title><m>\defn</m> – Trace</title>
      
                <p>
                  For a commutative ring <m>R</m> and a square matrix <m>A \in \Mat_{n \times n}(R)</m>, the <em><m>\defnn{trace}</m></em> of <m>A</m>, written <m>\trace(A)</m>, is the sum of its diagonal entries: <m>\trace(A) = \sum_i a_{i,i}</m>.
                </p>
      
                <p>
                  If <m>V</m> is a free <m>R</m>-module of finite rank and <m>g: V \to V</m> is an <m>R</m>-module homomorphism, we define <m>\trace(g)</m> to be <m>\trace([g]^B_B)</m>[^1] for any basis <m>B</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-trace-is-commutative-with-two-matrices">
                <title><m>\lem</m> – Trace is Commutative with Two Matrices</title>
      
                <p>
                  If <m>R</m> is a commutative ring and <m>X \in \Mat_{m \times n}(R)</m> and <m>Y \in \Mat_{n \times m}(R)</m>, then <m>\trace(XY) = \trace(YX)</m>[^1].
                </p>
      
                <p>
                  In particular, with the notation of the previous definition, <m>\trace(g)</m> is well-defined.
                </p>
      
      
                <paragraphs xml:id="proof.-115">
                  <title><em>Proof.</em></title>
      
                  <p>
                    The <m>(i,i)</m> entry of <m>XY</m> is <m>\sum_l x_{i,l} y_{l, i}</m> and so <m>\trace(XY) = \sum_i \sum_l x_{i,l} y_{l,i}</m>. The <m>(j,j)</m> entry of <m>YX</m> is <m>\sum_t y_{j,t} x_{t,j}</m> and so <m>\trace(YX) = \sum_j \sum_t y_{j,t} x_{t,j}</m>. Since <m>R</m> is commutative, these are equal.
                  </p>
      
                  <p>
                    Say <m>B</m> and <m>C</m> are two bases of <m>V</m>. Then <m>[g]_B^B = P [g]_{C}^{C} P^{-1}</m> for some invertible matrix <m>P</m> (namely, <m>P = [\id_V]_C^B</m>). Using the first part of the Lemma we have <me>\trace([g]_B^B) = \trace(P [g]_{C}^{C} P^{-1}) = \trace((P [g]_{C}^{C}) P^{-1}) = 
      \trace(P^{-1} (P [g]_{C}^{C})) =
      \trace((P^{-1} P) [g]_{C}^{C})) =
      \trace([g]_{C}^{C}).</me>
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="war-1">
                <title><m>\war</m></title>
      
                <p>
                  The Lemma tells us that the trace of a product of two matrices in either order is the same, but it does not follow that the trace is unchanged if you permute arbitrarily a product of three of more matrices. For instance, <m>\trace(XYZ)</m> is usually not equal to <m>\trace(XZY)</m>.
                </p>
      
                <p>
                  It is true that trace is invariant under all “cyclic permutations’’ of products of matrices. For instance, <m>\trace(XYZ) = \trace(ZXY) = \trace(YZX)</m> and more generally we have <me>
      \trace(X_1 \cdots X_n) = \trace(X_i X_{i+1} \cdots X_n X_1 X_2 \cdots X_{i-1})
      </me> for all <m>i</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-trace-and-multiplication-by-x">
                <title><m>\exe</m> – Trace and Multiplication by <m>x</m></title>
      
                <p>
                  With the notation of Example , <m>\trace(g) = -a_{n-1}</m>.[^1]
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-det-and-trace-of-c-and-linear-operator">
                <title><m>\exe</m> – Det and Trace of <m>\C</m> and Linear Operator</title>
      
                <p>
                  Let <m>\C</m> denote the complex numbers. We may regard <m>\C</m> as an <m>\R</m>-vector space. Given <m>z = a + bi \in \C</m>, multiplication by <m>z</m> on <m>\C</m> is an <m>\R</m>-linear operator. What are the determinant and trace of this operator?
                </p>
      
                <p>
                  Relative to the basis <m>1, i</m> of <m>\C</m> as a real vector space, multiplication by <m>z = a = ib</m> is represented by the matrix <me>\begin{bmatrix}
          a &amp; -b \\ 
          b &amp; a   \\
        \end{bmatrix}</me> and hence the determinant of this operator is <m>a^2 + b^2</m>, the square of the norm of <m>z</m>. The trace is <m>2a</m>. Note that the determinant of this operator is <m>\|z\|^2</m> the square of the usual complex norm.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-matrix-of-field-extension">
                <title><m>\exe</m> – Matrix of Field Extension</title>
      
                <p>
                  Similarly, we may consider the field extension <m>\Q \subseteq \Q(\sqrt{-2})</m>. For any <m>a + b \sqrt{-2}</m> in the larger field, define <m>g: \Q(\sqrt{-2}) \to \Q(\sqrt{-2})</m> to be multiplication by <m>a + b \sqrt{-2}</m>. Then <m>\Q(\sqrt{-2})</m> is a <m>\Q</m>-vector space and <m>g</m> is a <m>\Q</m>-linear operator. A <m>\Q</m>-basis of <m>\Q(\sqrt{-2}</m> is given by <m>B = \{1, \sqrt{-2}\}</m> and relative to this basis the matrix representing <m>g</m> is <me>[g]_B^B = 
        \begin{bmatrix}
          a &amp; -2b \\ 
          b &amp; a   \\
        \end{bmatrix}</me> and hence <m>\det(g) = a^2 + 2b^2</m>, once again the square of the of usual complex norm of the element <m>a + b \sqrt{-2}</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-characteristic-polynomial">
                <title><m>\defn</m> – Characteristic Polynomial</title>
      
                <p>
                  Let <m>A \in \Mat_{n \times n}(F)</m> where <m>F</m> is a field. The <em><m>\defnn{characteristic polynomial}</m></em> of <m>A</m> is[^1] <me>\cp_A(x) := \det(x I_n - A) \in F[x].</me> Note that <m>\cp_A(x)</m> is a monic polynomial of degree <m>n</m> with coefficients in <m>F</m>. More generally, if <m>V</m> is a finite dimensional <m>F</m>-vector space and <m>g: V \to V</m> is an <m>F</m>-linear operator on <m>V</m>, then <me>\cp_g(x) := \cp_A(x)</me> where <m>A</m> is the matrix representing <m>g</m> with respect to a choice of basis of <m>V</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-48">
                <title><m>\rem</m></title>
      
                <p>
                  We should pause to check that <m>\cp_g(x)</m> is well-defined: Say <m>B</m> and <m>C</m> are two ordered bases of <m>V</m>, so that <m>[g]_B^B</m> and <m>[g]_C^C</m> both represent <m>g</m>. Then <m>[g]_C^C = P [g]_B^B P^{-1}</m> for some invertible matrix <m>P</m> and hence <me>
      P(xI_n - [g]_B^B)P^{-1} = PxI_nP^{-1} - P [g]_B^B P^{-1} = xI_n - [g]_C^C,
      </me> since the scalar matrix <m>xI_n</m> commutes with all matrices. That is, <m>xI_n - [g]_B^B</m> and <m>xI_n - [g]_C^C</m> are similar matrices (with entries in <m>F[x]</m>), and it follows that <m>\det(xI_n - [g]_B^B) = \det(xI_n - [g]_C^C)</m>.
                </p>
      
                <p>
                  The following two exercises show that the characteristic polynomial captures both the determinant and the trace of an operator:
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-cp0">
                <title><m>\exe</m> – <m>\cp(0)</m></title>
      
                <p>
                  Show <m>\cp_g(0) = (-1)^n \det(g)</m>[^1].
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-charpoly-and-trace">
                <title><m>\exe</m> – CharPoly and Trace</title>
      
                <p>
                  Show that if <m>\dim_F(V) = n</m>[^1], then the coefficient of <m>x^{n-1}</m> in <m>\cp_g(x)</m>[^2] is <m>-\trace(g)</m>[^3].
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-triangular-matrix-and-charpoly">
                <title><m>\exe</m> – Triangular Matrix and CharPoly</title>
      
                <p>
                  If <m>A</m> is upper or lower triangular, then <m>\cp_A(x) = \prod_i (x - a_{i,i})</m>[^1]. As you may recall from an undergraduate class, in this case <m>a_{1,1}, \dots, a_{n,n}</m> are the eigenvalues of <m>A</m>. More on this later.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-34">
                <title>Problem</title>
      
                <p>
                  Let <m>V</m> be the <m>\R</m>-vector space consisting of polynomials in the variable <m>x</m> of degree at most <m>3</m> and let <m>g: V \to V</m> be the <m>\R</m>-linear operator given by <me>g(p(x)) = p''(x) + (x+1) p'(x) + 7 p(x),</me>where <m>p''(x)</m> and <m>p'(x)</m> denote the first and second derivatives of <m>p(x)</m>. (You may take it on faith that <m>g</m> is <m>\R</m>-linear.) Find the determinant, the trace, and the characteristic polynomial of <m>g</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-35">
                <title>Problem</title>
      
                <p>
                  Let <m>F</m> be a field and <m>V = F[x]/I</m> with <m>I = F[x] \cdot f(x)</m> for some monic polynomial <m>f(x) = x^n + a_{n-1} x^{n-1} + \cdots + a_1 x + a_0</m>. Regard <m>V</m> as an <m>F</m>-vector space (via restriction of scalars along <m>F \into F[x]</m>), and recall that the function <m>g: V \to V</m> given by <m>g(v) = x \cdot v</m> is an <m>F</m>-linear operator on <m>V</m>.
                </p>
      
                <p>
                  Prove that the characteristic polynomial of <m>g</m> is <m>f(x)</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-8-unfinished-1">
                <title>Problem 8 #unfinished</title>
      
                <p>
                  The trace of a square matrix <m>A</m>, denoted <m>Tr(A)</m>, is the sum of its diagonal<!-- linebreak -->entries. Prove the following assertions.
                </p>
      
                <p><ol>
                  <li>
                  <m>Tr(AB) = Tr(BA)</m> for any <m>n \times n</m> matrices <m>A, B</m>.
                  </li>
      
                  <li>
                  Use (a) to prove that the trace of a matrix <m>A</m> over <m>\C</m> is the sum<!-- linebreak -->of its eigenvalues (with multiplicities).
                  </li>
      
                </ol></p>
      
      
                <paragraphs xml:id="proof.-116">
                  <title><em>Proof.</em></title>
      
      
                  <paragraphs xml:id="part-a-32">
                    <title>Part (a)</title>
      
                    <p>
                      Let <m>A,B</m> be <m>n\times n</m> matrices.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-b-29">
                    <title>Part (b)</title>
      
                    <p>
                      Let <m>A</m> be a matrix in <m>\C</m>. Eigenvalues are the roots of <m>\cp_A(x)</m>, which is the determinant of the matrix <m>A-xI</m>. Every matrix <m>A</m> is similar to a matrix in Rational Canonical Form, and similar matrices share invariant factors and thus characteristic polynomials, the roots of which are the eigenvalues of the matrix.
                    </p>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
            </paragraphs>
          </subsection>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>
        
      </chapter>

      <chapter xml:id="ch-modgen">
        <title>Finitely Generated Modules over PIDs</title>

        <section xml:id="sec-modpres">
          <title>Finitely Presented Modules</title>

          <subsection xml:id="module-presentations">
            <title>Module Presentations</title>
      
      
            <paragraphs xml:id="rem-49">
              <title><m>\rem</m></title>
      
              <p>
                You have seen presentations for groups in the past; these consisted of a set of generators and a set of relations among these generators. Presentations are important for modules as well. In the case of modules, the relations may be encoded by a matrix.
              </p>
      
      
              <paragraphs xml:id="defn-presentation-r-module">
                <title><m>\defn</m> – Presentation (<m>R</m>-Module)</title>
      
                <p>
                  Let <m>R</m> be a non-zero commutative ring, let <m>A \in \Mat_{m,n}(R)</m>, and let <m>T_A: R^n \to R^m</m> be the <m>R</m>-module homomorphism represented by <m>A</m> with respect to the standard bases; that is, define <m>T_A(v)=A \cdot v</m>. The <m>R</m>-<em><m>\defnn{module presented by}</m></em> <m>A</m> is the <m>R</m>-module <m>\coker(T_A) = R^m/\im(T_A)</m>.
                </p>
      
                <p>
                  Equivalently, the module presented by <m>A</m> is <me>\frac{R^m}{R v_1 + \cdots + R v_n}</me> where <m>v_1, \dots, v_n</m> are the columns of <m>A</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-z-module-presentation">
                <title><m>\exe</m> – <m>\Z</m>-Module Presentation</title>
      
                <p>
                  What <m>\Z</m>-module <m>M</m> is presented by <me>
      A =
      \begin{bmatrix}
      2 &amp; 1 &amp; 0 \\
      3 &amp; 9 &amp; 5 \\
      1 &amp; -2 &amp; 7 \\
      0 &amp; 1 &amp; 2 \\
      \end{bmatrix}?</me>Formally, <m>M</m> is the quotient module <m>M=\Z^4/\im(T_A)</m>, where <m>T_A:\Z^3\to \Z^4</m> is defined by <m>T_A(v)=Av</m>. Since <m>\Z^4</m> is generated by its standard basis elements <m>\{e_1,e_2,e_3,e_4\}</m>, it follows that <m>M=\Z^4/\im(T_A)</m> is generated by the cosets of the <m>e_i</m>. To keep the notation short, we set <m>m_i=e_i+\im(T_A)</m>.
                </p>
      
                <p>
                  Let <m>N=\im(T_A)</m> and note that <m>N</m> is the submodule of <m>\Z^4</m> generated by the columns of <m>A</m>, i.e. <me>
      N=\Z \cdot \left\{\begin{bmatrix} 2 \\ 3 \\ 1\\
      0 \end{bmatrix}, \begin{bmatrix} 1 \\ 9 \\ -2
      \\1 \end{bmatrix},\begin{bmatrix} 0 \\ 5 \\
      7\\2 \end{bmatrix}\right\}=
      \Z \cdot \{2e_1 + 3e_2 +
      e_3,e_1 + 9e_2 -2 e_3 + e_4, 5e_2 + 7 e_3 + 2 e_{4  \}.}</me>Since <m>N</m> maps to <m>0</m> under the quotient map <m>q:\Z^4\to M=\Z^4/N</m>, we have that the relations of <m>M</m> can be written as <me>
      \begin{aligned}
      2m_1 + 3m_2 + m_3 &amp; = 0 \\
      m_1 + 9m_2 -2 m_3 + m_4 &amp; = 0 \\
      5m_2 + 7 m_3 + 2 m_4 &amp; = 0. 
      \end{aligned}
      </me> We can now see that this is a rather inefficient presentation, since we can clearly use the first equation to solve for for <m>m_3=-2m_1 - 3m_2</m>. This implies that <m>M</m> can be generated using only <m>m_1, m_2</m> and <m>m_4,</m> that is <me>
      M=\Z \cdot \{m_1,m_2,m_3,m_4\}= \Z \cdot \{m_1,m_2,m_4\}.
      </me> This eliminates the first equation, and by substituting <m>m_3 = -2m_1 -3m_2</m> the latter two relations become <me>
      \begin{aligned}
      5m_1 + 15m_2 + m_4 &amp; = 0 \\
      -14m_2  -16m_2 + 2m_4 &amp; = 0 \\
      \end{aligned}
      </me> Now we can also eliminate <m>m_4</m>, i.e leaving just two generators <m>m_1, m_2</m> that satisfy <me>
      -24m_1 -46m_2 = 0.
      </me> Let us notice that what we have really done is to perform certain transformations of the matrix <m>A</m>. In detail, we can use elementary row operations to “make zeros’’ on the 1st and 2nd columns as follows: <me>
      A =
      \begin{bmatrix}
      2 &amp; 1 &amp; 0 \\
      3 &amp; 9 &amp; 5 \\
      1 &amp; -2 &amp; 7 \\
      0 &amp; 1 &amp; 2 \\
      \end{bmatrix}
      \rightarrow
      \begin{bmatrix}
      0 &amp; 5 &amp; -14 \\
      0 &amp; 15 &amp; -16 \\
      1 &amp; -2 &amp; 7 \\
      0 &amp; 1 &amp; 2 \\
      \end{bmatrix}
      \rightarrow
      \begin{bmatrix}
      0 &amp; 0 &amp; -24 \\
      0 &amp; 0 &amp; -46 \\
      1 &amp; 0 &amp; 13\\
      0 &amp; 1 &amp; 0 \\
      \end{bmatrix}
      </me> Eliminating the generators <m>m_3</m> and <m>m_4</m> amounts to dropping the first two columns (which are the 3rd and 4th standard basis vectors) as well as the last two rows. As we will prove soon, this shows that the <m>\Z</m>-module presented by <m>A</m> is isomorphic to the <m>\Z</m>-module presented by <me>
      B=
      \begin{bmatrix} -24 \\ - 46
      \end{bmatrix}.
      </me> We can go further. Set <m>m_1' := m_1 + 2m_2</m>. Then <m>m_1'</m> and <m>m_2</m> also form a generating set of <m>M</m>. The relation on <m>m_1, m_2</m> translates to <me>
      -24m'_1  +2 m_2 = 0
      </me> given by the matrix <me>
      C = E_{1,2}(-2)B=
      \begin{bmatrix} -24 \\ 2
      \end{bmatrix}.
      </me> Note that we have done a row operation (subtract twice row 1 from row 2) to get from <m>B</m> to <m>C</m>. Continuing in this fashion by subtracting 12 row 2 from row 1 we also form <me>
      D = E_{1,2}(12)C=
      \begin{bmatrix} 0 \\ 2
      \end{bmatrix},
      </me> The last matrix <m>D</m> presents the module <m>M'=\Z^2/\im(t_D)</m> with generators <m>a, b</m> <m>(a=e_1+\im(t_D), b=e_2+\im(t_D)</m>) and relation <m>2a = 0</m>. As we will see, this proves <m>M \cong \Z \oplus \Z/2</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="thm-matrices-modules-and-isomorphisms">
                <title><m>\thm</m> – Matrices, Modules, and Isomorphisms</title>
      
                <p>
                  Let <m>R</m> be a non-zero commutative ring and let <m>A \in \Mat_{m \times n}(R)</m> and <m>B \in Mat_{m' \times n'}(R)</m> for some <m>m,n,m',n' \geq 1</m>. Then <m>A</m> and <m>B</m> present isomorphic <m>R</m>-modules if <m>B</m> can be obtained from <m>A</m> by any finite sequence of operations of the following form: 1. an elementary column operation, 2. an elementary column operation, 3. deletion of the <m>j</m>-th column and <m>i</m>-th row of a matrix whose <m>j</m>-th column is the vector <m>e_i</m>, 4. the reverse of (3), 5. deletion of a column of all <m>0</m>’s, 6. the reverse of (5).
                </p>
      
      
                <paragraphs xml:id="proof.-117">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Note: This proof was not covered in class. Assume <m>B</m> is obtained from <m>A</m> by a single one of the steps listed above. We need to prove that there is an isomorphism <m>R^m/\im(T_A) \cong R^{m'}/\im(T_B)</m> of <m>R</m>-modules. 1. In this case, <m>B = E A</m> for some elementary matrix <m>E</m>. More generally, let <m>E</m> be any invertible matrix such that <m>B = EA</m>. Then <m>T_E: R^m \to R^m</m> is an isomorphism and it maps <m>\im(T_A)</m> bijectively onto <m>\im(T_B)</m>. It follows that the kernel of the composition <m>R^m \xra{T_E} R^m \onto R^m/\im(T_B)</m> is <m>\im(T_A)</m> and hence by the first isomorphism theorem it induces an isomorphism <me>R^m/\im(T_A) \xra{\cong} R^m/\im(T_B).</me> 2. In this case, <m>B=AE</m> for some elementary matrix <m>E</m>. More generally, assume <m>E</m> is any invertible matrix such that <m>B = AE</m>. Since <m>T_E</m> is an isomorphism, we have <me>\im(T_B) = \im(T_{AE})=\im(T_A\circ T_E)=\im(T_A)</me>and so <m>R^m/\im(T_A) = R^{m}/\im(T_B)</m>. (For this one we get equality, not merely an isomorphism.) 3. For notational simplicity, let us assume <m>i = j =1</m>; that is, the first column of <m>A</m> is <m>e_1</m> and <m>B</m> is obtained by deleting the first row and column of <m>A</m>, giving a <m>(m-1) \times (n-1)</m> matrix. So <me>A = \begin{bmatrix} 1 &amp; r \\ 0 &amp; B \\\end{bmatrix}</me>where <m>r</m> denotes some row vector and <m>0</m> denotes a column of all <m>0</m>’s. Let <m>p: R^{m} \onto R^{m-1}</m> and <m>q: R^{n} \onto R^{n-1}</m> be projection onto the last <m>m-1</m> and <m>n-1</m> components, respectively. Because of the nature of <m>A</m> and <m>B</m>, the diagram (page 40 in notes) commutes. Moreover, the kernel of <m>q</m> is <m>Re_1</m> and the kernel of <m>p</m> is <m>Re_1</m>, and since the first column of <m>A</m> is <m>e_1</m>, <m>T_A</m> maps the kernel of <m>q</m> bijectively onto the kernel of <m>p</m>. A “diagram chase’’ shows that <m>\coker(T_A) \cong \coker(T_B)</m>. In detail: Since the diagram commutes, <m>p(\im(T_A)) \subseteq \im(T_B)</m> and hence <m>p</m> induces an <m>R</m>-module homomorphism <me>\ov{p}: R^m/\im(T_A)   \to R^{m-1}/\im(T_B).</me>(by the <m>0</m>-th isomorphism theorem). Since <m>p</m> is onto, so is <m>\ov{p}</m>. Suppose <m>v + \im(T_A) \in \ker(\ov{p})</m>. So, <m>p(v) \in \im(T_B)</m>. Say <m>p(v) = B w</m>. Since <m>q</m> is onto, <m>w = q(u)</m> for some <m>u</m>. Then <me>
      p(v - Au) = p(v) - pA(u) = p(v) - Bq(u) = p(v) - Bw = p(v) - p(v) = 0, </me> and thus <m>v - Au \in \ker(p)</m>. As noted above, <m>T_A</m> maps <m>\ker(q)</m> onto <m>\ker(p)</m> and hence <me>v - Au = Ay</me>for some vector <m>y</m>. This proves <m>v = A(u+y) \in \im(T_A)</m> and hence that <m>v +\im(T_A) = 0</m> in <m>R^m/\im(T_A)</m>. This proves <m>\ov{p}</m> is one-to-one. 4. It is clear that the columns of <m>B</m> generate the same submodule of <m>R^m</m> as do the columns of <m>A</m>, and thus <m>\im(T_A) = \im(T_B)</m> and <m>R^m/\im(T_A) =R^m/\im(T_B)</m>. 5. Since the isomorphism relation is reflexive, the statements of parts 3. &amp; 5. show that parts 4.&amp; 6. are true as well.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-50">
                <title><m>\rem</m></title>
      
                <p>
                  The converse is true for some rings <m>R</m>, including Euclidean domains.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-51">
                <title><m>\rem</m></title>
      
                <p>
                  In fact, if <m>A</m> and <m>B</m> are equivalent matrices, then <m>\coker(T_A) \cong \coker(T_B)</m>, as I shall prove below. This implies both (1) and (2) from the Theorem.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-diagonal-presentation">
                <title><m>\lem</m> – Diagonal Presentation</title>
      
                <p>
                  Suppose <m>R</m> is a commutative ring and <m>A = (a_{i,j})</m> is a <m>m \times n</m> matrix such that <m>a_{i,j} = 0</m> for all <m>i \ne j</m> and set <m>d_i = a_{i,i}</m> for all <m>1 \leq i \leq \min\{m,n\}</m>. If <m>n \geq m</m> then <me>\coker(T_A) \cong R/R \cdot d_1 \oplus R/R\cdot d_2 \oplus \cdots \oplus R/R \cdot d_m </me> and if <m>n \leq m</m> then<me>\coker(T_A) \cong R/R \cdot d_1 \oplus R/R\cdot d_2 \oplus \cdots \oplus R/R \cdot d_n \oplus R^{m-n}.</me>
                </p>
      
      
                <paragraphs xml:id="proof.-118">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Assume <m>n \leq m</m> and define <m>g: R^m \to R/R \cdot d_1 \oplus R/R \cdot d_2 \oplus \cdots \oplus R/ R \cdot d_n \oplus R^{m-n}</m> to be the map sending <m>(r_1, \dots, r_m)^\tau</m> to <m>(\ov{r}_1, \dots, \ov{r}_n, r_{n+1}, \dots, r_m)</m> where <m>\ov{r}_i = r_i + R \cdot d_i</m> for <m>1 \leq i \leq n</m>. (I.e., <m>g</m> is the unique <m>R</m>-map sending the <m>i</m>-th standard basis vector <m>e_i</m> to <m>(0, \dots, 0, \ov{1}, 0, \dots, 0)</m> with <m>\ov{1}</m> in the <m>i</m>-th position, for <m>1 \leq i \leq n</m>, and to <m>e_i</m> itself for <m>i &gt; n</m>.) Then <m>g</m> is clearly onto and the kernel <m>\ker(g)</m> of <m>g</m> is the set of those tuples <m>(r_1, \dots, r_m)^\tau</m> such that <m>r_i = x_i d_i</m> for some <m>x_i</m> for all <m>1 \leq i \leq n</m> and <m>r_j = 0</m> for <m>j &gt; n</m>.<!-- linebreak -->Given such a tuple, <me> (r_1, \dots, r_m)^\tau = x_1 (d_1, 0, \dots, 0)^\tau + x_2 (0,d_2, 0, \dots, 0)^\tau + \cdots + x_n (0, \dots, 0, d_n, 0, \dots, 0)^\tau. </me> This proves <m>\ker(g)</m> is contained <me>
      R \cdots (d_1, 0, \dots, 0)^\tau + R \cdot (0,d_2, 0, \dots, 0)^\tau 
        + \cdots + R \cdot (0, \dots, 0, d_n, 0 \dots, 0)^\tau = \im(T_A).
        </me>Arguing backwards we see that the opposite containment also holds, so that in fact <m>\ker(g) = \im(T_A)</m>.
                  </p>
      
                  <p>
                    By the First Isomorphism Theorem, <me> \coker(T_A) = R/\im(T_A) = R/\ker(g) \cong R/R \cdot d_1 \oplus \cdots \oplus R/R \cdot d_n \oplus R^{m-n}.</me>If <m>n \leq m</m> then, by deleting columns of all <m>0</m>’s, we may reduce to the case when <m>n = m</m>, which is included in the first case.
                  </p>
      
                </paragraphs>
              </paragraphs>
            </paragraphs>
          </subsection>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

        <section xml:id="sec-snf">
          <title>Smith Normal Form</title>

          <subsection xml:id="smith-normal-form">
            <title>Smith Normal Form</title>
      
      
            <paragraphs xml:id="thm-smith-normal-form">
              <title><m>\thm</m> – Smith Normal Form</title>
      
              <p>
                Let <m>R</m> be a Euclidean domain and let <m>A \in \Mat_{m \times n}(R)</m>. Then there is a sequence of elementary column and column operations that transform <m>A</m> into a matrix <m>S = [s_{i,j}]</m> such that all non-diagonal entries of <m>S</m> are <m>0</m> and the diagonal entries of <m>S</m> satisfy <me>
      s_{1,1} \mid s_{2,2} \mid s_{3,3} \mid \cdots.</me> ###### <em>Proof.</em> The main point of the proof is to establish:
              </p>
      
              <p>
                {}: There is a sequence of row and column operations that transforms <m>A</m> to <me>
      A' = \begin{bmatrix}
      g &amp; 0 \\
      0 &amp; B \\
      \end{bmatrix}
      </me> for some <m>(n-1) \times (m-1)</m> matrix <m>B</m> and where <m>g = \gcd(A)</m>. (We adopt the convention that if <m>A</m> is the matrix of all <m>0</m>’s, then <m>g = 0</m>.) Note that, by Lemma , we have <m>\gcd(A) = \gcd(A')</m> and thus <m>g \mid \gcd(B)</m>.
              </p>
      
              <p>
                Granting this claim, by applying it again to <m>B</m> we arrive at a matrix of the form <me>
      A'' = \begin{bmatrix}
      g &amp; 0 &amp; 0\\
      0 &amp; g' &amp; 0 \\
      0 &amp; 0 &amp; C
      \end{bmatrix}
      </me> where <m>g' = \gcd(B)</m> and <m>g \mid g'</m> and <m>g' \mid \gcd(C)</m>. (Observe that the row and column operations on <m>B</m> will not affect the first row and column of <m>A'</m>.) Continuing in this fashion, we arrive at the matrix <m>S</m> in the statement.
              </p>
      
              <p>
                To prove the claim, let <m>a</m> be the upper-left entry of <m>A</m>.
              </p>
      
              <p>
                Suppose <m>a</m> happens to be <m>g = \gcd(A)</m>. Then, in particular, it divides every entry of the first row and column of <m>A</m>, and so by doing row and column operations of type I, we may <m>0</m> out these entries to arrive at a matrix of the desired form directly.
              </p>
      
              <p>
                In general, let <m>l</m> to be the number of prime factors in a prime factorization of of <m>a/\gcd(A)</m>, and proceed by induction on <m>l</m>.
              </p>
      
              <p>
                If <m>l = 0</m>, then <m>a = \gcd(A)</m> (up to associates), and we already did this case.
              </p>
      
              <p>
                Assume <m>l &gt; 0</m>. Then there is at least one entry <m>b = a_{i,j}</m> such that <m>a \nmid b</m>.
              </p>
      
              <p>
                Case I: There is such a <m>b</m> belonging to the first row of <m>A</m>. In this case we we may implement the Euclidean algorithm in the form of suitable column operations to replace <m>a</m> by <m>\gcd(a,b)</m> and <m>b</m> by <m>0</m>, as in the example above. Since <m>a \nmid b</m>, <m>\gcd(a,b)</m> is a proper divisor of <m>a</m>, and it follows that the number of factors in a prime factorization of <m>\gcd(a,b)/g</m> is smaller than <m>l</m>, and we are done by induction.
              </p>
      
              <p>
                Case II: There is such a <m>b</m> in the first column. Just as in the previous case, we are done by induction upon implementing the Euclidean algorithm using suitable row operations.
              </p>
      
              <p>
                Case III: <m>a</m> divides every entry of the first row and first column. In this case, suitable row and column operations transform <m>A</m> to <me>
      D = 
      \begin{bmatrix}
      a &amp; 0 \\
      0 &amp; E \\
      \end{bmatrix}.
      </me> By Lemma  we have <m>\gcd(D) = \gcd(A)</m>, and thus there is some element <m>e</m> of <m>E</m> such that <m>a \nmid e</m> (since we are assuming <m>a</m> is not the gcd of <m>A</m>). A suitable row operation puts <m>e</m> into row one without affecting <m>a</m>, and we are back to the previously solved Case I.
              </p>
      
      
              <paragraphs xml:id="rem-52">
                <title><m>\rem</m></title>
      
                <p>
                  Note that some of the diagonal entries could be <m>0</m>. Recall <m>s \mid 0</m> for all <m>s \in R</m> (including <m>s = 0</m>), and <m>0</m> is the only element that divides <m>0</m>. So, the “tail’’ end of the sequence could be all <m>0</m>’s, and indeed if <m>s_{i,i} = 0</m> for some <m>i</m> then all subsequent diagonal entries must be <m>0</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="cor-ftfpmed-first-version">
                <title><m>\cor</m> – FTFPMED: First Version</title>
      
                <p>
                  If <m>R</m> is a Euclidean domain and <m>M</m> is a finitely presented <m>R</m>-module, then <m>M</m> is isomorphic to a direct sum of cyclic modules: <me>M \cong R/d_1 \oplus \cdots \oplus R/d_l \oplus R^a</me>for some <m>a \geq 0</m> and some non-zero, non-invertible elements <m>d_1, \dots, d_l \in R</m> such that <m>d_1 \mid d_2 \mid \cdots \mid d_l</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-119">
                  <title><em>Proof.</em></title>
      
                  <p>
                    This is an immediate consequence of the SNF Theorem, Theorem  parts (1) and (2), and Lemma .
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="exe-special-case-of-snf">
                <title><m>\exe</m> – Special Case of SNF</title>
      
                <p>
                  Suppose <m>R</m> is a Euclidean domain and <m>A = (x_1, \dots, x_n)</m> is a <m>1 \times n</m> matrix. Column operations of type I in this case amount to adding a multiple of one element in this list to another one. The SNF Theorem in this case amounts to the Euclidean algorithm: By adding a suitable multiple of the one entry in the first two positions to the other, in the usual back-and-forth way, we arrive at <m>(g_1, 0, x_3, \dots, x_n)</m> where <m>g_1 = \gcd(x_1, x_2)</m>. Repeat this for columns <m>1</m> and <m>3</m> to arrive at <m>(g_2, 0, 0, x_4, \dots, x_n)</m> where <m>g_2 = \gcd(g_1, x_3) = \gcd(x_1, x_2, x_3)</m>. Continuing in this fashion we arrive at <m>(g, 0, \dots, 0)</m> where <m>g = \gcd(x_1, \dots, x_n)</m>.
                </p>
      
                <p>
                  The proof of the SNF Theorem in general amounts to an extended version of the idea used in this special case.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-finding-snf">
                <title><m>\exe</m> – Finding SNF</title>
      
                <p>
                  Consider the matrix with entries in <m>\Z</m> <me>
       A=\begin{bmatrix}
       1 &amp; 6 &amp; 5 &amp; 2 \\
       2 &amp; 1 &amp; -1 &amp; 0 \\
       3 &amp; 0 &amp; 3 &amp; 0
       \end{bmatrix}.
       </me> Do row and column operations to put <m>A</m> into its Smith Normal Form: <me>S=\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 6 &amp; 0 \\\end{bmatrix}.</me>Conclude that the module presented by <m>A</m> is isomorphic to <m>\Z/6</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-53">
                <title><m>\rem</m></title>
      
                <p>
                  A version of the SNF Theorem and its Corollary are valid for PIDs: If <m>R</m> is a PID and <m>A \in \Mat_{m \times n}(R)</m>, there there exist invertible matrices <m>P</m> and <m>Q</m> such that <m>PAQ</m> has the form <m>S</m> as in the theorem. It follows that every finitely presented module over a PID is direct sum of cyclic ones.
                </p>
      
                <p>
                  For Euclidean domains, <m>P</m> and <m>Q</m> may be taken to be products of elementary matrices, and the Smith normal form can be found algorithmically.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-gcd-of-a-matrix-in-pid">
                <title><m>\lem</m> – GCD of a Matrix in PID</title>
      
                <p>
                  Suppose <m>R</m> is a PID and <m>A</m> is a matrix with entries in <m>R</m> and <m>B</m> is a matrix obtained from <m>A</m> via elementary column and column operations. Then <m>\gcd(A) = \gcd(B)</m> where the gcd of a matrix is defined to be the gcd of all of its entries.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-minor">
                <title><m>\defn</m> – Minor</title>
      
                <p>
                  A <m>k \times k</m> <em><m>\defnn{minor}</m></em> of <m>A</m> is the determinant of a <m>k \times k</m> submatrix of <m>A</m>; more formally, if <m>A</m> is an <m>m \times n</m> matrix, a <m>k\times k</m> minor of <m>A</m> is any element of <m>R</m> given as follows: Choose <m>1 \leq i_1 &lt; i_2 &lt; \cdots &lt; i_k \leq m</m> and <m>1 \leq j_1 &lt; \cdots &lt; j_k \leq n</m>, let <m>B = (b_{p,q})</m> be the <m>k \times k</m> matrix with <m>b_{p,q} = a_{i_p, j_q}</m>. Then <m>\det(B)</m>[^1] is a minor of <m>A</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-ideals-generated-by-minors">
                <title><m>\lem</m> – Ideals Generated by Minors</title>
      
                <p>
                  For any commutative ring <m>R</m> and matrix <m>A \in \Mat_{m \times n}(R)</m>, if <m>B</m> is obtained from <m>A</m> via an elementary column or column operation, then the ideal of <m>R</m> generated by all the <m>k \times k</m> minors of <m>B</m> equals the ideal of <m>R</m> generated by all the <m>k \times k</m> minors of <m>A</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="thm-smith-normal-form-is-unique">
                <title><m>\thm</m> – Smith Normal Form is Unique</title>
      
                <p>
                  Assume <m>R</m> is a PID and <m>A \in \Mat_{m \times n}(R)</m>. Suppose <m>S \in \Mat_{m \times n}(R)</m> can be obtained from <m>A</m> via a sequence of elementary column and column operations and that <m>S</m> is diagonal with diagonal entries <m>d_1, d_2, \dots</m> such that <m>d_1 \mid d_2 \mid \cdots</m>. Then <me>d_1 = \gcd_1(A), d_2 = \frac{\gcd_2(A)}{\gcd_1(A)}, d_3 = \frac{\gcd_3(A)}{\gcd_2(A)}, \cdots.</me>In particular, the SNF of a matrix is unique up to associates.
                </p>
      
      
                <paragraphs xml:id="proof.-120">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Recall that for a PID <m>R</m>, the gcd of any set of elements is defined to be a generatpr of the ideal they generate. So, Lemma  implies that <m>\gcd_k(A) = \gcd_k(S)</m> for all <m>k</m>. Since <m>S</m> is diagonal, the only non-zero minors of <m>S</m> are those given by indices <m>1 \leq i_1 &lt; \cdots &lt; i_k \leq m</m> and<m>1 \leq j_1 &lt; \cdots &lt; j_k \leq m</m> for which <m>i_t = j_t</m> for all <m>1 \leq t \leq k</m>, and moreover such a minor is equal to <m>d_{i_1} \cdots d_{i_k}</m>. Since <m>d_l \mid d_{l+1}</m> for all <m>l</m>, it follows that <m>d_{1} \cdots d_k</m> divides <m>d_{i_1} \cdots d_{i_k}</m> for all <m>1 \leq i_1 &lt; \cdots &lt; i_k \leq n</m>. Thus <m>\gcd_k(S) = d_1 \cdots d_k</m>, for each <m>k</m>, and hence <m>d_k = \gcd_k(S)/\gcd_{k-1}(S)</m> as claimed.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-54">
                <title><m>\rem</m></title>
      
                <p>
                  So, another way of finding the SNF of a matrix <m>A</m> with entries in a Euclidean domain is to calculate <m>\gcd_k(A)</m> for all <m>k</m>. This is not practical except in very special cases.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-36">
                <title>Problem</title>
      
                <p>
                  Let <m>A = \begin{bmatrix} 3 &amp; 1 &amp; -4 \\ 2 &amp; -3 &amp; 1 \\ -4 &amp; 6 &amp; -2 \end{bmatrix} \in \Mat_{3 \times 3}(\Z)</m>. Express the <m>\Z</m>-module presented by <m>A</m> as the direct sum of cyclic groups. Justify your answer.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-37">
                <title>Problem</title>
      
                <p>
                  Let <m>A = \begin{bmatrix} 2 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 6 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 20 &amp; 0 \end{bmatrix}</m> and <m>B = \begin{bmatrix} 2 &amp; 0 &amp; 0 \\ 0 &amp; 6 &amp; 0 \\ 0 &amp; 0 &amp; 20 \\ 0 &amp; 0 &amp; 0 \end{bmatrix}</m>, regarded as matrices with entries in <m>\Z</m>. 1. Express the cokernel of <m>A</m> (i.e., <m>\Z^3/\im(t_A)</m>) in elementary divisor form (that is, as a direct sum of cyclic groups each of which is either infinite or of prime power order). 2. Express the cokernel of <m>B</m> in elementary divisor form.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-38">
                <title>Problem –</title>
      
                <p>
                  Consider the matrix <me>A=\begin{bmatrix}
      1 &amp; 6 &amp; 5 &amp; 2 \\
      2 &amp; 1 &amp; -1 &amp; 0 \\
      3 &amp; 0 &amp; 3 &amp; 0
      \end{bmatrix}
      \in \M_{3,4}(\Z).</me> Determine the simplest representative in the isomorphism class of the <m>\Z</m>-module presented by <m>A</m>.
                </p>
      
              </paragraphs>
            </paragraphs>
          </subsection>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

        <section xml:id="sec-noeth">
          <title>noeth</title>
          
          <subsection xml:id="commutative-noetherian-rings">
            <title>Commutative Noetherian Rings</title>
      
      
            <paragraphs xml:id="rem-55">
              <title><m>\rem</m></title>
      
              <p>
                We now address the question of which modules have finite presentations. It is clear than any such module must be finitely generated (since the cosets of <m>e_1, \dots, e_m</m> generate <m>R^m/\im(T_A)</m> for any <m>m \times n</m> matrix <m>A</m>). If <m>M</m> is finitely generated, say by <m>m</m> elements, then we can find a surjective <m>R</m>-module homomorphism <me>
      \pi: R^m \onto M.
      </me> Provided the kernel of <m>\pi</m> is also finitely generated, say by <m>n</m> elements, then we may find a surjection <me>
      g: R^n \onto \ker(\pi).
      </me> The composition <m>R^n \xra{g} \ker(\pi) \subseteq R^m</m> is a map between free <m>R</m>-modules and is thus equal to <m>T_A</m> for some <m>n \times m</m> matrix <m>A</m>. Clearly <m>\im(T_A) = \ker(\pi)</m> and hence by the first isomorphism theorem <me>
      R^m/\im(A) = R^m/\ker(\pi) \cong M,
      </me> so that <m>M</m> is finitely presented.
              </p>
      
              <p>
                So the real question is: For a given ring <m>R</m>, is it the case that for all <m>m</m>, every submodule of <m>R^m</m> is finitely generated? The answer is “no’’ in general, but it does hold for many rings of interest:
              </p>
      
      
              <paragraphs xml:id="defn-ascending-chain-condition">
                <title><m>\defn</m> – Ascending Chain Condition</title>
      
                <p>
                  A commutative ring <m>R</m> has the <em><m>\defnn{ascending chain condition}</m></em> (on ideals) if given any chain of ideals in <m>R</m> of the form <me>I_1 \subseteq I_2 \subseteq \cdots</me>there is an <m>N</m> such that <me>I_N = I_{N+1} = \cdots.</me>
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="lem-noetherian-rings">
                <title><m>\lem</m> – Noetherian Rings</title>
      
                <p>
                  Suppose <m>R</m> is a commutative ring. The following conditions are equivalent: 1. <m>R</m> has the ascending chain condition on ideals. 2. Every ideal of <m>R</m> is finitely generated — i.e., for every ideal <m>I</m>, there exists a finite set of elements <m>x_1, \dots, x_n</m> in <m>I</m> such that <m>I = R \cdot x_1 + \cdots + R \cdot x_n</m>. In this case we say <m>R</m> is <em>Noetherian</em>.
                </p>
      
      
                <paragraphs xml:id="proof.-121">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Assume every ideal is finitely generated and that such a chain is given. Let <m>J = \cup_n I_n</m>. Then it is easy to see that <m>J</m> is an ideal. (In detail, for <m>a \in J, r \in R</m> we have <m>a \in I_n</m> for some <m>n</m> and hence <m>ra \in I_n \subseteq J</m>.
                  </p>
      
                  <p>
                    If <m>a,b \in J</m>, then <m>a \in I_n</m> and <m>b \in I_m</m> for some <m>n,m</m> and hence there is a <m>N</m> such that <m>a,b \in I_N</m>. It follows that <m>a \pm b \in I_N \subseteq J</m>. Finally <m>0 \in J</m>.) Thus by assumption <m>J</m> is finitely generated, say <m>J = Rx_1 + \cdots + Rx_m</m> for some <m>x_1, \dots, x_m \in J</m>.
                  </p>
      
                  <p>
                    Each <m>x_i</m> belongs to one of the <m>I_n</m>’s and hence, since there are only a finite number of such elements and ideas are nested, there is an <m>N</m> such that <m>x_1, \dots, x_m \in I_N</m>. It follows that <m>I_N = J</m> and hence <m>I_N = I_{N+1} = I_{N+2} = \cdots</m>.
                  </p>
      
                  <p>
                    Assume <m>R</m> has the acc for ideals and let <m>J</m> be any ideal. Pick any element <m>a_1\in I</m> and set <m>I_1 = Ra_1</m>. If <m>J = I_1</m> we are done. If not, pick <m>a_2 \in I \setminus I_1</m> and set <m>I_2 = Ra_1 + Ra_2</m>. If <m>J = I_2</m> we are done and if not pick <m>a_3 \in I\setminus I_2</m> and let <m>I_3 = Ra_1 + Ra_2 + Ra_3</m>. In this way we form a strictly ascending chain <m>I_1 \subset I_2 \subset I_3 \subset</m>, and this process cannot be continued forever since <m>R</m> has the acc. When it terminates, we have <m>J = Ra_1 + Ra_2 + \cdots + Ra_N</m> for some <m>a_1, \dots, a_N</m> and thus <m>J</m> is finitely generated.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="thm-hilberts-basis-theorem">
                <title><m>\thm</m> – Hilbert’s Basis Theorem</title>
      
                <p>
                  If <m>R</m> is a noetherian ring, then the polynomial rings <m>R[x_1,\dots,x_d]</m> and <m>R[[ x_1,\dots,x_d]]</m> are noetherian for any <m>d \geqslant 1</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-122">
                  <title><em>Proof.</em></title>
      
                  <p>
                    We will give the proof for polynomial rings, and at the end we will indicate what the difference is in the argument for the power series ring case. First, note that by induction on <m>d</m>, we can reduce to the case <m>d=1</m>.
                  </p>
      
                  <p>
                    Given an ideal <m>I\subseteq R[x]</m>, consider the set of leading coefficients of all polynomials in <m>I</m>, <me>J :=  \{ a \in R \ \mid  \ \textrm{there is some } a x^n + \text{lower order terms (with respect to }x) \in I\}.</me> By the Hilbert Basis Lemma, we see that this is an ideal of <m>R</m>. Since <m>R</m> is noetherian, <m>J</m> is finitely generated[^2], so let <m>J = (a_1,\dots,a_t)</m>. Pick <m>f_1,\dots,f_t\in R[x]</m> such that the leading coefficient of <m>f_i</m> is <m>a_i</m>, and set <m>N=\displaystyle\max_i \{\deg{f_i} \}</m>.
                  </p>
      
                  <p>
                    Let <m>f \in I</m>. The leading coefficient of <m>f</m> is an <m>R</m>-linear combination of <m>a_1, \ldots, a_t</m>. If <m>f</m> has degree greater than <m>N</m>, then we can cancel off the leading term of <m>f</m> by subtracting a suitable combination of the <m>f_i</m>. Therefore, any <m>f \in I</m> can be written as <m>f = g+ h</m> for some <m>h \in (f_1, \ldots, f_t)</m> and <m>g \in I</m> with degree at most <m>N</m>. In particular, note that <me>g \in I \cap \frac{R + Rx + \cdots + R x^N}.</me>Since <m>I \cap (R + Rx + \cdots + R x^N)</m> is a submodule of the finitely generated free <m>R</m>-module <m>R + Rx + \cdots + R x^N</m>, it must also be finitely generated as an <m>R</m>-module[^3]. Given such a generating set, say <m>I \cap (R + Rx + \cdots + Rx^N) = (f_{t+1}, \ldots, f_s)</m>, we can write any element <m>f \in I</m> as an <m>R[x]</m>-linear combination of these generators <m>f_{t+1}, \ldots, f_s</m> and the original <m>f_{1}, \ldots, f_t</m>. Therefore, <m>I = (f_1, \ldots, f_t, f_{t+1}, \ldots, f_s)</m> is finitely generated as an ideal in <m>R[x]</m>, and <m>R[x]</m> is a noetherian ring.
                  </p>
      
                  <p>
                    In the power series case, take <m>J</m> to be the set of coefficients of lowest degree terms.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="prop-submodules-of-f.g.-modules-are-f.g.-when-noetherian">
                <title><m>\prop</m> – Submodules of f.g. Modules are f.g. when Noetherian</title>
      
                <p>
                  If <m>R</m> is a noetherian commutative ring, then every submodule of a finitely generated module is again finitely generated.
                </p>
      
      
                <paragraphs xml:id="proof.-123">
                  <title><em>Proof.</em></title>
      
                  <p>
                    I will just prove the following special case (since it is all we need): For each <m>n \geq 1</m>, every submodule of <m>R^n</m> is finitely generated. The base case <m>n=1</m> holds by definition (and Lemma ), since a submodule of <m>R^1</m> is the same thing as an ideal.
                  </p>
      
                  <p>
                    Assume <m>n &gt; 1</m> and the result holds for <m>R^{n-1}</m>. Let <m>M</m> be any submodule of <m>R^n</m>. Define <me>
      \pi: R^n \onto R^1
      </me> to be the projection onto the last component of <m>R^n</m>. The kernel of <m>\pi_n</m> may be identified with <m>R^{n-1}</m> and so <m>N := \ker(\pi) \cap M</m> is a submodule of <m>R^{n-1}</m>, and it is therefore finitely generated by assumption. The image <m>\pi(M)</m> of <m>M</m> under <m>\pi</m> is a submodule of <m>R^1</m>, that is, an ideal of <m>R</m>, and so it too is finitely generated by assumption (and Lemma ).
                  </p>
      
                  <p>
                    Furthermore, by the first isomorphism theorem <m>M/\ker(\pi)\cong \pi(M)</m> is also finitely generated. By a homework problem, we deduce that <m>M</m> is a finitely generated module.
                  </p>
      
                  <p>
                    % I’ll just sketch the general case (which I don’t think we’ll actually need): let <m>T</m> be any finitely generated <m>R</m>-module and <m>N \subseteq T</m> any submodule. % Since <m>T</m> is finitely generated, there exists a surjective <m>R</m>-module homomorphism <m>q: R^n \onto T</m> for some <m>n</m>. Then <m>q^{-1}(N)</m> is a submodule of <m>R^n</m> and % hence it is finitely generated by the case we already proved. Moreover, <m>q</m> induces a surjective <m>R</m>-module homomorphism <m>q: q^{-1}(N) \onto N</m>, % and hence <m>N</m> is isomorphic to a quotient of a finitely generated <m>R</m>-module and thus it is also finitely generated.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-56">
                <title><m>\rem</m></title>
      
                <p>
                  The converse is also true: If <m>R</m> is not Noetherian, there there exists an ideal <m>I</m> that is not finitely generated (by the Lemma). This gives an example of a non-finitely-generated submodule, namely <m>I</m>, of a finitely generated module, namely <m>R</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="cor-f.g.-modules-finitely-presented-when-noetherian">
                <title><m>\cor</m> – F.G. Modules Finitely Presented when Noetherian</title>
      
                <p>
                  Any finitely generated module <m>M</m> over a noetherian ring <m>R</m> has a finite presentation; that is, given such a module over such a ring, there exists an <m>m \times n</m> matrix <m>A</m> in <m>R</m> and an isomorphism <me>M\cong R^m/\im(T_A).</me>
                </p>
      
      
                <paragraphs xml:id="proof.-124">
                  <title><em>Proof.</em></title>
      
                  <p>
                    We basically already proved this, but let me recap it:
                  </p>
      
                  <p>
                    If <m>M</m> is finitely generated, then for some <m>m</m> we can find a surjective <m>R</m>-module homomorphism <me>
      \pi: R^m \onto M.
      </me> Since we assume <m>R</m> is Noetherian, the kernel of <m>\pi</m> is also finitely generated by the Proposition, and so we may find a surjection of <m>R</m>-modules <me>
      g: R^n \onto \ker(\pi)
      </me> for some <m>n</m>. The composition <m>R^n \xra{g} \ker(\pi) \subseteq R^m</m> is equal to <m>T_A</m> for some <m>n \times m</m> matrix <m>A</m>. Since <m>\im(T_A) = \ker(\pi)</m>, the first isomorphism theorem gives an isomorphism <me>
      R^m/\im(T_A) = R^m/\ker(\pi) \cong M.
      </me>
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-39">
                <title>Problem –</title>
      
                <p>
                  Let <m>R</m> be a commutative ring and <m>I</m> an ideal of <m>R</m>. Show that if <m>R</m> is noetherian then <m>R/I</m> is also noetherian.
                </p>
      
              </paragraphs>
            </paragraphs>
          </subsection>


        </section>

        <section xml:id="sec-modclass">
          <title>Classifications</title>

          <subsection xml:id="classification-of-finitely-generated-modules-over-pids">
            <title>Classification of Finitely Generated Modules over PIDs</title>
      
      
            <paragraphs xml:id="thm-ftfgmopidiff">
              <title><m>\thm</m> – FTFGMOPIDIFF</title>
      
              <p>
                Fundamental Theorem of finitely generated modules over a PID, invariant factors form Let <m>R</m> be a PID and let <m>M</m> be a finitely generated module. Then there exist integers <m>r \geq 0, k \geq 0</m>, and non-zero, non-unit elements <m>d_1,\ldots, d_k</m> of <m>R</m> satisfying <m>d_1 \mid d_2 \mid \dots \mid d_k</m>[^1] such that <me>M \cong R^r \oplus R/(d_1) \oplus \dots \oplus R/(d_k).</me> Moreover <m>r</m> and <m>k</m> are uniquely determined by <m>M</m>, and the <m>d_i</m>’s are unique up to associates.
              </p>
      
      
              <paragraphs xml:id="defn-invariant-factors">
                <title><m>\defn</m> – Invariant Factors</title>
      
                <p>
                  The polynomials <m>f_1, \dots, f_s</m> occurring in the Theorem are called the {} of the operator <m>g</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="cor-ftfgmopriff">
                <title><m>\cor</m> – FTFGMOPRIFF</title>
      
                <p>
                  Let <m>k</m> be a field and <m>k[x]</m> be the ring of polynomials with coefficients in <m>k</m> in one variable <m>x</m>. If <m>M</m> a finitely generated <m>k[x]</m>-module then <me>M \cong k[x]^r \oplus k[x]/(f_1(x)) \oplus \cdots \oplus k[x]/(f_s(x))</me>for some <m>r \geq 0</m>, <m>s \geq 0</m> and non-constant monic polynomials <m>f_1, \dots, f_s</m> such that <m>f_1 \mid f_2 \mid \cdots \mid f_s.</m> Moreover, <m>r</m>, <m>s</m> and <m>f_1, \dots, f_s</m> are unique.
                </p>
      
      
                <paragraphs xml:id="proof.-125">
                  <title><em>Proof.</em></title>
      
                  <p>
                    This follows from the Theorem since <m>k[x]</m> is a Euclidean domain and every non-zero polynomial is associate to a unique monic one.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="exe-kx-modules-of-dimension-4">
                <title><m>\exe</m> – <m>k[x]</m>-Modules of Dimension <m>4</m></title>
      
                <p>
                  Let <m>k</m> be a field and <m>M</m> a <m>k[x]</m>-module such that the dimension of <m>M</m> as a <m>k</m>-vector space is <m>4.</m> What are the possibilities for <m>M</m> up to isomorphism?
                </p>
      
                <p>
                  We have <me>M \cong k[x]^r \oplus k[x]/(f_1(x)) \oplus \cdots \oplus k[x]/(f_s(x))</me>with <m>r, s, f_1, \dots, f_s</m> as in the Corollary. But <m>r</m> must be <m>0</m> since <m>k[x]</m> is infinite dimensional as a <m>k</m>-vector space. Moreover, <m>\dim_F k[x]/g = \deg(g)</m> for any non-zero polynomial <m>g</m>. So we must have <m>\sum_i \deg(f_i) = 4</m>. There are five possibilities: 1. <m>s = 1</m> and <m>f_1</m> is monic of degree <m>4</m>. 2. <m>s = 2</m>, <m>f_1</m> is linear, <m>f_2</m> is cubic and <m>f_1 \mid f_2</m>. (So if <m>f_1 = x-a</m>, then <m>a</m> must be a root of <m>f_2</m>). 3. <m>s = 2</m>, <m>f_1 = f_2</m> is quadratic. 4. <m>s = 3</m>, <m>f_1 = f_2</m> is linear and <m>f_3</m> is quadratic with <m>f_1 \mid f_3</m>. 5. <m>s = 4</m>, <m>f_1 = f_2 = f_3 = f_4</m> is linear. Now suppose <m>k = \Z/p</m>. What is the total number of possibilities? For case <m>1</m>, there are <m>p^4</m> monic polynomial of degree <m>4</m>. For case <m>2</m>, there are <m>p</m> choices for <m>f_1</m> and <m>p^2</m> choices for <m>f_2</m> since <m>f_2 = f_1 \cdot q</m> for a unique quadratic <m>q</m>, for a total of <m>p^3</m> possibilities. For case <m>3</m> there are <m>p^2</m> choices. For case <m>4</m> there are <m>p^2</m> choices since there are <m>p</m> choices for <m>f_1 = f_2</m> and <m>f_3 = f_1 \cdot l</m> for a unique linear <m>l</m>. For case <m>5</m>, there are <m>p</m> choices for <m>f_1</m>. In total there are <m>2p + 2p^2 + p^4</m> such modules up to isomorphism.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="thm-sunzis-remainder-theorem-rings">
                <title><m>\thm</m> – Sunzi’s Remainder Theorem (Rings)</title>
      
                <p>
                  Suppose <m>I</m> and <m>J</m> are ideals in a commutative ring <m>R</m> such that <m>I + J = R</m>. Then <m>I \cap J = I \cdot J</m> (where <m>I \cdot J</m> is defined as the set of all sums of products of the form <m>ab</m> with <m>a \in I</m> and <m>b \in J</m>) and there is an isomorphism of <m>R</m>-modules <me>R/(I J) \cong R/I \oplus R/J.</me> In particular, if <m>R</m> is a UFD and <m>x</m> and <m>y</m> are relatively prime elements of <m>R</m>, then <me>R/(xy) \cong R/x \oplus R/y</me>
                </p>
      
      
                <paragraphs xml:id="proof.-126">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Note that <m>I \cdot J \subseteq I \cap J</m> holds in general for any pair of ideals. If <m>I + J = R</m> then <m>a + b = 1</m> for some <m>a \in I</m> and <m>b \in J</m>. Given <m>x \in I \cap J</m> we have <m>xa + xb = x</m> with <m>xa \in I \cdot J</m> and <m>xb \in I \cdot J</m>, which proves <m>x \in I \cdot J</m>.
                  </p>
      
                  <p>
                    For the second assertion define <m>g: R \to R/I \oplus R/J</m> to be the <m>R</m>-module homomorphism <m>g(r) = (r +I, r + J)</m>. Note that the kernel of <m>g</m> is <m>I \cap J</m> which equals <m>I\cdot J</m> by the first assertion. I claim <m>g</m> is onto: Pick <m>(x + I, y +J) \in R/I \oplus R/J</m>. With <m>a,b</m> chosen as above, we have <me> g(xb + ya) = (xb + ya + I, xb + ya + J) = (xb + I, ya + J) = (x + I, y +J).</me>The last equation holds since <m>xa + xb = x</m> and thus <m>xb + I = xb + xa + I = x + I</m> and similarly <m>ya + J = y + J</m>.
                  </p>
      
                  <p>
                    For the final assertion, recall that when <m>R</m> is a UFD, two principle ideals <m>Rx</m> and <m>Ry</m> satisfy <m>Rx + Ry = R</m> if and only if <m>x</m> and <m>y</m> are relatively prime. Also, for <m>I = Rx</m> and <m>J = Ry</m>, we have <m>I\cdot J = R(xy)</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="thm-ftfgmopidedf">
                <title><m>\thm</m> – FTFGMOPIDEDF</title>
      
                <p>
                  Let <m>R</m> be a PID and let <m>M</m> be a finitely generated <m>R</m>-module. Then there exist integers <m>r \geq 0, k \geq 0</m>, prime elements <m>p_1,\ldots,p_s</m> of <m>R</m> (not necessarily distinct), and integers <m>e_1,\ldots ,e_s \geq 1</m>, such that <me>M \cong R^r \oplus R/(p_1^{e_1}) \oplus \cdots \oplus R/(p_s^{e_s}).</me>Moreover <m>r</m> and <m>k</m> are uniquely determined by <m>M</m>, and the list <m>p_1^{e_1},\ldots, p_s^{e_s}</m> is unique up to associates and reordering.
                </p>
      
      
                <paragraphs xml:id="proof.-127">
                  <title><em>Proof.</em></title>
      
                  <p>
                    First write <m>M</m> in invariant factor form <m>M \cong R^r \oplus R/(d_1) \oplus \dots \oplus R/(d_k)</m>. For any non-zero, non-unit element <m>d</m> of <m>R</m>, we have <m>d = p_1^{e_1} \cdots p_m^{e_m}</m> for some distinct (non-associate) prime elements <m>p_j</m> and integers <m>e_j \geq 1</m>. By the Chinese remainder theorem (applied over and over again) we have <me>R/(d)\cong R/(p_1^{e_1})  \oplus \cdots \oplus R/(p_m^{e_m})</me> Doing this for each fact <m>R/(d_i)</m> in the invariant factor form of <m>M</m> we obtain the existence of an elementary divisor form of <m>M</m>.
                  </p>
      
                  <p>
                    Uniqueness follows from the uniqueness of the invariant factor form and of the prime factorizations of the <m>d_i</m>’s.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="defn-elementary-divisor-module">
                <title><m>\defn</m> – Elementary Divisor (Module)</title>
      
                <p>
                  With the notation in the Theorem above, the elements <m>p_1^{e_1},\ldots, p_s^{e_s}</m> of <m>R</m> are the elementary divisors of <m>M</m>. #fix
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-z90">
                <title><m>\exe</m> – <m>\Z/90</m></title>
      
                <p>
                  Since <m>\Z/90 \cong \Z/9 \oplus \Z/2 \oplus \Z/5</m>, so the elementary divisors of <m>\Z/90</m> are <m>9</m>, <m>2</m> and <m>3</m>. The only invariant factor of <m>\Z/90</m> is <m>90</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-direct-sums-and-z">
                <title><m>\exe</m> – Direct Sums and <m>\Z</m></title>
      
                <p>
                  Consider the group <m>G = \Z/90 \oplus \Z/9 \oplus \Z/6 \oplus \Z/3 \oplus \Z/4</m>. This is neither in IFF nor in EDF. Applying the Sunzi Remainder Theorem, we have <me>G \cong \Z/9 \oplus \Z/2 \oplus \Z/5 \oplus \Z/9 \oplus \Z/2 \oplus \Z/3 \oplus \Z/3 \oplus \Z/4,</me> and this gives the EDF. The elementary divisors of <m>G</m> are <m>9,2,5,9,2,3,3,4</m> (ordering does not matter).
                </p>
      
                <p>
                  To find the IFF we start by finding the largest prime power order for each prime in the list of orders of the summands. These are <m>9, 5, 4</m>. The CRT gives <me>G \cong \Z/(9 \cdot 4 \cdot 5) \oplus \Z/2 \oplus \Z/9 \oplus \Z/2 \oplus \Z/3 \oplus \Z/3.</me> Then we find the highest prime power orders for each prime among the left-over summands: <m>9, 2.</m> By the CRT we have <me>G \cong \Z/(9 \cdot 4 \cdot 5) \oplus \Z/(2 \cdot 9) \oplus   \Z/2 \oplus \Z/3 \oplus \Z/3.</me> The highest orders of the prime power order not yet used are <m>2</m> and <m>3</m>, and we have <me>G \cong \Z/(9 \cdot 4 \cdot 5) \oplus \Z/(2 \cdot 9) \oplus   \Z/(2 \cdot 3)  \oplus \Z/3.</me> which can be rearranged to give <me>G \cong \Z/3 \oplus \Z/6 \oplus \Z/18 \oplus \Z/180.</me> This is the IFF, and the invariant factors of <m>G</m> are <m>3,6,18,180</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="cor-ftfgagedf">
                <title><m>\cor</m> – FTFGAGEDF</title>
      
                <p>
                  Let <m>G</m> be a finitely generated abelian group. Then there exist integers <m>r,s \geq 0</m>, positive prime integers <m>p_1, \ldots, p_s</m>, and strictly positive integers <m>e_i \geq 1</m> such that <me>G \cong \Z^r\oplus\Z/p_1^{e_1} \oplus \dots \oplus \Z/p_s^{e_s}.</me>Moreover, the <m>r,p_i</m>’s, and <m>e_i</m>’s are uniquely determined by <m>G</m> (up to ordering).
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="cor-ftfgmopredf">
                <title><m>\cor</m> – FTFGMOPREDF</title>
      
                <p>
                  Let <m>k</m> be a field and <m>k[x]</m> be the ring of polynomials with coefficients in <m>k</m> in one variable <m>x</m>. If <m>M</m> a finitely generated <m>k[x]</m>-module then <me>
      M \cong k[x]^r \oplus k[x]/(p_1^{e_1}) \oplus \cdots \oplus k[x]/(p_s^{e_s})</me>for some <m>r \geq 0</m>, <m>s \geq 0</m>, non-constant monic, irreducible polynomials <m>p_1, \dots, p_s</m> and integers <m>e_1, \dots, e_s</m> with <m>e_i \geq 1</m>, and these are unique up to ordering.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-finding-ifs-and-eds">
                <title><m>\exe</m> – Finding IFs and EDs</title>
      
                <p>
                  Find the invariant factor form and the elementary divisor form of the <m>k[x]</m>-module <me>M = k[x]/(x^2-1) \oplus k[x]/(x-1) \oplus k[x]/(x^2 + 1) \oplus k[x]/(x^2 + 2x + 1)</me> first when (a) <m>k = \Z/2</m>, (b) <m>k = \R</m> and (c) <m>k = \C</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-40">
                <title>Problem</title>
      
                <p>
                  Let <m>R</m> be a Euclidean domain, <m>A</m> an <m>m \times n</m> matrix with elements from <m>R</m>, and <m>A^T</m> the transpose matrix of <m>A</m>. Let <m>\coker(A)</m> denote the quotient of <m>R^m</m> by the submodule generated by the columns of <m>A</m>. The {} submodule of an <m>R</m>-module <m>M</m> is the submodule <me>\{m \in M \mid rm = 0 \text{ for some $r \ne 0$}\}.</me> (It is indeed a submodule and you need not prove this.) 1. Prove that the torsion submodules of <m>\coker(A)</m> and <m>\coker(A^T)</m> are isomorphic. 2. Prove that the modules <m>\coker(A)</m> and <m>\coker(A^T)</m> are isomorphic if and only if <m>m = n</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-41">
                <title>Problem –</title>
      
                <p>
                  Let <m>R</m> be a PID and let <m>M</m> be a finitely generated <m>R</m>-module. 1. Determine a generator for the principal ideal <m>\ann_R(M)</m> in terms of the invariant factors and the free rank of <m>M</m>. 2. Determine a generator for the principal ideal <m>\ann_R(M)</m> in terms of the elementary divisors and the free rank of <m>M</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-42">
                <title>Problem –</title>
      
                <p>
                  Consider the matrix <me>A=\begin{bmatrix}
      x &amp; 1 &amp; 0 \\
      1 &amp; x &amp; -3 \\
      0 &amp; 0 &amp; x-1
      \end{bmatrix}
      \in \M_{3,3}(R),</me> where <m>R=\Q[x]</m>. 1. Determine the Smith normal form for <m>A</m>. 2. Determine the representative in the isomorphism class of the module presented by <m>A</m> which is written in invariant factor form and in elementary divisor form. # Canonical Forms for Linear Operators ## Modules over Polynomial Rings #### <m>\rem</m> Suppose <m>F</m> is a field and <m>M</m> is a <m>F[x]</m>-module. By restriction of scalars along the canonical ring map <m>F \into F[x]</m> we may regard <m>M</m> as a <m>F</m>-vector space — let us write this vector space as <m>M|_F</m> to be precise. Let <m>\lx: M|_F \to M|_F</m> be the map given by <m>\lx(m) = xm</m>. Then <m>\lx</m> is an <m>F</m>-linear operator on <m>M|_F</m>. So, to a <m>F[x]</m>-module <m>M</m> we may associate the pair <m>(M|_F, \lx)</m> where <m>M|_F</m> is an <m>F</m>-vector space and <m>\lx</m> is an <m>F</m>-linear operator on <m>M|_F</m>. This process is reversible:
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-fx-module-v_g">
                <title><m>\defn</m> – <m>F[x]</m>-Module <m>V_g</m></title>
      
                <p>
                  Let <m>F</m> be a field, let <m>V</m> be a finite dimensional vector space over <m>F</m>, and let <m>g: V \to V</m> be an <m>F</m>-linear operator. The <m>F[x]</m>-module <m>V_g</m> is defined to be the abelian group <m>(V, +)</m> equipped with the rule for <m>F[x]</m>[^1] scaling given by <me>(c_nx^n + \dots+ c_0)v = c_ng^{\circ n}(v) + \dots + c_1 g(v) + c_0v</me>for any polynomial <m>c_nx^n + \dots + c_0 \in F[x]</m> and vector <m>v \in V</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="prop-v_g-is-actually-a-fx-module">
                <title><m>\prop</m> – <m>V_g</m> is Actually a <m>F[x]</m>-Module</title>
      
                <p>
                  Given a pair <m>(V, g)</m> as in the definition, <m>V_g</m> really is a <m>F[x]</m>-module.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-special-case-of-v_g">
                <title><m>\exe</m> – Special Case of <m>V_g</m></title>
      
                <p>
                  We have the following special case (it isn’t really special — the general case reduces to this one upon choosing a basis):
                </p>
      
                <p>
                  Given a matrix <m>A \in \Mat_{n \times n}(F)</m>, then <m>F^n_A</m> is the <m>F[x]</m>-module whose underlying abelian group is <m>F^n</m> (column vectors) with the usual rule for addition and with the rule for <m>F[x]</m> scaling given by <me>
      (\sum_i c_i x^i) \cdot v := \sum c_i A^i v
      </me> for any column vector <m>v</m>. For short, we write this rule as <me>
      f(x) \cdot v = f(A) v
      </me> for any polynomial <m>f(x) \in k[x]</m>, where <m>f(A)</m> is the matrix obtained by evaluating <m>f(x)</m> at <m>x = A</m> in the evident sense.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-mat_2times-2q">
                <title><m>\exe</m> – <m>\Mat_{2\times 2}(\Q)</m></title>
      
                <p>
                  Let <m>A=\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}\in \Mat_{2 \times 2}(\Q)</m> and let <m>M</m> be the <m>\Q[x]</m>-module <m>\Q^2_A</m>. So <m>M = \Q^2</m> as a <m>\Q</m>-vector space, and <m>x</m> acts on <m>M</m> by sending <m>\vectwo{a}{b} \in M</m> to <m>\vectwo{a+b}{b} \in M</m>. I claim there is an isomorphism <me>
      M \cong \Q[x]/(x-1)^2
      </me> of <m>\Q[x]</m>-modules.
                </p>
      
                <p>
                  Let <m>v = \vectwo{0}{1} \in M</m>. Note that <m>x \cdot v = \vectwo{1}{1}</m> and that <m>v</m> and <m>x \cdot v</m> span <m>\Q^2</m> as a <m>\Q</m>-vector space. It follows that <m>v</m> generates <m>M</m> as a <m>\Q[x]</m>-module; in detail, for any <m>\vectwo{a}{b} \in \Q^2</m> we have <m>(b- a + ax)v = \vectwo{0}{a} + \vectwo{a}{a} = \vectwo{a}{b}</m>.
                </p>
      
                <p>
                  Define a <m>\Q[x]</m>-module homomorphism <me>\pi: \Q[x] \onto M</me> by sending <m>1</m> to <m>v \in M</m> and hence <m>p(x)</m> to <m>p(x) \cdot v</m>. It is onto since <m>v</m> generates <m>M</m> as a <m>\Q[x]</m>-module. The kernel will be a (necessarily principle) ideal of <m>\Q[x]</m>; we just need to find it. Note that <m>x^2 v = \vectwo21</m>, <m>xv</m> and <m>v</m> are linearly dependent and in fact we have <me>x^2 v = 2x v- v</me>and hence <m>(x^2 -2x + 1)v =0</m>. This gives that <m>x^2 -2x + 1</m> is in the kernel of <m>\pi</m> and hence, by the <m>0</m>-th Isomorphism Theorem we have an induced homomorphism of <m>\Q[x]</m>-modules <me>\ov{\pi}: \Q[x]/(x^2 - 2x + 1) \onto M</me>induced by <m>\pi</m>. The map <m>\ov{\pi}</m> is onto since <m>\pi</m> is onto. Since the source and target both have dimension two as <m>\Q</m>-vector spaces, <m>\ov{\pi}</m> is <m>\Q</m>-linear, and <m>\ov{\pi}</m> is onto, it must in fact be an isomorphism of <m>\Q[x]</m>-modules (by the Rank-Nullity Theorem).
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="prop-equality-of-fx-modules">
                <title><m>\prop</m> – Equality of <m>F[x]</m>-Modules</title>
      
                <p>
                  The two assignments <m>M \mapsto (M|_F, \lx)</m> and <m>(V, g) \mapsto V_g</m> defined above are mutually inverse: Given a <m>F[x]</m>-module <m>M</m>, there is an <em>equality</em> of <m>F[x]</m>-modules <m>M = (M|_F)_{\lx}</m> and given a pair <m>(V, g)</m> with <m>V</m> an <m>F</m>-vector space and <m>g</m> an <m>F</m>-linear operator on <m>V</m> we have an equality of pairs <m>((V_g)|_F, \lx) = (V,g)</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-57">
                <title><m>\rem</m></title>
      
                <p>
                  In fact, these rules determine an “isomorphism of categories’’.
                </p>
      
              </paragraphs>
            </paragraphs>
          </subsection>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>
        
      </chapter>

      <chapter xml:id="ch-canform">
        <title>Canonical Forms</title>

        <section xml:id="sec-rcf">
          <title>Rational Canonical Form</title>

          <subsection xml:id="rcf">
            <title>RCF</title>
      
      
            <paragraphs xml:id="defn-block-diagonal-matrix">
              <title><m>\defn</m> – Block Diagonal Matrix</title>
      
              <p>
                Given square matrices <m>A_1 \in \Mat_{n_1 \times n_1}(F), \dots, A_m \in \Mat_{n_m \times n_m}(F)</m>, we define <m>A_1 \oplus \cdots \oplus A_m</m> to be the <em><m>\defnn{block diagonal}</m></em> matrix <me>
      \begin{bmatrix} 
      A_1 &amp; 0 &amp; \cdots &amp; 0 \\
      0 &amp; A_2 &amp;  \cdots &amp; 0 \\
      \vdots &amp; &amp; \ddots &amp; \vdots \\
      0 &amp; \cdots &amp; 0 &amp; A_m \\
      \end{bmatrix}
      </me> which belongs to <m>\Mat_{n \times n}(F)</m> for <m>n = \sum_i n_i</m>.
              </p>
      
      
              <paragraphs xml:id="thm-rational-canonical-form">
                <title><m>\thm</m> – Rational Canonical Form</title>
      
                <p>
                  Given a finite dimensional <m>F</m>-vector space <m>V</m> and an <m>F</m>-linear operator <m>g: V \to V</m>, there is a basis <m>B</m> of <m>V</m> such that the matrix representing <m>g</m> relative to <m>B</m> is <me>[g]_B^B = C(f_1) \oplus \cdots \oplus C(f_s)</me>for monic polynomials <m>f_1, \dots, f_s</m> of degree at least one such that <m>f_1 | f_2 | \cdots | f_s</m>[^1]. Moreover, this matrix is unique, and is known as the <em>rational canonical form</em> of the operator <m>g</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-128">
                  <title><em>Proof.</em></title>
      
                  <p>
                    We know by the Fundamental Theorem of modules over <m>F[x]</m> (i.e., Corollary ) that there is a <m>F[x]</m>-module isomorphism <me>
      V_{g} \cong F[x]/(f_1)\oplus \cdots \oplus F[x]/(f_s)
      </me> for some unique list of monic, non-constant polynomials <m>f_1, \cdots, f_s</m> with <m>f_i \mid f_{i+1}</m> for all <m>i</m>. Recall that the operator <m>g</m> on <m>V</m> is given as <m>\lx</m> (multiplication by <m>x</m>) on <m>V_g</m>. Since this is a <m>F[x]</m>-module isomorphism, <m>g</m> corresponds to multiplication by <m>x</m> on each summand <m>F[x]/(f_i)</m>. As we have seen before, for each <m>i</m>, the matrix representing <m>\lx</m> on <m>F[x]/f_i</m> relative to the basis <m>B_i := \{1, x, x^2, \dots, x^{\deg(f_i)-1}\}</m> of <m>F[x]/f_i</m> is the companion matrix <m>C(f_i)</m> of <m>f_i</m>. Let <m>B</m> be the <m>F</m>-basis of <m>F[x]/(f_1)\oplus \cdots \oplus F[x]/(f_s)</m> given by tuples <me>B = \{(b_1, 0, \dots, 0) \mid b_1 \in B_1\} \cup \{(0, b_2, 0, \dots, 0) \mid b_2 \in B_{2\} \cup}\cdots \cup \{(0, 0, \dots, 0, b_s) \mid b_s \in B_s\}</me>(in that order). Then the matrix of <m>\lx</m> on <m>F[x]/(f_1)\oplus \cdots \oplus F[x]/(f_k)</m> for <m>B</m> is <m>C(f_1) \oplus \cdots \oplus C(f_s)</m>.
                  </p>
      
                  <p>
                    This gives existence. Uniqueness is a consequence of the uniqueness of the list <m>f_1, \dots, f_s</m>, but I will omit the details.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-58">
                <title><m>\rem</m></title>
      
                <p>
                  The matrix is unique, but the basis <m>B</m> that realizes it is, in general, not unique. As an extreme example illustrating this: Take <m>g</m> to be the identity operator on a finite dimensional vector space <m>V</m>. Then <m>[g]_B^B = I_n</m> holds for any basis <m>B</m>. (Note that <m>I_n</m> is indeed in rational canonical form: it is equal to <m>C(x-1) \oplus \cdots \oplus C(x-1)</m>.)
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-invariant-factor">
                <title><m>\defn</m> – Invariant Factor</title>
      
                <p>
                  In Theorem , the number <m>r</m> is the <em>rank</em> of <m>G</m>, the numbers <m>n_1,\ldots,n_t</m> are the <em><m>\defnn{invariant factors}</m></em> of <m>G</m>, and the decomposition of <m>G</m> in part (1) is the <em>invariant factor decomposition</em> of <m>G</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-back-to-mat_2times-2q">
                <title><m>\exe</m> – Back to <m>\Mat_{2\times 2}(\Q)</m></title>
      
                <p>
                  Let us return to the example of <m>A = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}\in \Mat_{2 \times 2}(\Q)</m> to illustrate the Theorem and its proof. By the previous example we have an isomorphism of <m>\Q[x]</m>-module <me>
      \Q^2_A \cong \Q[x]/(x^2 - 2x + 1).
      </me> Recall that <m>\lx</m> (multiplication by <m>x</m>) on <m>\Q^2_A</m> is given by multiplication by the matrix <m>A</m>. This is an isomorphism of <m>\Q[x]</m>-modules, and so <m>A</m> corresponds to the operator <m>\lx</m> on <m>\Q[x]/(x^2 - 2x + 1)</m>. As we have seen before, relative to the basis <m>\{1, x\}</m>, the matrix for <m>\lx</m> is <me>
      C(x^2 - 2x + 1) = 
      \begin{bmatrix} 
      0 &amp; -1 \\
      1 &amp; 2 \\
      \end{bmatrix}.
      </me> This is the Rational Canonical Form of <m>A</m>. <m>A</m> has just one invariant factor, namely <m>x^2 - 2x + 1</m>.
                </p>
      
                <p>
                  By the way, tracking through the calculations that got us here, we see that the basis of <m>\Q^2</m> that gives the RCF of <m>A</m> if <m>\vectwo01, \vectwo11</m> of <m>\Q^2</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="cor-every-matrix-similar-to-unique-rcf-matrix">
                <title><m>\cor</m> – Every Matrix Similar to Unique RCF Matrix</title>
      
                <p>
                  Every matrix <m>A</m> is similar to a unique matrix in RCF.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-similarity-classes-of-4times-4-matrices">
                <title><m>\exe</m> – Similarity Classes of <m>4\times 4</m> Matrices</title>
      
                <p>
                  Let <m>F = \Z/p</m> be the field with <m>p</m> elements for some prime <m>p</m>. Up to similarity, how many <m>4 \times 4</m> matrices are there with entries in <m>F</m>?
                </p>
      
                <p>
                  Each such matrix is similar to a unique one of the form <me>
      C(f_1) \oplus \cdots \oplus C(f_k)
      </me> with <m>f_1, \dots, f_k</m> monic polynomials of positive degree such that <m>f_1 \mid \cdots \mid f_k</m>. Moreover, since <m>C(f)</m> is a <m>d \times d</m> matrix where <m>d = \deg(f)</m>, we must have <m>\deg(f_1) + \cdots + \deg(f_k) = 4</m>. So the goal becomes to count all such tuples <m>(f_1, \dots, f_k)</m> of polynomials. We proceed by cases on <m>k</m>. Note that <m>k \geq 5</m> is not possible. - Case <m>k = 1</m>. Then <m>\deg(f_1) = 4</m> and the number of such polynomials is <m>p^4</m> (since <m>f_1 = x^4 + a_3 x^3 + a_2 x^2 + a_1 x + a_0</m> and <m>F</m> has <m>p</m> elements). - Case <m>k = 2</m>: Note that <m>\deg(f_1) \geq 3</m> is not possible. If <m>\deg(f_1) = 2</m> then <m>f_1 = f_2</m>, and there are <m>p^2</m> possibilities. If <m>\deg(f_1) = 1</m>, then <m>\deg(f_2) = 3</m> and <m>f_2 = f_1 \cdot g</m> with <m>g</m> monic and <m>\deg(g) = 2</m>. There are <m>p</m> possibilities for <m>f_1</m> and <m>p^2</m> for <m>g</m>, for a total of <m>p^3</m> in this subcase. The total for this case is thus <m>p^2 + p^3</m>. - Case <m>k= 3</m>: The only possibilities are <m>\deg(f_1) = 1</m>, <m>\deg(f_2) = 1</m> and <m>\deg(f_3) = 2</m> so that <m>f_2 = f_1</m> and <m>f_3 = f_1 \cdot g</m> with <m>\deg(g) =1</m>. We get <m>p^3</m> possibilities. - Case <m>k = 4</m>. We must have <m>f_1 = f_2 = f_3 = f_4</m> with each of degree <m>1</m>, for a total of <m>p</m> possibilities.<!-- linebreak -->The total is <me>p^4 + p^2+ p^3 + p^3 + p.</me>
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-59">
                <title><m>\rem</m></title>
      
                <p>
                  The proof of Theorem  makes clear the following fact:
                </p>
      
                <blockquote>
                            <p>
                  For a field <m>F</m>, finite dimensional vector space <m>V</m>, and <m>F</m>-linear operator <m>g: V \to V</m>,the invariant factors of the operator <m>g</m> are identical to the invariant factors of the <m>F[x]</m>-module <m>V_g</m>.
                </p>
                </blockquote>
      
                <p>
                  The following result is thus very useful for finding the Rational Canonical Form of an operator (we will state it just for operators given explicitly by matrices):
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="thm-rcf-and-cokernels">
                <title><m>\thm</m> – RCF and Cokernels</title>
      
                <p>
                  Let <m>F</m> be a field and let <m>A \in \Mat_{n \times n}(F)</m>. The matrix <m>xI_n - A \in \Mat_{n \times n}(F[x])</m> presents the <m>F[x]</m>-module <m>F^n_A</m>[^1]; that is, there is an isomorphism of <m>F[x]</m>-modules <me>
      F^n_A \cong \coker(xI_n -A) = F[x]^n/\im(xI_n - A).
      </me>
                </p>
      
      
                <paragraphs xml:id="proof.-129">
                  <title><em>Proof.</em></title>
      
                  <p>
                    For this proof it is useful to identity <m>F[x]^n</m> with <m>F^n[x]</m> where the latter refers to all expressions of the form <m>\sum_i v_i x^i = v_0 + v_1 x + \cdots + v_m x^m</m> with <m>v_i \in F^n</m>. For instance, (when <m>n = 2</m>) we identify <m>\vectwo{x^2 -3x + 1}{x^7 +x + 5}</m> with <m>\vectwo{1}{5} + \vectwo{-3}{1} x + \vectwo{1}{0} x^2 + \vectwo{0}{1} x^7</m>. Using this identification we define <me>\phi:  F[x]^n = F^n[x] \to F^n_A</me>by <m>\phi(\sum_i v_i x^i) = \sum_i A^i v_i</m>. Then <m>\phi</m> is a <m>F[x]</m>-module homomorphism — I leave it to you to verify this. <m>\phi</m> is onto since, e.g., for any <m>v \in F^n</m> we have <m>\phi(vx^0) = v</m>.
                  </p>
      
                  <p>
                    We have <me>\phi((xI_n - A) \cdot \sum_i v_i x^i) = \phi(\sum_i v_i x^{i+1} - \sum_i (Av_i) x^i) = \sum_i A^{i+1}(v_i) - \sum_i A^i(Av_i) = 0</me>and hence <m>\ker(\phi) \supseteq \im(xI_n - A)</m>. By the <m>0</m>-th isomorphism theorem, there is an induced <m>F[x]</m>-module homomorphism <me>\ov{\phi}: \frac{F^n[x]}{\im(xI_n - A)}  \onto F^n_A</me>induced by <m>\phi</m>, and it is onto since <m>\phi</m> is onto. It remains to show this map is one-to-one.
                  </p>
      
                  <p>
                    Since <m>\overline{\phi}</m> is <m>F[x]</m>-linear it is certainly <m>F</m>-linear. Since <m>\dim_F(F^n_A) = n</m>, to prove <m>\overline{\phi}</m> is one-to-one, it suffices to prove <m>\dim_F \frac{F^n[x]}{\im(xI_n - A)} \leq n</m> (by Rank-Nullity). I claim the images of the standard basis <m>e_1, \dots, e_n</m> in <m>\frac{F^n[x]}{\im(xI_n - A)}</m> span it as an <m>F</m>-vector space. To see this, note that <m>x^je_i</m>, for <m>1 \leq i \leq n</m>, <m>j \geq 0</m> span <m>F^n[x]</m> as an <m>F</m>-vector space, and hence they span the quotient. It thus suffices to show <m>\overline{x^j e_i}</m> lies in the span of <m>\overline{e_1}, \dots, \overline{e_n}</m> in <m>\frac{F^n[x]}{\im(xI_n - A)}</m> for all <m>i</m> and <m>j</m>.<!-- linebreak -->We have <m>x^{j-1} \cdot (A - x) e_i \in \im(xI_n - A)</m> and thus <me>\overline{x^j e_i} = \overline{x^{j-1} Ae_i}</me>and by repeating this argument we have <me>\overline{x^j e_i} =\overline{x^{j-1} Ae_i} = \overline{x^{j-2} A^2e_i} = \cdots =\overline{A^je_i} \in \Span_F(e_1, \dots, e_n).</me>
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="cor-invariant-factors-are-diagonal-snf-entries">
                <title><m>\cor</m> – Invariant Factors are Diagonal SNF Entries</title>
      
                <p>
                  The invariant factors of a matrix <m>A</m> are the non-zero, non-unit diagonal entries of the SNF of <m>xI_n - A</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-130">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Let <m>S</m> be the Smith Normal Form of <m>xI_n - A</m> and let <m>d_1, \dots, d_n</m> be its diagonal entries. As proven before, the matrix <m>xI_n - A</m> and <m>S</m> present isomorphic <m>F[x]</m>-modules, and thus the Theorem gives an isomorphism <me>
      F^n_A \cong F[x]/(d_1) \oplus \cdots \oplus F[x]/(d_n).
      </me> Since <m>\dim_F F^n_A &lt; \infty</m>, none of the <m>d_i</m>’s can be zero. So, each <m>d_i</m> is monic and <m>d_1 \mid \cdots \mid d_n</m>. Now some of the <m>d_i</m> might be non-zero constants, in which case <m>d_i</m> is a unit and <m>F[x]/(d_i) = 0</m>. Upon tossing those out, we are left with <me>F^n_A \cong F[x]/(f_1) \oplus \cdots \oplus F[x]/(f_k)</me>with each <m>f_i</m> monic of positive degree and <m>f_1 \mid \cdots \mid f_k</m>. These are, by definition, the invariant factors of <m>A</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="exe-once-more-to-back-to-mat_2times-2q">
                <title><m>\exe</m> – Once More to Back to <m>\Mat_{2\times 2}(\Q)</m></title>
      
                <p>
                  Let’s find the invariant factors of the matrix <m>A= \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}\in \M_{2 \times 2}(\Q)</m> we looked at before, but this time using the Theorem and its Corollary.
                </p>
      
                <p>
                  We have <me>
      xI_2-A= \begin{bmatrix} x-1 &amp; -1 \\ 0 &amp; x-1  \end{bmatrix}.
      </me> To find the invariant factors of <m>A</m> we just need to find the Smith Normal Form of <m>xI_2-A</m>. I’ll do this two ways:
                </p>
      
                <p>
                  Method I: Do row and column operations using the generalized Euclidean algorithm: <me>
      \begin{bmatrix} x-1 &amp; -1 \\ 0 &amp; x-1  \end{bmatrix} \mapsto
      \begin{bmatrix} x-1 &amp; -1 \\ (x-1)^2 &amp; 0   \end{bmatrix} \mapsto
      \begin{bmatrix} 0 &amp; -1 \\ (x-1)^2 &amp; 0   \end{bmatrix} \mapsto
      \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; (x-1)^2  \end{bmatrix}.
      </me> Tossing out the unit, we see that the only invariant factor is <m>(x-1)^2</m>, as before.
                </p>
      
                <p>
                  Method II: Call <m>d_1, d_2</m> the entries on the diagonal of the SNF of <m>xI_2-A</m>. Recall from Theorem  that <m>d_1</m> is the gcd of the entries of <m>xI_2-A</m> and <m>d_1d_2=\det(xI_2-A)</m>. Thus <m>d_1=1</m> and <m>d_2=\det(xI_2-A)=(x-1)^2</m>. Therefore the only invariant factor of <m>A</m> is <m>(x-1)^2</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-finding-ifs-and-rcf">
                <title><m>\exe</m> – Finding IFs and RCF</title>
      
                <p>
                  Let <me>
      A = 
      \begin{bmatrix} 
      1 &amp; -1 &amp; 1 \\
      0 &amp; 0 &amp; 1 \\
      0 &amp; 1 &amp; 0 \\
      \end{bmatrix}.
      </me> Let us find the invariant factors and Rational Canonical Form of <m>A</m> by finding the Smith Normal Form of <m>xI_3 - A</m>.
                </p>
      
                <p>
                  We have <me>
      xI_3 - A =
      \begin{bmatrix} 
      x-1 &amp; 1 &amp; -1 \\
      0 &amp; x &amp; -1 \\
      0 &amp; -1 &amp; x \\
      \end{bmatrix}.
      </me> A sequence of messy row and column operations yields <me>
      \begin{bmatrix} 
      1 &amp; 0 &amp; 0 \\
      0 &amp; x-1 &amp; 0 \\
      0 &amp; 0 &amp; x^2-1 \\
      \end{bmatrix}.
      </me> Note that this is indeed in Smith Normal Form. It follows that the invariant factors of <m>A</m> are <m>x-1, x^2-1</m> and the RCF of <m>A</m> is <me>
      C(x-1) \oplus C(x^2-1) = 
      \begin{bmatrix} 
      1 &amp; 0 &amp; 0 \\
      0 &amp; 0 &amp; 1 \\
      0 &amp; 1 &amp; 0 \\
      \end{bmatrix}.
      </me>
                </p>
      
                <p>
                  For an alternative approach, we could use that the diagonal entries <m>d_1, d_2, d_3</m> of the Smith Normal Form of <m>xI_3 - A</m> satisfy <m>d_1 = \gcd(xI_3 - A)</m>, <m>d_1d_2</m> is the gcd of the <m>2 \times 2</m> minors of <m>xI_3 - A</m>, and <m>d_1d_2d_3 = \det(xI_3 - A)</m>. It’s clear that <m>d_1 = 1</m> and an easy calculation gives that <m>\det(xI_3 - A) = (x-1)(x^2 -1)</m>. There are nine <m>2 \times 2</m> minors of <m>xI_3 - A</m>, and a tedious check reveals that each of them is one of <m>x^2 -1</m>, <m>x(x-1)</m>, <m>x-1</m> or <m>0</m> (up to signs). So <m>d_1d_2 = x-1</m>. We get that <me>
      d_1 = 1, d_2 = x-1, d_3 = x^2 -1
      </me> as before.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-7-nilpotent-matrices-and-similarity">
                <title>Problem 7 – Nilpotent Matrices and Similarity</title>
      
                <p>
                  Let <m>F</m> be a field and recall that a square matrix <m>A</m> with entries in <m>F</m> is called nilpotent if <m>A^m = 0</m> for some positive integer <m>m</m>.
                </p>
      
                <p><ol>
                  <li>
                  Prove that if <m>A</m> is an <m>n \times n</m> nilpotent matrix, then <m>A^n = 0</m>.
                  </li>
      
                  <li>
                  Assume <m>n \leq 3</m> and prove that two <m>n \times n</m> nilpotent matrices are similar if and only if they have the same rank. (Recall the rank of a matrix is the dimension of the vector space spanned by its columns.)
                  </li>
      
                  <li>
                  Give an example, with justification, of two <m>4 \times 4</m> nilpotent matrices that have the same rank but are not similar.
                  </li>
      
                </ol></p>
      
      
                <paragraphs xml:id="proof.-131">
                  <title><em>Proof</em>.</title>
      
                  <p>
                    Let <m>F</m> be a field.
                  </p>
      
      
                  <paragraphs xml:id="part-a-33">
                    <title>Part (a)</title>
      
                    <p>
                      Let <m>A</m> be an <m>n \times n</m> nilpotent matrix. Let <m>\lambda</m> be some eigenvalue of <m>A</m>. Thus there exists some vector <m>v</m> such that <m>A\lambda=\lambda v</m>. Let’s consider this the base-case of some rather banal induction. Now assume that for <m>n</m> we have <m>A^nv=\lambda^nv</m>. Consider <me>A^{n+1}v=A(A^nv)=A(\lambda^n v)=\lambda^n(Av)=\lambda^{n+1}v.</me>Recall that as <m>A</m> is nilpotent, there exists some <m>m\in\N</m> such that <m>A^m=0</m>. As <m>\lambda</m> is an eigenvalue of <m>A</m>, by the above induction we see that <m>\lambda^m</m> is an eigenvalue for <m>A^m=0</m>. As <m>F</m> is a field and thus an integral domain, we see that <m>\lambda^n=0</m> implies that <m>\lambda=0</m> is as well. As this holds in the algebraic closure of <m>F</m> as well, we see that when factored into linear terms all the <m>\lambda=0</m>. Thus <m>\cp_A(x)=x^n</m>.
                    </p>
      
                    <p>
                      By the Cayley Hamilton Theorem we know <m>\mp_A(x)|\cp_A(x)=x^n</m>, and thus <m>A^n=0</m>.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-b-30">
                    <title>Part (b)</title>
      
                    <p>
                      Assume <m>n \leq 3</m> and let <m>A,B</m> be nilpotent matrices with entries in <m>F</m>.
                    </p>
      
                    <p>
                      <m>(\Rightarrow)</m> Suppose <m>A\sim B</m>. Thus there exists some invertible matrix <m>P</m> such that <m>PAP\inv=B</m> by the definition of similar matrices. Let <m>U\in\ker(PA)</m>. Thus <m>PA(U)=0</m> and <m>P(AU)=0</m>. We multiply both sides by <m>P\inv</m> to see that <m>AU=0</m>. Therefore the <m>\ker(PA)=\ker(A)</m>, and hence the ranks of <m>PA</m> and <m>A</m> are equal by Rank Nullity.
                    </p>
      
                    <p>
                      Next, observe <me>\begin{align}
          \rank(AP\inv)&amp;=\rank((AP\inv)^T)\\
          &amp;=\rank(P^{-1^T}A^T)\\
          &amp;=\rank(A^T)\\
          &amp;=\rank(A),
      \end{align}</me>as <m>(P^{-1})^T</m> is an invertible matrix. Thus <m>\rank(B)=\rank(PAP\inv)=\rank(A)</m>.
                    </p>
      
                    <p>
                      <m>(\Leftarrow)</m> Suppose that <m>\rank(A)=\rank(B)</m>. From Part (a) we know <m>\cp_A(x)=\cp_B(x)=x^n</m>.
                    </p>
      
                    <p>
                      We consider the case where <m>\cp_A(x)=\cp_B(x)=x^3</m>.
                    </p>
      
                    <p>
                      The only possible invariant factors involving <m>x^3</m> are 1. <m>x,x,x</m>; 2. <m>x,x^2</m>; and 3. <m>x^3</m> itself. However, if <m>x,x,x</m> are the invariant factors then the rank of <m>A</m> would be 3, making it invertible, contradicting the fact that 0 is an eigenvalue of <m>A</m>. Thus we need only consider the latter two cases.
                    </p>
      
                    <p>
                      Note that <me>C(x)=[0], C(x^2)=
      \begin{bmatrix}
      0 &amp; 0\\
      1 &amp; 0\\
      \end{bmatrix}, 
      C(x)\oplus C(x^2)\begin{bmatrix}
      0 &amp; 0 &amp; 0\\
      0 &amp; 0 &amp; 0\\
      0 &amp; 1 &amp; 0\\
      \end{bmatrix}, \textrm{ and } C(x^3)=
      \begin{bmatrix}
      0 &amp; 0 &amp; 0\\
      1 &amp; 0 &amp; 0\\
      0 &amp; 1 &amp; 0\\
      \end{bmatrix}.</me> As <m>C(x)\oplus C(x^2)</m> has rank 1 and <m>C(x^3)</m> has rank 2, since <m>\rank(A)=\rank(B)</m> we see that they must have the same invariant factors, making them similar.
                    </p>
      
                    <p>
                      If <m>n=2</m> then the only possible invariant factor is <m>x^2</m>, as having two <m>x</m>’s would make <m>A</m> and <m>B</m> invertible again. If <m>n=1</m> then <m>A=B=0</m> and we’re done.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-c-4">
                    <title>Part (c)</title>
      
                    <p>
                      Consider <m>A=C(x^2)\oplus C(x^2)</m> and <m>B=C(x)\oplus C(x^3)</m>.
                    </p>
      
                    <p>
                      Thus <me>A=\begin{bmatrix}
      0 &amp; 0 &amp; 0 &amp; 0\\
      1 &amp; 0 &amp; 0 &amp; 0\\
      0 &amp; 0 &amp; 0 &amp; 0\\
      0 &amp; 0 &amp; 1 &amp; 0\\
      \end{bmatrix} \textrm{ and } 
      B=\begin{bmatrix}
      0 &amp; 0 &amp; 0 &amp; 0\\
      0 &amp; 0 &amp; 0 &amp; 0\\
      0 &amp; 1 &amp; 0 &amp; 0\\
      0 &amp; 0 &amp; 1 &amp; 0\\
      \end{bmatrix},</me>
                    </p>
      
                    <p>
                      both of which have rank <m>2</m> and are in RCF. Thus they are not similar.
                    </p>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-43">
                <title>Problem</title>
      
                <p>
                  Find the Rational Canonical Form of <me>
      A = \begin{bmatrix}
      0 &amp; -1 &amp; - 1\\
      0 &amp; 0 &amp; 0 \\
      -1 &amp; 0 &amp; 0 \\
      \end{bmatrix} \in \Mat_{3 \times 3}(F),
      </me> where <m>F</m> is any field.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-44">
                <title>Problem</title>
      
                <p>
                  Find, with justification, a complete and non-redundant list of conjugacy class representatives for the group <m>\GL_3(\F_2)</m>, where <m>\F_2</m> is the field with two elements.
                </p>
      
      
                <paragraphs xml:id="proof.-132">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Let <m>\F_2</m> denote the field with two elements, and consider the group <m>\GL_3(\F_2)</m>.
                  </p>
      
                  <p>
                    Recall that matrices are in the same conjugacy class if and only if they are similar, and that two matrices are similar if and only if they share the same invariant factors.
                  </p>
      
                  <p>
                    Let <m>A\in\GL_3(\F_2)</m>. All characteristic polynomials are monic, and as <m>A</m> is invertible we see the <m>a_0</m> term in the <m>\cp_A(x)\neq0</m>, and thus <m>a_0=1</m>, the only other element in <m>\F_2</m>. There are only so many monic polynomials with coefficients in <m>\F_2</m>; hence there are only four possible characteristic polynomials of <m>A</m>: 1. <m>f(x)=x^3+x^2+x+1</m>, 2. <m>g(x)=x^3+x^2+1</m>, 3. <m>h(x)=x^3+x+1,</m> and 4. <m>j(x)=x^3+1</m>. Note that since <m>a_0=1</m>, 0 cannot be a root of any of these polynomials. Thus all that remains is to check <m>1</m>. Luckily, <m>1</m> is not a root of <m>g(x)</m> or <m>h(x)</m>, so the only invariant factor of each is themselves. While 1 is a root of <m>j(x)</m>, we see that <m>j(x)=(x+1)(x^2+x+1)</m>. As <m>(x+1)\not|(x^2+x+1)</m>, neither of these polynomials can be invariant factors by the RCF theorem; thus the only invariant factor of <m>j(x)</m> is itself. However <m>f(x)=(x+1)^3</m>, and so its possible invariant factors are - <m>(x+1)^3</m>; - <m>(x+1),(x+1)^2;</m> and - <m>(x+1),(x+1),(x+1)</m>. Let <m>k(x)=(x+1)^2</m> and <m>\ell(x)=x+1</m>. Thus the conjugacy class representatives for <m>\GL_3(\F_2)</m> are 1. <m>C(f)</m>, 2. <m>C(\ell)\oplus C(k)</m>, 3. <m>C(\ell)\oplus C(\ell)\oplus C(\ell)</m>, 4. <m>C(g)</m>, 5. <m>C(h)</m>, and 6. <m>C(j)</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
            </paragraphs>
          </subsection>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

        <section xml:id="sec-minpoly">
          <title>The Minimum Polynomial</title>

          <subsection xml:id="the-minimum-polynomial-of-an-operator">
            <title>The Minimum Polynomial of an Operator</title>
      
      
            <paragraphs xml:id="rem-60">
              <title><m>\rem</m></title>
      
              <p>
                Given a square matrix <m>A</m> and polynomial <m>g(x) = \sum_i a_i x^i</m>, recall that <m>g(A)</m> refers to the square matrix <m>\sum_i a_i A^i</m>.
              </p>
      
      
              <paragraphs xml:id="prop-ideals-and-gx">
                <title><m>\prop</m> – Ideals and <m>g(x)</m></title>
      
                <p>
                  Given a <m>n \times n</m> matrix <m>A</m> with entries in a field <m>F</m>, the set <m>I := \{g(x) \mid g(A) = 0 \}</m> forms a non-zero ideal of <m>F[x]</m>[^1].
                </p>
      
      
                <paragraphs xml:id="proof.-133">
                  <title><em>Proof.</em></title>
      
                  <p>
                    <m>I</m> is an ideal since the result of evaluating the sum of two polynomials <m>g(x) + f(x)</m> at <m>x = A</m> is <m>g(A) f(A)</m>. the result of evaluating the product <m>g(x) f(x)</m> at <m>x = A</m> is <m>g(A) f(A)</m>.
                  </p>
      
                  <p>
                    To show it is non-zero, consider the matrices <m>I_n, A, A^2, \dots, A^{n^2}</m>. This is a collection of <m>n^2 + 1</m> matrices in the <m>n^2</m> dimensional <m>F</m>-vector space <m>\Mat_{n \times n}(F)</m>, and hence the must be linearly dependent: there are <m>a_0, \dots, a_{n^2+1} \in F</m>, not all of which are <m>0</m>, such that <m>\sum_i a_i A^i = 0</m>. This proves <m>\sum_i a_i x^i \in I</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="defn-minimum-polynomial-matrix">
                <title><m>\defn</m> – Minimum Polynomial (Matrix)</title>
      
                <p>
                  Let <m>F</m> be a field and let <m>A \in \Mat_{n \times n}(F)</m>. The <em><m>\defnn{minimum polynomial}</m></em> of <m>A</m>, denoted <m>\mp_A(x)</m>, is the unique monic generator of the ideal <m>\{g(x) \in F[x] \, \mid \, g(A) = 0\}</m>. Equivalently, <m>\mp_A(x)</m> is the monic polynomial of least degree such that <m>\mp_A(A) = 0</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="prop-ga0-iff-gx-annihilates-fn_a">
                <title><m>\prop</m> – <m>g(A)=0</m> iff <m>g(x)</m> Annihilates <m>F^n_A</m></title>
      
                <p>
                  Given an <m>n \times n</m> matrix <m>A</m> and polynomial <m>g(x)\in F[x]</m>[^2], we have <m>g(A) = 0</m> if and only if <m>g(x)</m> annihilates the <m>F[x]</m>-module <m>F^n_A</m>.
                </p>
      
                <p>
                  In particular, <m>\mp_A(x)</m>[^1] is the unique monic generator of the annihilator ideal <me>
      \ann_{F[x]}(F^n_A) := \{ g(x) \in F[x] \mid g(x) \cdot v = 0 \text{ for all $v \in F^n_A$}\}.</me> ###### <em>Proof.</em> If <m>g(A) = 0</m>, then for each <m>v \in F^n</m>, by definition of the action of <m>F[x]</m> on <m>F^n_A</m> we have <me>
      g(x) \cdot v = g(A) v = 0
      </me> and so <m>g(x)</m> annihilates <m>F^n_A</m>. Conversely, if <m>g(x)</m> annihilates <m>F^n_A</m>, then <m>g(A) \cdot v = 0</m> for all <m>v \in F^n</m>. Taking <m>v = e_i</m> for each <m>i</m>, this says that each column of <m>g(A)</m> is <m>0</m> and hence <m>g(A)</m> is the zero matrix.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-minimum-polynomial-linear-transformation">
                <title><m>\defn</m> – Minimum Polynomial (Linear Transformation)</title>
      
                <p>
                  More generally, let <m>V</m> be an <m>F</m>-vector space of dimension <m>n</m>, and let <m>g: V \to V</m> be a linear transformation. The <em><m>\defnn{minimum polynomial}</m></em> of <m>g</m>, denoted <m>\mp_g(x)</m>, is the unique monic polynomial generating the ideal <m>\{f(x) \in F[x] \mid f(g) = 0\}</m> or, equivalently, the annihilator ideal <m>{\ann}_{F[x]}(V_g)</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="theorem-cayley-hamilton-thm">
                <title>Theorem – Cayley-Hamilton <m>\thm</m></title>
      
                <p>
                  Let <m>F</m> be a field, <m>V</m> a finite dimensional <m>F</m>-vector space, and <m>g: V \to V</m> an <m>F</m>-linear operator. Let <m>f_1(x), \dots, f_s(x)</m> be the invariant factors of <m>g</m>. 1. The product of the invariant factors of <m>g</m> equals the characteristic polynomial of <m>g</m>: <me>\cp_g(x) = f_1(x) \cdots f_s(x).</me> 2. The largest invariant factor of <m>g</m> is equal to the minimum polynomial of <m>g</m>: <me>\mp_g(x) = f_s(x).</me> 3. (The Cayley-Hamilton Theorem) The minimum polynomial of <m>g</m> divides its characteristic polynomial. In particular, <m>g</m> satisfies its characteristic polynomial: <me>\cp_g(g) = 0.</me> ###### <em>Proof.</em> The first assertion is a consequence of Corollary , since the product of the diagonal elements of the Smith Normal Form of <m>xI_n - A</m> is equal to the determinant of <m>xI_n - A</m>. (Technically, we can only conclude at first that they are only associates, but since each is monic, they must be equal.)
                </p>
      
                <p>
                  For the second, we use the isomorphism of <m>F[x]</m>-modules <me>
      V_g \cong F[x]/(f_1(x)) \oplus \cdots \oplus F[x]/(f_s(x)). 
      </me> Note that a polynomial <m>p(x)</m> annihilates <m>F[x]/(f_i(x))</m> if and only if <m>f_i(x)</m> divides <m>p(x)</m>. Since <m>f_1 \mid \cdots \mid f_s</m>, the annihilator of the <m>F[x]</m>-module <m>F[x]/(f_1(x)) \oplus \cdots \oplus F[x]/(f_s(x))</m> is generated by <m>f_s(x)</m>. Thus the annihilator of <m>V_t</m> is also generated by <m>f_s(x)</m>, and by the Proposition  <m>f_s(x)</m> is the minimum polynomial of <m>t</m>.
                </p>
      
                <p>
                  The third assertion is an immediate consequence of the first two.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-finding-minimum-polynomial">
                <title><m>\exe</m> – Finding Minimum Polynomial</title>
      
                <p>
                  Let’s find the minimum polynomial of <me>
      \begin{bmatrix}
      1 &amp; 1 &amp; 0 &amp; 0 \\
      0 &amp; 1 &amp; 1 &amp; 0 \\
      0 &amp;  0 &amp; 1 &amp; 1 \\
      0 &amp;  0 &amp; 0 &amp; 1 \\
      \end{bmatrix}
      </me>
                </p>
      
                <p>
                  We apply the Cayley-Hamilton Theorem: <m>\mp_A(x) \mid \cp_A(x)</m>. The polynomial <m>\cp_A(x)</m> is easy to compute since this matrix is upper-triangular:<me>\cp_A(x) =\det(xI_4 - A) = (x-1)^4.</me> So <m>\mp_A(x) = (x-1)^j</m> for some <m>j \leq 4</m>. By brute-force, we verify that <m>(A-I_4)^3 \ne 0</m> and thus it must be the case that <m>\mp_A(x) = \cp_A(x) = (x-1)^4</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-finding-minimum-polynomial-2">
                <title><m>\exe</m> – Finding Minimum Polynomial (2)</title>
      
                <p>
                  Let’s find the minimum polynomial of <me>
      \begin{bmatrix}
      1 &amp; 1 &amp; 0 &amp; 0 \\
      0 &amp; 1 &amp; 0 &amp; 0 \\
      0 &amp;  0 &amp; 1 &amp; 1 \\
      0 &amp;  0 &amp; 0 &amp; 1 \\
      \end{bmatrix}
      </me> As in the previous example, <m>\cp_A(x) = (x-1)^4</m> and so by the Cayley-Hamilton Theorem <m>\mp_A(x) = (x-1)^j</m> for some <m>j \leq 4</m>. This time we notice that <m>(A-I_4)^2 = 0</m> and so, since <m>(A-I_4) \ne 0</m>, <m>\mp_A(x) = \cp_A(x) = (x-1)^2</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-6-similarity-classes-and-charpoly">
                <title>Problem 6 – Similarity Classes and CharPoly</title>
      
                <p>
                  Determine all similarity classes of matrices with entries in <m>\Q</m> with characteristic polynomial <m>(x^4-1)(x^2-1)</m>. Provide an explicit representative for each of these similarity classes.
                </p>
      
      
                <paragraphs xml:id="proof.-134">
                  <title><em>Proof</em>.</title>
      
                  <p>
                    Let <m>A</m> be a matrix with entries in <m>\Q</m> with characteristic polynomial <m>(x^4- 1)(x^2 -1)</m>.
                  </p>
      
                  <p>
                    By this Corollary we know that every matrix is similar to a unique matrix in RCF. Note that RCF is based on the invariant factors if a matrix, and thus if two matrices have the same invariant factors they will have the same RCF, making them both similar to the same (unique) matrix, making them similar to each other. By part (1) of this Theorem, the characteristic polynomial of a matrix is equal to the product of the invariant factors of that same matrix.
                  </p>
      
                  <p>
                    Recall that the invariant factors must divide all preceding invariant factors in RCF, and observe that <m>(x^4-1)</m> factors as <m>(x^2-1)(x^2+1)</m> and <m>(x^2-1)</m> factors as <m>(x-1)(x+1)</m>. Given this information, after some fiddling with the factors, we find four possible options for invariant factors of <m>A</m>: 1. <m>x^6-x^4-x^2+1=(x^2 -1)(x^4- 1)</m>, 2. <m>(x^2-1),(x^4-1)</m>, 3. <m>(x+1), (x-1)(x^4-1)</m>, and 4. <m>(x-1),(x+1)(x^4-1)</m>. Let <m>f(x)=x^6-x^4-x^2+1, g(x)=(x^4-1),</m> <m>h(x)=x^2-1</m>, <m>j(x)=(x-1),</m> and <m>k(x)=(x+1)</m>. Observe the companion matrices of each of these polynomials: <me>C(f)=\begin{bmatrix}
      0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -1\\
      1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
      0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
      0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
      0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1\\
      0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
      \end{bmatrix},
      C(g)=\begin{bmatrix}
      0 &amp; 0 &amp; 0 &amp; 1\\
      1 &amp; 0 &amp; 0 &amp; 0\\
      0 &amp; 1 &amp; 0 &amp; 0\\
      0 &amp; 0 &amp; 1 &amp; 0\\
      \end{bmatrix},
      C(h)=\begin{bmatrix}
      0 &amp; 1\\
      1 &amp; 0\\
      \end{bmatrix},
      C(j)=\begin{bmatrix}
      1\\ 
      \end{bmatrix}\textrm{ and } 
      C(k)=\begin{bmatrix}
      -1\\ 
      \end{bmatrix}
      </me> Behold: explicit representatives of each similarity class: 1. <m>C(f)</m>, 2. <m>C(g)\oplus C(h)</m>, 3. <m>C(k)\oplus C(gj)</m>, and 4. <m>C(j)\oplus C(gk)</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-6-similarity-of-3-x-3-matrices">
                <title>Problem 6 – Similarity of 3 x 3 Matrices</title>
      
                <p>
                  Let <m>F</m> be any field.
                </p>
      
                <p><ol>
                  <li>
                  Let <m>A</m> and <m>B</m> be two <m>3\times 3</m> matrices with entries in <m>F</m>. Prove <m>A</m> and <m>B</m> are similar if and only if they have the same characteristic polynomial and the same minimum polynomial.
                  </li>
      
                  <li>
                  Show, by way of an example with justification, that the previous part would become false if <q><m>3\times 3</m></q> were replaced by <q><m>4\times 4</m></q>.
                  </li>
      
                  <li>
                  Give an example of a field <m>F</m> and two <m>3\times 3</m> matrices with entries in <m>F</m> having the same minimum polynomial that are not similar.
                  </li>
      
                </ol></p>
      
      
                <paragraphs xml:id="proof.-135">
                  <title><em>Proof</em>.</title>
      
                  <p>
                    Let <m>F</m> be any field.
                  </p>
      
      
                  <paragraphs xml:id="part-a-34">
                    <title>Part (a)</title>
      
                    <p>
                      Let <m>A</m> and <m>B</m> be two <m>3\times 3</m> matrices with entries in <m>F</m>. First, suppose that <m>A\sim B</m> . Matrices are similar if and only if they share the same invariant factors. As minimum polynomial is an invariant factor and the characteristic polynomial is a product of the invariant factors, we see that <m>A</m> and <m>B</m> must share the same invariant factors.
                    </p>
      
                    <p>
                      Next suppose that <m>A</m> and <m>B</m> share the same characteristic polynomial and the same minimal polynomial. As <m>A</m> and <m>B</m> are <m>3\times 3</m> matrices, the characteristic polynomial of both <m>A</m> and <m>B</m> must be a degree <m>3</m> polynomial. We proceed via cases based on the degree of <m>\mp_{A,B}(x)</m>. - First, suppose <m>\deg\mp_{A,B}(x)=3</m>. Then <m>\mp_{A,B}(x)=\cp_{A,B}(x)</m>, making <m>\cp_{A,B}(x)</m> the only invariant factor of both <m>A</m> and <m>B</m>. Thus <m>A</m> and <m>B</m> have the same invariant factors and are therefore similar. - Next, suppose <m>\deg\mp_{A,B}(x)=2</m>. As <m>\mp|\cp</m> and the degrees of all invariant factors must sum to the <m>\deg\cp</m>, we know that <m>\cp=\mp\cdot k(x)</m>, where <m>k(x)</m> is a degree <m>1</m> polynomial, which we denote <m>f(x)</m> for <m>A</m> and <m>g(x)</m> for <m>B</m>.. Since <m>A</m> and <m>B</m> share the same minimum and characteristic polynomials, we see <m>\mp\cdot f=\cp</m> and <m>\mp\cdot g=\cp</m>, and thus that <m>f=g</m>. Hence <m>A</m> and <m>B</m> share the same invariant factors, making <m>A\sim B</m>. - Finally, suppose <m>\deg\mp=1</m>. The minimum polynomial is the largest invariant factor, and thus the invariant factors of <m>A</m> and <m>B</m> must be <m>\{\mp,\mp\mp\}</m>, making them similar.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-b-31">
                    <title>Part (b)</title>
      
                    <p>
                      If we replaced <m>3\times 3</m> with <m>4\times 4</m> then this would allow for <m>\cp_{A,B}(x)=x^4</m> and <m>\mp_{A,B}(x)=x^2</m>, allowing two sets of invariant factors: <m>\{x^2,x^2\}</m> <m>\{x,x,x^2\}</m>, Notice that <m>C(x)=\begin{bmatrix}0\end{bmatrix}</m> and <m>C(x^2)=\begin{bmatrix}0&amp;0\\1&amp;0\end{bmatrix}</m>. Set <m>A=C(x^2)\oplus C(x^2)</m> and <m>B=C(x)\oplus C(x)\oplus C(x^2)</m>, so <me>A=\begin{bmatrix}0 &amp; 0 &amp; 0 &amp; 0 \\1 &amp; 0 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 1 &amp; 0\end{bmatrix} \text{ and } B=\begin{bmatrix}0 &amp; 0 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 1 &amp; 0\end{bmatrix}.</me> These matrices have the same <m>\cp</m> and <m>\mp</m> but are not similar.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-c-5">
                    <title>Part (c)</title>
      
                    <p>
                      Let <m>F=\Q</m>. We define <m>A=C(x-1)\oplus C(x^2-1)</m> and <m>B=C(x+1)\oplus C(x^2-1)</m>. Notice that these matrices are in RCF. However, the invariant factors of <m>A</m> are <m>\{(x+1), (x^2-1)\}</m> and the invariant factors of <m>B</m> are <m>\{(x-1)(x^2-1)\}</m>. Thus <m>A</m> is not similar to <m>B</m>, but the minimal polynomial of both is <m>x^2-1</m>. #### Problem 6 – Unipotent 4 x 4 Matrices Let <m>F</m> be a field and <m>n</m> a positive integer. We say an <m>n\times n</m> matrix <m>A</m> with entries in <m>F</m> is <em>unipotent</em> if <m>A-In</m> is nilpotent (i.e., <m>(A-In)^k = 0</m> for some <m>k \geq 1</m>). For the field <m>F = \Q</m>, find (with complete justification) the number of similarity classes of <m>4\times 4</m> unipotent matrices and give an explicit representative for each class.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="proof.-136">
                    <title><em>Proof</em>.</title>
      
                    <p>
                      Let <m>F=\Q</m>, <m>n</m> a positive integer, and <m>A</m> a unipotent <m>4\times 4</m> matrix with entries in <m>F</m>. Thus <m>A-I</m> is nilpotent. Let <m>\l</m> be an eigenvalue of <m>A-I</m>. Then <m>(A-I)v=\l v</m>, so <m>Av-Iv=\l v</m> and <m>Av=\l v+Iv</m>. As <m>Iv=v</m>, we have <m>Av=\l v+v</m> and <m>Av=(\l+1)v</m>.
                    </p>
      
                    <p>
                      Notice that as <m>\l</m> is an eigenvalue of <m>I</m>, we have <m>\l +1</m> as an eigenvalue of <m>A</m>.
                    </p>
      
                    <p>
                      Assume inductively that <m>\l^{n-1}</m> is an eigenvalue of <m>(A-I)^{n-1}</m>. Notice <me>(A+I)^nv=(A+I)(A+I)^{n-1}v=(A+I)\l^{n-1}v=\l^{n-1}(A+I)v=\l^{n-1}\l=\l^n,</me>making <m>\l</m> an eigenvalue of <m>(A+I)^n</m>. Thus if <m>\l</m> is an eigenvalue of <m>A-I</m>, it is an eigenvalue of <m>(A-I)^n</m> as well. As <m>(A-I)</m> is nilpotent, there exists some <m>k</m> such that <m>(A-I)^k=0</m>. This means that <m>\l^kv=0</m>. As <m>v\neq 0</m> and <m>\l^k</m> is a scalar in a field (and hence integral domain) we have <m>\l=0</m>. Thus the only eigenvalue of <m>(A-I)</m> is <m>0</m>, meaning that the only eigenvalue of <m>A</m> is <m>1</m>.
                    </p>
      
                    <p>
                      Eigenvalues of <m>A</m> correspond to the roots of <m>\cp_A(x)</m>, which is a monic quartic polynomial, as <m>A</m> is a <m>4\times 4</m> matrix. Thus <m>\cp_A(x)=(x-1)^4</m>, as all roots must be <m>1</m>.
                    </p>
      
                    <p>
                      Two matrices are similar if and only if they share the same invariant factors. Given that invariant factors divide <m>\cp_A(x)</m> and each invariant factor must divide the following one, the possible sets of invariant factors for <m>A</m> are the following: - <m>\{(x-1)^4\}</m>, - <m>\{(x-1),(x-1)^3\}</m>, - <m>\{(x-1), (x-1), (x-1)^2\}</m>, - <m>\{(x-1)^2, (x-1)^2\}</m>, and - <m>\{(x-1), (x-1), (x-1), (x-1)\}</m>. We identify the companion matrices for each possible invariant factor: <me>C(x-1)=\begin{bmatrix} 1 \end{bmatrix},</me><me>C((x-1)^2)=C(x^2-2x+1)=\begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 2\end{bmatrix},</me> <me>C((x-1)^3)=C(x^3+3x^2+3x-1)=\begin{bmatrix} 0 &amp; 0 &amp; 1 \\ 1 &amp; 0 &amp; -3 \\ 0 &amp; 1 &amp; -3\end{bmatrix},</me> and <me>C((x-1)^4)=C(x^4-4x^3+6x^2-4x+1)=\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; 0 &amp; 4 \\ 0 &amp; 1 &amp; 0 &amp; -6 \\ 0 &amp; 0 &amp; 1 &amp; 4\end{bmatrix}.</me> We define the following: 1. <m>B=C((x-1)^4)</m>, 2. <m>C=C(x-1)\oplus C((x-1)^3)</m>, 3. <m>D=C(x-1)\oplus C(x-1)\oplus C((x-1)^2)</m>, 4. <m>E=C((x-1)^2)\oplus C((x-1)^2)</m>, and 5. <m>F=C(x-1)\oplus C(x-1)\oplus C(x-1)\oplus C(x-1)</m>. As each of these matrices is in RCF, they are explicit representatives for each similarity class.
                    </p>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-45">
                <title>Problem</title>
      
                <p>
                  Let <m>A</m> and <m>B</m> be <m>n \times n</m> matrices with entries in <m>\Q</m>. Prove <m>A</m> and <m>B</m> are similar in <m>\Mat_{n \times n}(\Q)</m> if and only if <m>A</m> and <m>B</m> are similar in <m>\Mat_{n \times n}(\R)</m>. (That is, show there is a <m>P \in GL_n(\Q)</m> such that <m>A = PBP^{-1}</m> if and only if there is a <m>R \in GL_n(\R)</m> such that <m>A = RBR^{-1}</m>.) {}: Use the Theorem on Rational Canonical Forms.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-46">
                <title>Problem</title>
      
                <p>
                  Similarity of two-by-two matrices: 1. Let <m>F</m> be any field and <m>A, B \in \Mat_{2 \times 2}(F)</m>, and assume that neither <m>A</m> nor <m>B</m> is a scalar matrix. (Recall that a scalar matrix is one of the form <m>c I_n</m> for some <m>c \in F</m>.) Prove <m>A</m> and <m>B</m> are similar if and only if they have the same determinant and the same trace. 2. Let <m>F</m> be a finite field with <m>q</m> elements. Find, with justification, the number of similarity classes of <m>2 \times 2</m> matrices with entries in <m>F</m>. 3. Let <m>F</m> be a finite field with <m>q</m> elements. Find, with justification, the number of conjugacy classes of the group <m>GL_2(F)</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-47">
                <title>Problem</title>
      
                <p>
                  Let <m>F</m> be any field. Up to similarity, how many matrices in <m>\Mat_{5 \times 5}(F)</m> of the form <me>A = \begin{bmatrix}
      1 &amp; * &amp; * &amp; * &amp; * \\
      0 &amp; 1 &amp; * &amp; * &amp; * \\
      0 &amp; 0 &amp; 1 &amp; * &amp; * \\
      0 &amp; 0 &amp; 0 &amp; 1 &amp; * \\
      0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
      \end{bmatrix}
      </me> are there? Justify.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-48">
                <title>Problem</title>
      
                <p>
                  Let <m>F</m> be a field, <m>V</m> a finite dimensional <m>F</m>-vector space, and <m>g: V \to V</m> an <m>F</m>-linear operator. Prove that the following are equivalent for an element <m>\lambda \in F</m>: 1. <m>\lambda</m> is an eigenvalue of <m>g</m>. 2. <m>\lambda</m> is a root of the minimum polynomial <m>\mp_g(x)</m> of <m>g</m>. 3. <m>\lambda</m> is a root of the characteristic polynomial <m>\cp_g(x)</m> of <m>g</m>. (Recall that <m>\lambda \in F</m> is a {} of <m>F</m> provided <m>g(v) = \lambda \cdot v</m> for some non-zero vector <m>v \in V</m>.)
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-7-unfinished-3">
                <title>Problem 7 #unfinished</title>
      
                <p>
                  Suppose <m>F</m> is any field. Recall that a square matrix <m>A</m> with entries in <m>F</m> is nilpotent if<!-- linebreak --><m>A^j = 0</m> for some positive integer j.
                </p>
      
                <p><ol>
                  <li>
                  Prove that if <m>A\in Mat_{n\times n}(F)</m> and <m>A</m> is nilpotent, then <m>A^n = 0</m>.
                  </li>
      
                  <li>
                  Find, with justification, the number of similarity classes of <m>5 \times 5</m> nilpotent matrices with entries in <m>F</m>. #### Problem 9 #unfinished Find, with justification, a complete and non-redundant list of conjugacy class representatives for the group <m>\GL_2(\F_3)</m>, where <m>\F_3</m> is the field with three elements. ## JCF #### <m>\exe</m> – Companion Matrix and Jordan Blocks Let us consider the companion matrix of <m>(x-2)^3 = x^3 -6x^2 + 12x -8</m>: <me>
      A =
      \begin{bmatrix}
      0 &amp; 0 &amp; 8 \\
      1 &amp; 0 &amp; -12 \\
      0 &amp; 1 &amp; 6 
      \end{bmatrix}=C((x-2)^3)\in \M_{3 \times 3}(\Q).
      </me> We can interpret this matrix as arising from the linear transformation <m>g</m> on <me>
      V := \Q[x]/(x-2)^3
      </me> defined as multiplication by <m>x</m>. Recall that the ordered basis of <m>V</m> that gives the matrix <m>A</m> is the “obvious one’’: <me>
      B=\{\ov{1}, \ov{x}, \ov{x^2}\}.
      </me> But notice that <me>
      B'=\{\ov{1}, \ov{x-2}, \ov{(x-2)^2}\}
      </me> is also a basis of <m>V</m>. Let us calculate what the operator <m>g</m> does to this alternative basis. We could work this out by brute force, but a cleaner way is to first compute what the operator <m>g -2 \cdot \id_V</m> does. Since <m>g - 2 \cdot \id_V</m> is multiplication by <m>x-2</m>, it sends each basis element to the next one, except for the last one, which is sent to <m>0</m>. It follows that the matrix of this operator relative to the ordered basis <m>B'</m> is <me>
      [g - 2 \cdot \id_V]_{B'}^{B'} = 
      \begin{bmatrix}
      0 &amp; 0 &amp; 0 \\
      1 &amp; 0 &amp; 0 \\
      0 &amp; 1 &amp; 0 \\
      \end{bmatrix}
      </me> and hence the matrix for <m>g</m> itself for this basis is <me>
      [g]_{B'}^{B'} = 
      \begin{bmatrix}
      2 &amp; 0 &amp; 0 \\
      1 &amp; 2 &amp; 0 \\
      0 &amp; 1 &amp; 2 \\
      \end{bmatrix} =: J_3(2).
      </me> This is what’s known as a Jordan Block.
                  </li>
      
                </ol></p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-61">
                <title><m>\rem</m></title>
      
                <p>
                  In this example, if we used the basis <m>\{ \ov{(x-2)^2}, \ov{x-2}, \ov{1}\}</m> instead we would have gotten the transpose of <m>J_3(2)</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-jordan-block">
                <title><m>\defn</m> – Jordan Block</title>
      
                <p>
                  Given a field <m>F</m>, and integer <m>n \geq 1</m>, and an element <m>r\in F</m>, the <em><m>\defnn{Jordan block}</m></em> <m>J_n(r)</m> is the <m>n \times n</m> with entries in <m>F</m> such that its diagonal entries are all <m>r</m>, each entry just below the diagonal is a <m>1</m>, and all other entries are <m>0</m>: <me>
      J_n(r) = 
      \begin{bmatrix}
        r &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
        1 &amp; r &amp; 0 &amp; \cdots &amp; 0 \\
        0 &amp; \ddots &amp; \ddots &amp;&amp; \vdots \\
        \vdots &amp;&amp; \ddots &amp; \ddots &amp; 0\\
        0 &amp; \cdots &amp; 0 &amp; 1 &amp; r \\
        \end{bmatrix}.
      </me> (More precisely, <m>a_{i,i} = r</m> for all <m>1 \leq i \leq n</m>, <m>a_{i,i-1} = 1</m> for all <m>2 \leq i \leq n</m>, and <m>a_{i,j} = 0</m> for all other <m>i,j</m>.)
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-62">
                <title><m>\rem</m></title>
      
                <p>
                  Some people (including I think the authors or our text) defined a Jordan block to be the transpose of what I have defined it to be.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="thm-jordan-canonical-form">
                <title><m>\thm</m> – Jordan Canonical Form</title>
      
                <p>
                  Let <m>F</m> be a field, let <m>V</m> be a finite dimensional <m>F</m>-vector space, and let <m>g: V \to V</m> be a linear transformation satisfying the property that the characteristic polynomial <m>\cp_g(x)</m> of <m>g</m> factors completely in <m>F[x]</m>[^1] into linear factors. Then there is an ordered basis <m>B</m> for <m>V</m> such that[^3] <me>[g]_B^B = J_{e_1}(r_1) \oplus \dots \oplus J_{e_s}(r_s)=
      \begin{bmatrix}
      J_{e_1}(r_1) &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
      0 &amp;  J_{e_2}(r_2)  &amp; 0 &amp; \cdots &amp; 0 \\
      \vdots &amp; \vdots &amp; \ddots &amp;  &amp; \vdots \\
      0 &amp; 0 &amp; \cdots &amp; 0 &amp;  J_{e_m}(r_m) \\
      \end{bmatrix},
      </me> where <m>m</m>, the <m>e_i</m>’s, and the <m>r_i</m>’s are such that <m>(x - r_1)^{e_1}, \ldots, (x - r_m)^{e^m}</m> are the elementary divisors of the <m>F[x]</m>-module <m>V_g</m>[^2]. Moreover, this matrix is unique up to ordering of the Jordan Blocks, and it is known as “the’’ <em>Jordan Canonical Form</em> of <m>g</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-137">
                  <title><em>Proof.</em></title>
      
                  <p>
                    The proof is similar to the proof the RCF theorem, using the idea of Example  above, but starting with the FTFGMPIDEDF (instead of the FTFGMPIDIFF). Here are the details:
                  </p>
      
                  <p>
                    We consider the <m>F[x]</m>-module <m>V_g</m>. Since we assume <m>\cp_g(x)</m> factors completely, the only irreducible polynomials in its factorization are linear. Thus the invariant factors of <m>V_g</m> are products of polynomials of the form <m>(x-r)^e</m> for various <m>r \in F</m> and integers <m>e \geq 1</m>. It follows that the elementary divisors have this form too. The FTFGMPIDEDF therefore gives an isomorphism of <m>F[x]</m>-modules <me> V_g\cong \frac{F[x]}{(x - r_1)^{e_1}} \oplus \frac{F[x]}{(x -r_2)^{e_2}} \oplus \cdots \oplus\frac{F[x]}{(x - r_s)^{e_s}}. </me>Now pick ordered bases <m>B_i=\{\ov{1}, \ov{x-r_i}, \ldots, \ov{(x-r_i)^{e_i-1}}\}</m> for each of the summands and set <m>B</m> to be their “ordered union’’ just as we did for the proof of the Theorem on RCF. By the same argument as in Example  applied to each summand individually, the matrix representing multiplication by <m>x</m> on each summand is <m>J_{e_i}(r_i)</m>. This gives the existence of the JCF.
                  </p>
      
                  <p>
                    The uniqueness follows from the uniqueness clause in the FTFGMPIDEDF.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="rem-63">
                <title><m>\rem</m></title>
      
                <p>
                  Not every operator has a Jordan Canonical Form: The Theorem only applies if <m>\cp_g(x)</m> factors completely, and, conversely, if an operator <m>g</m> is represented by any lower-triangular matrix, then its characteristic polynomial must be a product of linear polynomials. For algebraically closed fields, such as <m>\C</m>, every linear operator does indeed have a JCF.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="rem-64">
                <title><m>\rem</m></title>
      
                <p>
                  If we flip the order of each <m>B_i</m> in the proof, we would end up with the transpose of what I have defined the JCF to be. This is what our text does, and it defines the JCF as the transpose of what I have defined it to be.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="cor-jcf-exists-if-charpoly-factors-linearly">
                <title><m>\cor</m> – JCF Exists if CharPoly Factors Linearly</title>
      
                <p>
                  If <m>A</m> is an <m>n \times n</m> matrix with entries in a field <m>F</m> and <m>\cp_A(x)</m>[^1] factors completely into linear factors, then <m>A</m> is similar to a matrix in Jordan Canonical Form, and this matrix is unique up to the ordering of the Jordan Blocks.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="exe-finding-jcf">
                <title><m>\exe</m> – Finding JCF</title>
      
                <p>
                  Let us find the Jordan Canonical Form of <me>
      A = 
      \begin{bmatrix} 
      1 &amp; -1 &amp; 1 \\
      0 &amp; 0 &amp; 1 \\
      0 &amp; 1 &amp; 0 \\
      \end{bmatrix}
      \in \Mat_{3 \times 3}(\Q).
      </me>
                </p>
      
                <p>
                  We found the Rational Canonical Form of this matrix before. In the process we showed that we have an isomorphism of <m>\Q[x]</m>-module. <me>
      \Q^3_A \cong \Q[x]/(x-1) \oplus \Q[x]/(x^2-1).
      </me> By the Sunzi Remainder Theorem <me>
      \Q[x]/(x-1) \oplus \Q[x]/(x^2-1) \cong \Q[x]/(x-1) \oplus \Q[x]/(x-1) \oplus \Q[x]/(x+1)
      </me> and thus the elementary divisors of <m>\Q^3_A</m> are <m>x-1, x-1, x+1</m>. By the Theorem this shows that the JCF of <m>A</m> is <me>
      J_1(1) \oplus J_1(1) \oplus J_1(-1) = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; -1 \end{bmatrix}.</me>
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="defn-diagonalizable">
                <title><m>\defn</m> – Diagonalizable</title>
      
                <p>
                  Let <m>V</m> be a finite dimensional vector space over a field <m>F</m> and let <m>g: V \to V</m> be an <m>F</m>-linear operator. We say <m>g</m> is <em><m>\defnn{diagonalizable}</m></em> if there is a basis <m>B</m> for <m>V</m> such that the matrix <m>[g]_B^B</m>[^1] is a diagonal matrix.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-7-diagonalizable-if-minpoly-splits">
                <title>Problem 7 – Diagonalizable if MinPoly Splits</title>
      
                <p>
                  Let <m>V</m> be a finite dimensional vector space over a field <m>F</m> and let <m>\t : V \to V</m> be an <m>F</m>-linear operator on <m>V</m>. Prove <m>\t</m> is diagonalizable over <m>F</m> if and only if its minimum polynomial <m>p(x)</m> factors into distinct linear terms in <m>F[x]</m>[^1].
                </p>
      
      
                <paragraphs xml:id="proof.-138">
                  <title><em>Proof</em>.</title>
      
                  <p>
                    <m>(\Rightarrow)</m> Suppose that <m>g</m> is diagonalizable. Thus there exists a change of basis matrix <m>A</m> such that <m>A</m> is diagonal. As it is diagonal, its diagonal entries are the eigenvalues of <m>g</m>, and thus the roots of the minimal polynomial of <m>g</m>. Using row and column operations we can rearrange <m>A</m> so that all repeated linear factors are next to each other in the diagonal, for convenience.
                  </p>
      
                  <p>
                    We know that <m>\mp_A(x)</m> is the smallest monic polynomial that sends <m>A</m> to 0. Take all the distinct eigenvalues <m>\lambda_i</m> and consider <m>q(x)=\prod(x-\lambda_i)</m>.
                  </p>
      
                  <p>
                    We examine <m>q(A)</m>. It will be a product of matrices, one for each <m>\lambda_i</m>. First, take the first matrix in this product, <m>(A-\lambda_1I)</m>, and note that it sends all <m>\lambda_1</m> in <m>A</m> to 0. Thus all of the rows and columns that contained a <m>\lambda_1</m> are now 0, and thus all these rows and columns will be 0 in the final product <m>q(A)</m>. As this is we set <m>q(x)=\prod(x-\lambda_i)</m> for all <m>l\lambda_i\in A</m>, we see that for each row and column in <m>A</m> there will exist a matrix <m>A-\lambda_iI</m> in the product <m>q(A)</m> such that the row and column will be 0. Thus the entire matrix will be 0, and <m>q(A)=0</m>.
                  </p>
      
                  <p>
                    Note that if any <m>\lambda_i</m> were excluded from <m>q(x)</m> there would exist a non-zero row and column for every matrix in the product, and thus <m>q</m> would not send <m>A</m> to 0. Thus <m>q(x)</m> is indeed the minimal polynomial of <m>A</m>. As <m>q(x)=\prod(x-\lambda_i)</m>, we see it does indeed factor into distinct linear terms.
                  </p>
      
                  <p>
                    <m>(\Leftarrow)</m> Suppose the minimum polynomial of <m>g</m> factors completely into distinct linear factors, each of which has the form <m>x-\lambda_i</m> for some <m>\lambda_i\in F</m>. As each <m>\lambda_i</m> is distinct, each elementary divisor is of the form <m>(x-\lambda_i)^1</m>.
                  </p>
      
                  <p>
                    We construct the Jordan Canonical Form of <m>g</m>. As the elementary divisors are linear the Jordan blocks are <m>1\times1</m> matrices, making the <m>JCF</m> a diagonal matrix. As the JCF is itself a change of basis matrix, we see that <m>g</m> is diagonalizable.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-6-derivative-linear-operator">
                <title>Problem 6 – Derivative Linear Operator</title>
      
                <p>
                  Consider the <m>\C</m>-vector space <m>V = {p \in \C[x] : \deg(p) \leq n - 1}</m>. (You may assume without proof that <m>V</m> is n-dimensional.) Consider the following linear maps <m>V \to V</m>:<!-- linebreak --><me>D : V \to V \hspace{4cm} T : V \to V</me> <me>p\mapsto p'\hspace{4cm} p\mapsto xp'</me> (where p′ denotes the derivative of p). Determine the JCF of (a) <m>D</m> (b) <m>B</m>
                </p>
      
      
                <paragraphs xml:id="proof.-139">
                  <title><em>Proof</em>.</title>
      
      
                  <paragraphs xml:id="part-a-35">
                    <title>Part (a)</title>
      
                    <p>
                      For any integer <m>n \geq 1</m>, consider the <m>\C</m>-vector space <m>V = \{p \in \C[x] : \deg(p) \leq n - 1\}</m>.
                    </p>
      
                    <p>
                      Let <m>T:V \to V</m> be the linear operator given by <m>T(p) = xp'</m> (where <m>p'</m> denotes the derivative of <m>p</m>). Note that the change of basis matrix for this operator is given by <me>
      \begin{bmatrix}
        0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
        0 &amp; 1 &amp; \cdots &amp; 0 &amp; 0 \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
        0 &amp; 0 &amp; \cdots &amp; 0 &amp; n-1 \\   
      \end{bmatrix},
      </me> with the basis <m>\{1,x,\dots,x^{n-1}\}</m>. Thus <m>\cp_A(x)</m>[^1] will be given by the determinant of the matrix
                    </p>
      
                    <p>
                      <me>
      \begin{bmatrix}
        x &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
        0 &amp; x-1 &amp; \cdots &amp; 0 &amp; 0 \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
        0 &amp; 0 &amp; \cdots &amp; 0 &amp; x-(n-1) \\   
      \end{bmatrix},
      </me>
                    </p>
      
                    <p>
                      which is diagonal. Hence <me>\cp_AT(x)=\displaystyle\prod_{i=0}^{n-1}(x-i).</me>Thus <m>\cp_AT(x)</m> factors into distinct linear polynomials, each of which is in the form <m>(x-i)</m> for <m>0\leq i\leq n-1</m>. Thus each linear term is an elementary divisor, making each Jordan Block a <m>1\times1</m> matrix with <m>i</m> as the only entry. Thus the Jordan Canonical form is <me>\begin{bmatrix}
        0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
        0 &amp; 1 &amp; \cdots &amp; 0 &amp; 0 \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
        0 &amp; 0 &amp; \cdots &amp; 0 &amp; n-1 \\   
      \end{bmatrix}.
      </me>
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-b-32">
                    <title>Part (b)</title>
      
                    <p>
                      This time around the change of basis matrix (denoted <m>B</m> and using the same basis as above) for this matrix has 0s along the diagonal, and increasing natural numbers (starting at 0, sorry) along the upper diagonal. Thus <m>\cp_B(x)=x^n</m>.
                    </p>
      
                    <p>
                      Recall that the minimum polynomial corresponding to <m>B</m> will be the the smallest monic polynomial such that <m>B</m> is sent to 0. Note that as <m>D</m> is the operator sending <m>p</m> to its derivative, and that <m>D^{(2)}=D(D(p))=p''=p^{(2)}</m>. Thus <m>B^2</m> can be viewed as a change of basis matrix for taking the second derivative of the basis, and so on.
                    </p>
      
                    <p>
                      As the basis extends to <m>x^{n-1}</m>, it requires <m>n</m> derivatives to make this polynomial become 0. Thus the minimal polynomial of <m>B</m> must be <m>x^n</m>, as it it monic and <m>B^n=0</m>. As the degree of the invariant factors must sum to <m>n</m> and <m>\mp_B(x)=x^n</m>, which is itself an invariant factor, we see that it must in fact be the only one.
                    </p>
      
                    <p>
                      As <m>x^n</m> is already a power of a prime, it is the only elementary divisor as well. Thus the Jordan Canonical Form for <m>B</m> is an <m>n\times n</m> Jordan Block with 0s along the diagonal and 1s along the sub-diagonal.
                    </p>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-6-jcf-and-rcf-of-derivative-operator">
                <title>Problem 6 – JCF and RCF of Derivative Operator</title>
      
                <p>
                  Let <m>n</m> be a positive integer. Consider the real vector space <m>V = \{p \in R[x] | \deg(p) \leq n\}</m> and the linear transformation <m>T : V \to V, T (p) = p'</m>, where <m>p'(x)</m> is the derivative of p(x).
                </p>
      
                <p><ol>
                  <li>
                  Find the characteristic polynomial and the minimum polynomial for <m>T</m>.
                  </li>
      
                  <li>
                  Find the invariant factors and the elementary divisors for <m>T</m>.
                  </li>
      
                  <li>
                  Find the RCF and the Jordan Canonical Form for <m>T</m>.
                  </li>
      
                </ol></p>
      
      
                <paragraphs xml:id="proof.-140">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Let <m>T:V \to V</m> be the linear operator given by <m>T(p) = xp'</m> (where <m>p'</m> denotes the derivative of <m>p).</m>
                  </p>
      
      
                  <paragraphs xml:id="part-a-36">
                    <title>Part (a)</title>
      
                    <p>
                      Note that the change of basis matrix for this operator is given by <me>
      \begin{bmatrix}
        0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
        0 &amp; 1 &amp; \cdots &amp; 0 &amp; 0 \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
        0 &amp; 0 &amp; \cdots &amp; 0 &amp; n-1 \\   
      \end{bmatrix},
      </me> with the basis <m>\{1,x,\dots,x^{n-1}\}</m>. Thus <m>\cp_A(x)</m> will be given by the determinant of the matrix <me>\begin{bmatrix}
        x &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
        0 &amp; x-1 &amp; \cdots &amp; 0 &amp; 0 \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
        0 &amp; 0 &amp; \cdots &amp; 0 &amp; x-(n-1) \\   
      \end{bmatrix},</me> which is diagonal. Hence <m>\cp_AT(x)=\displaystyle\prod_{i=0}^{n-1}(x-i)</m>.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-b-33">
                    <title>Part (b)</title>
      
                    <p>
                      Our <m>\cp_AT(x)</m> factors into distinct linear polynomials, each of which is in the form <m>(x-i)^1</m> for <m>0\leq i\leq n-1</m>. Thus each linear term is an elementary divisor. However, as none of these elementary divisors divide any of the others, we see that the only invariant factor is <m>\cp_AT(x)</m> itself.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-c-6">
                    <title>Part (c)</title>
      
                    <p>
                      As each linear term is an elementary divisor, each Jordan block a <m>1\times1</m> matrix with <m>i</m> as the only entry. Thus the Jordan Canonical form is <me>\begin{bmatrix}
        0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
        0 &amp; 1 &amp; \cdots &amp; 0 &amp; 0 \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
        0 &amp; 0 &amp; \cdots &amp; 0 &amp; n-1 \\   
      \end{bmatrix}.</me> As the only invariant factor is <m>\cp_AT(x)</m>, we see that the Rational Canonical Form of <m>T</m> is <m>C(\cp_AT(x))</m>.
                    </p>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-4-jcf-and-similarity">
                <title>Problem 4 – JCF and Similarity</title>
      
                <p>
                  Let <m>A</m> be the matrix with entries in <m>\C</m> <me>A=\begin{bmatrix}
      3 &amp; 0 &amp; 0 &amp; 0\\
      1 &amp; 3 &amp; 0 &amp; 0\\
      1 &amp; 0 &amp; 3 &amp; 0\\
      0 &amp; 2 &amp; 1 &amp; 3\\
      \end{bmatrix}</me> (a) Find the Jordan Canonical Form of A. (b) Is <m>A</m> similar to <me>\begin{bmatrix}
      0 &amp; -9 &amp; 0 &amp; 0\\
      1 &amp; 6 &amp; 0 &amp; 0\\
      0 &amp; 0 &amp; 0 &amp; -9\\
      0 &amp; 0 &amp; 1&amp; 6\\
      \end{bmatrix}?</me>
                </p>
      
      
                <paragraphs xml:id="proof.-141">
                  <title><em>Proof.</em></title>
      
      
                  <paragraphs xml:id="part-a-37">
                    <title>Part (a)</title>
      
                    <p>
                      First, notice that <m>A</m> is upwards triangular, and thus <m>\cp_A(x)=(x-3)^4</m>. By the Cayley Hamilton Theorem the minimum polynomial divides the characteristic polynomial, and from the definition of minimum polynomial we know <m>\mp_A(x)</m> is the smallest polynomial such that <m>\mp_A(A) = 0</m>. Since <m>\mp_A(x)</m> must be a power of <m>(x-3)^n</m> with <m>n\leq 4</m>, we plug in values of <m>n</m> until we get <m>0</m>. <me>(A-3I)^1=\begin{bmatrix}
      0 &amp; 0 &amp; 0 &amp; 0\\
      1 &amp; 0 &amp; 0 &amp; 0\\
      1 &amp; 0 &amp; 0 &amp; 0\\
      0 &amp; 2 &amp; 1 &amp; 0\\
      \end{bmatrix}</me> Shucks. Moving on, <me>(A-3I)^2=\begin{bmatrix}
      0 &amp; 0 &amp; 0 &amp; 0\\
      1 &amp; 0 &amp; 0 &amp; 0\\
      1 &amp; 0 &amp; 0 &amp; 0\\
      0 &amp; 2 &amp; 1 &amp; 0\\
      \end{bmatrix}\cdot\begin{bmatrix}
      0 &amp; 0 &amp; 0 &amp; 0\\
      1 &amp; 0 &amp; 0 &amp; 0\\
      1 &amp; 0 &amp; 0 &amp; 0\\
      0 &amp; 2 &amp; 1 &amp; 0\\
      \end{bmatrix}=\begin{bmatrix}
      0 &amp; 0 &amp; 0 &amp; 0\\
      0 &amp; 0 &amp; 0 &amp; 0\\
      0 &amp; 0 &amp; 0 &amp; 0\\
      3 &amp; 0 &amp; 0 &amp; 0\\
      \end{bmatrix},</me>which is also not <m>0</m>. However, multiplying one more time we see <m>\mp_A(x)=(x-3)^3</m>. By part (2) of this theorem <m>(x-3)^3</m> is an invariant factor. By part (1) of that same theorem, invariant factors must multiply to <m>\cp_A(x)</m>, and so the invariant factors are <m>x-3</m> and <m>(x-3)^3</m>. These are also the elementary divisors. So
                    </p>
      
                    <p>
                      <me>J_1(3)\oplus J_3(3)=\begin{bmatrix}
      3 &amp; 0 &amp; 0 &amp; 0\\
      0 &amp; 3 &amp; 0 &amp; 0\\
      0 &amp; 1 &amp; 3 &amp; 0\\
      0 &amp; 0 &amp; 1 &amp; 3\\
      \end{bmatrix}.</me>
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-b-34">
                    <title>Part (b)</title>
      
                    <p>
                      Let <me>B=\begin{bmatrix}
      0 &amp; -9 &amp; 0 &amp; 0\\
      1 &amp; 6 &amp; 0 &amp; 0\\
      0 &amp; 0 &amp; 0 &amp; -9\\
      0 &amp; 0 &amp; 1&amp; 6\\
      \end{bmatrix}</me> And notice that <m>(B-3I)^2=0</m>. Thus <m>(x-3)^3</m> cannot be the minimal polynomial of <m>B</m>. Two matrices are only similar if they share the same invariant factors (and thus the same minimum polynomial), so <m>A</m> and <m>B</m> are not similar.
                    </p>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-6-rcf-and-determining-existence-of-jcf">
                <title>Problem 6 – RCF and Determining Existence of JCF</title>
      
                <p>
                  For the matrix <me>A=\begin{bmatrix} 0 &amp; -1 &amp; 2 \\ 1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; -2\end{bmatrix}</me>in <m>\Mat_{3\times3}(\R)</m>: (a) Find the RCF of <m>A</m>. (b) Determine whether or not <m>A</m> has a Jordan Canonical Form, and if so, find this form.
                </p>
      
      
                <paragraphs xml:id="proof.-142">
                  <title><em>Proof</em>.</title>
      
      
                  <paragraphs xml:id="part-a-38">
                    <title>Part (a)</title>
      
                    <p>
                      Notice <m>\cp_A(x)=\det(A-xI)</m>. So we have <me>\begin{align}\cp_A(x)&amp;=-x\det\begin{bmatrix} -x &amp; 0 \\ 1 &amp; -2-x\end{bmatrix}-(-1)\det\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -2-x\end{bmatrix}+2\det\begin{bmatrix} 1 &amp; -x \\ 0 &amp; 1\end{bmatrix}\\&amp;=-x(-x(-2-x))+(-2-x)+2(1)\\&amp;=-x^3-2x^2-x\\&amp;-x(x^2+2x+1)\\&amp;=-x(x+1)^2.\end{align}</me>Not so bad! Now, the invariant factors all divide the characteristic polynomial and must divide the following factor, so our options for sets of invariant factors are the following: - <m>\{-x(x+1)^2\}</m> - <m>\{(x+1),-x(x+1)\}</m> However, the largest invariant factor is also the minimal polynomial. So we check to see if <m>-A(A+I)=0</m>. Luckily, the very first calculation shows that this is not the case. Thus the minimum polynomial is the characteristic polynomial is the only invariant factor of <m>A</m>. Thus the RCF of <m>A</m> is <me>C(-x^3-2x^2-x)=\begin{bmatrix} 0 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\0 &amp; 1 &amp; 2\end{bmatrix}.</me>
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-b-35">
                    <title>Part (b)</title>
      
                    <p>
                      Luckily for us, <m>\cp_A(x)</m> factors completely into linear terms! So our elementary divisors are <m>-x</m> and <m>(x+1)^2</m>. We see <m>J_1(0)=\begin{bmatrix} 0 \end{bmatrix}</m> and <m>J_2(-1)=\begin{bmatrix} -1 &amp; 0 \\ 1 &amp; -1\end{bmatrix}</m>, so the Jordan Canonical form of <m>A</m> is <me>J=\begin{bmatrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 0 \\0 &amp; 1 &amp; -1\end{bmatrix}.</me>
                    </p>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-9-transpose-similarity">
                <title>Problem 9 – Transpose Similarity</title>
      
                <p>
                  Let <m>F</m> be a field and <m>A</m> a square matrix with entries from <m>F</m>. Prove that <m>A</m> is similar to its transpose.
                </p>
      
      
                <paragraphs xml:id="proof.-143">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Let <m>L</m> be the algebraic closure of <m>F</m>. Thus <m>A</m> has a Jordan Canonical Form in <m>L</m>. For each Jordan block <m>J_i</m> in the JCF of <m>A</m>, let <m>B_i</m> denote the transpose of the identity matrix, and notice that <m>B_iJ_iB_i\inv=J_i^T</m>. As this is the case for every Jordan block, we see that the JCF of <m>A</m>, <m>J</m>, is similar to its transpose. As the <m>A</m> is similar to <m>J</m>, <m>A^T</m> is similar to <m>J^T</m>, and <m>J</m> is similar to <m>J^T</m>, we see that <m>A\sim A^T</m> in <m>L</m> by transitivity.
                  </p>
      
                  <p>
                    Suppose <m>A</m> and <m>B</m> are similar in <m>\Mat_{n \times n}(L)</m>. As <m>A</m> and <m>B</m> have entries in <m>F</m>, then they are both in <m>\Mat_{n \times n}(F)</m>. Thus there exist matrices <m>C,D\in\Mat_{n \times n}(F)</m> in RCF such that <m>A</m> is similar to <m>C</m> and that <m>B</m> is similar to <m>D</m>. However, <m>A</m> is similar to <m>C</m> and that <m>B</m> is similar to <m>D</m> in <m>\Mat_{n \times n}(L)</m> as well. Notice <m>C</m> and <m>D</m> are still in RCF. However, as the RCF is unique, this means that <m>C=D</m> in <m>\Mat_{n \times n}(L)</m>, making them equal in <m>\Mat_{n \times n}(F)</m> as well. Thus <m>A</m> is similar to <m>B</m>, as similarity is transitive.
                  </p>
      
                  <p>
                    This yields <m>A\sim A^T</m> in <m>F</m>.
                  </p>
      
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-49">
                <title>Problem</title>
      
                <p>
                  Let <m>V = \R^3</m> with the standard basis <m>B=\{e_1,e_2,e_3\}</m> and let <m>t: V \to V</m> be the linear transformation represented by the matrix <me>[t]_B^B =
      \begin{bmatrix}
      0   &amp;   -1  &amp;   0\\
      -1  &amp;   0   &amp;   3\\ 
      0   &amp;   0   &amp;   1   
      \end{bmatrix}.</me> 1. Find the invariant factor decomposition of the <m>\R[x]</m>-module <m>V_t</m>. 2. Find the characteristic and minimal polynomials of <m>t</m>. 3. Find the rational canonical form of <m>t</m>. 4. Find the Jordan canonical form of <m>t</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-8-unfinished-2">
                <title>Problem 8 #unfinished</title>
      
                <p>
                  Consider the following matrix: <me>A=\begin{bmatrix} -1 &amp; 3 &amp; 0\\0 &amp; 2 &amp; 0\\2 &amp; 1 &amp; -1\end{bmatrix}.</me> (a) Find the rational canonical form of <m>A</m>. (b) Find the Jordan canonical form of <m>A</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-6-unfinished">
                <title>Problem 6 #unfinished</title>
      
                <p>
                  Let <m>n</m> be a positive integer and let <m>J = J_{2n}(0)</m> be the Jordan block matrix of size <m>2n \times 2n</m> with eigenvalue 0 in <m>M_{2n\times2n}(\C)</m>.
                </p>
      
                <p><ol>
                  <li>
                  Find the minimal polynomials for <m>J</m> and for <m>J^2</m>, with justification.
                  </li>
      
                  <li>
                  Find the Jordan canonical form of <m>J^2</m>, with justification. <em>Hint</em>: consider the kernel of <m>J^2</m>.
                  </li>
      
                </ol></p>
      
      
                <paragraphs xml:id="proof.-144">
                  <title><em>Proof.</em></title>
      
                  <p>
                    Let <m>n</m> be a positive integer and let <m>J = J_{2n}(0)</m> be the Jordan block matrix of size <m>2n \times 2n</m> with eigenvalue 0 in <m>M_{2n\times2n}(\C)</m>.
                  </p>
      
      
                  <paragraphs xml:id="part-a-39">
                    <title>Part (a)</title>
      
                    <p>
                      Notice that <m>J</m> is a triangular matrix with <m>0</m>’s along the diagonal. Thus <m>\cp_J(x)=x^{2n}</m>, the product of the diagonal entries of the matrix <m>J-xI</m>.
                    </p>
      
                    <p>
                      Squaring a triangular matrix moves everything one down (Proof?), and so <m>x^{2n-1}</m> should do it. Squaring <m>J</m> just moves us one closer, so <m>x^{2n-2}</m>
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-b-36">
                    <title>Part (b)</title>
      
                    <p>
                      As <m>\cp_{J^2}(x)=x^{2n}</m>, the only roots of it are <m>0</m>, and thus these are the only elementary divisors of <m>J^2</m>.
                    </p>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-7-unfinished-4">
                <title>Problem 7 #unfinished</title>
      
                <p>
                  On canonical forms (a) Consider the <m>\Q[x]</m>-module <me>M=\frac{\Q[x]}{(x^4-1)}\oplus\frac{\Q[x]}{x^2(x-1)}</me> and let <m>V</m> the <m>\Q</m>-vector space obtained from <m>M</m> by restriction of scalars along the evident inclusion <m>\Q ⊆ \Q[x]</m> and let <m>t : V → V</m> be the <m>\Q</m>-linear transformation given as multiplication by <m>x</m>. Find, with justification, the rational canonical form of <m>t</m>. (b) Consider the <m>\C[x]</m>-module <me>N=\frac{\C[x]}{(x^4-1)}\oplus\frac{\Q[x]}{x^2(x-1)}</me>and let <m>W</m> the <m>\C</m>-vector space obtained from <m>N</m> by restriction of scalars along <m>\C ⊆ \C[x]</m> and let <m>t : W → W</m> be the <m>\C</m>-linear transformation given as multiplication by <m>x</m>. Find, with justification, the Jordan canonical form of <m>t</m>.
                </p>
      
              </paragraphs>
      
              <paragraphs xml:id="problem-9-unfinished-2">
                <title>Problem 9 #unfinished</title>
      
                <p>
                  Consider the following matrix over <m>\Q</m>: <me>\begin{bmatrix}-1&amp;0&amp;0&amp;0\\0&amp;-1&amp;0&amp;0\\-2&amp;-2&amp;0&amp;1\\-2&amp;0&amp;-1&amp;-2\end{bmatrix}</me> (a) Show that the characteristic and minimal polynomials of <m>A</m> are, respectively, <m>(x + 1)^4</m> and <m>(x + 1)^3</m>. (b) Find the rational canonical form <m>R</m> of <m>A</m> and the Jordan canonical form <m>J</m> of <m>A</m>. (c) Find an invertible matrix <m>P</m> such that <m>P\inv AP = J</m>.
                </p>
      
      
                <paragraphs xml:id="proof.-145">
                  <title><em>Proof</em>.</title>
      
      
                  <paragraphs xml:id="part-a-40">
                    <title>Part (a)</title>
      
                    <p>
                      Let <m>A</m> denote the above matrix.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-b-37">
                    <title>Part (b)</title>
      
                    <p>
                      As <m>\cp_A(x)=(x + 1)^4</m> and <m>\mp_A(x)=(x + 1)^3</m>, we know that our characteristic polynomial factors into linear factors, so <m>A</m> does indeed have a JCF. This also means that <m>A</m> only has one elementary divisor, <m>(x + 1)^4</m>, which corresponds to the Jordan Block <m>J_4(-1)</m>, a matrix with <m>-1</m>s along the diagonal, <m>1</m>s along the subdiagonal, and <m>0</m> everywhere else. This is the JCF of <m>A</m>.
                    </p>
      
                  </paragraphs>
      
                  <paragraphs xml:id="part-c-7">
                    <title>Part (c)</title>
      
                  </paragraphs>
                </paragraphs>
              </paragraphs>
      
              <paragraphs xml:id="problem-5-why-is-this-true-unfinished">
                <title>Problem 5 (why is this true?) #unfinished</title>
      
                <p>
                  Let <m>F</m> be a field and <m>f(x) \in F[x]</m> a monic polynomial of degree <m>n</m>. Prove: all matrices in <m>M_{n\times n}(F)</m> having characteristic polynomial <m>f (x)</m> are similar if and only if the irreducible factorization of <m>f(x)</m> has no repeated factors.
                </p>
              </paragraphs>
      
            </paragraphs>
      
          </subsection>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

        <section xml:id="sec-jcf">
          <title>Jordan Canonical Form</title>

          <p></p>


          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>
        
      </chapter>
    
    </part>

    <part xml:id="part-fields">
      <title>Field Theory</title>

      <xi:include href="./FieldExtensions.ptx" />

      <chapter xml:id="ch-galois">
        <title>Galois Theory</title>

        <section xml:id="sec-separable">
          <title>Separability</title>

          <definition xml:id="def-ring-characteristic">
            <statement>
              <p>
               Let R be a commutative ring. The <em>characteristic</em> of <m>R</m>, written <m>\char(R)</m>, is the unique non-negative generator of the kernel of the unique ring homomorphism <m>\varphi: \Z \to R</m>. (Recall <m>\varphi(n) = n \cdot 1_R</m>.)
              </p>

              <p>
                Equivalently, <m>\char(R)</m> is the smallest positive integer <m>n</m> such that <m>n\cdot 1_R=\underbrace{1_R+\cdots+1_R}_{n}=0_R</m>, if such and integer exists, and <m>\char(R) = 0</m> otherwise.
              </p>
            </statement>
          </definition>

				<paragraphs xml:id="rem-8">
					<title><m>\rem</m></title>

					<p>
						Observe that for any integer <m>n</m> and commutative ring <m>R</m>, we have <m>n = 0</m> in <m>R</m> (i.e., <m>n \cdot 1_R = 0</m>) if and only if <m>\char(R) \mid n</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="exe-charz">
					<title><m>\exe</m> – <m>\char(\Z)</m></title>

					<p>
						<m>\char(\Z)=0</m> and <m>\char(\Z/n)=n</m> for any <m>n \geq 0</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="defn-prime-field">
					<title><m>\defn</m> – Prime Field</title>

					<p>
						For a field <m>F</m> its <em><m>\defnn{prime field}</m></em> is the smallest subfield of <m>F</m>; i.e., it is the intersection of all subfields of <m>F</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="prop-characteristic-and-prime-fields">
					<title><m>\prop</m> – Characteristic and Prime Fields</title>

					<p>
						Let F be a field. 1. The characteristic <m>\char(F)</m> is either <m>0</m> or a prime number <m>p</m>. 2. <m>\char(F) = 0</m> if and only if the [[Mathematics/Definitions/Prime Field|prime]] subfield of <m>F</m> is isomorphic to <m>\Q</m>; <m>\char(F) = p</m> for a prime integer <m>p</m> if and only if the prime subfield of <m>F</m> is isomorphic to <m>\Z/p</m>.
					</p>

        </paragraphs>


					<paragraphs xml:id="proof.-43">
						<title><em>Proof.</em></title>

						<p>
							For the first assertion, consider the unique ring homomorphism <m>\varphi:\Z \to F</m>. Since <m>F</m> is a domain, the kernel of <m>\varphi</m> is a prime ideal (since <m>\Z/\ker(\varphi) \cong \im(\varphi)</m> and <m>\im(\varphi)</m> is a subring of <m>F</m>). The result holds since the only prime ideals of <m>\Z</m> are <m>(0)</m> and <m>(p)</m> for a prime integer <m>p</m>. (Note that this proof shows that, more generally, the characteristic of an integral domain must be either <m>0</m> or a prime.)
						</p>

						<p>
							For the second assertion, observe that the smallest {} of <m>F</m> is the image of the ring map <m>\varphi: \Z \to F</m>, and by the first assertion, this image is isomorphic to either <m>\Z</m> or <m>\Z/p</m>. The latter is already a field and hence it is the prime field of <m>F</m>. In the former case, the prime subfield is isomorphic to the field of fractions of <m>\Z</m>, which is <m>\Q</m>.
						</p>

					</paragraphs>

				<paragraphs xml:id="prop-no-homomorphisms-if-different-characteristics">
					<title><m>\prop</m> – No Homomorphisms if Different Characteristics</title>

					<p>
						If <m>F</m> and <m>E</m> are fields such that <m>\char(F) \ne \char(E)</m>[^1] then there exist no ring homomorphisms from <m>E</m> to <m>F</m> (or vice versa).
					</p>

        </paragraphs>


					<paragraphs xml:id="proof.-44">
						<title><em>Proof.</em></title>

						<p>
							Suppose <m>F</m> and <m>E</m> are fields and <m>\s: F \to E</m> is a ring homomorphism. Let <m>\phi_F: \Z \to F</m> and <m>\phi_E: \Z \to E</m> be the unique ring maps from <m>\Z</m> to <m>F</m> and <m>E</m>. Since <m>\s \circ \phi_F</m> is a ring map from <m>\Z</m> to <m>E</m>, we have <m>\s \circ \phi_F = \phi_E</m> by the uniqueness of <m>\phi_E</m>. Since <m>F</m> is a field and <m>E</m> is not the zero ring, the map <m>\s</m> is injective. Since <m>\s</m> is injective, it follows that <m>\ker(\s \circ \phi_F) = \ker(\phi_F)</m>, and hence we obtain that <m>\ker(\phi_F) = \ker(\phi_E)</m>. By definition of characteristic, we conclude <m>\ker(E) = \ker(F)</m>.
						</p>

					
				</paragraphs>

				<paragraphs xml:id="defn-root-multiplicity">
					<title><m>\defn</m> – Root Multiplicity</title>

					<p>
						For a field <m>F</m> and a polynomial <m>f(x) \in F[x]</m>[^1], let <m>\ov{F}</m> be an [[Mathematics/Definitions/Algebraic Closure|algebraic closure]] of <m>F</m> and <m>\a \in \ov{F}</m> a root of <m>f(x)</m>. The <em><m>\defnn{multiplicity}</m></em> of <m>\a</m> in <m>f</m> is the number of times <m>(x - \a)</m> appears in the factorization <m>f(x) = c \prod_i (x - \a_i)</m> of <m>f</m> in <m>\ov{F}[x]</m>. (This number is independent of choice of algebraic closure by uniqueness of such closures up to isomorphism.[^2])
					</p>

				</paragraphs>

				<paragraphs xml:id="defn-separable-polynomial">
					<title><m>\defn</m> – Separable (Polynomial)</title>

					<p>
						Let <m>F</m> be a field and <m>f(x)</m> a polynomial in <m>F[x]</m>[^1]. If the [[Mathematics/Definitions/Multiplicity|multiplicity]] of every root is <m>1</m>, we say <m>f(x)</m> is a <m>\defnn{separable polynomial}</m>; i.e. <m>f</m> is separable provided it has no repeated roots.
					</p>

				</paragraphs>

				<paragraphs xml:id="defn-derivative">
					<title><m>\defn</m> – Derivative</title>

					<p>
						For any field <m>F</m> and <m>f(x) = a_n x^n + \dots + a_1 x + a_0 \in F[x]</m>[^1], define its <em><m>\defnn{derivative}</m></em> to be <me>f'(x) = n a_n x^{n-1} + (n-1) a_{n-1} x^{n-2} + \cdots + 2 a_2 x + a_1.</me>
					</p>

				</paragraphs>

				<paragraphs xml:id="exe-derivatives-in-characteristic-p">
					<title><m>\exe</m> – Derivatives in Characteristic <m>p</m></title>

					<p>
						If <m>\char(F) = p</m> and <m>f(x) = x^p +c</m> for <m>c \in F</m>, then <m>f'(x) = 0</m>. So beware that non-constant polynomials can have <m>0</m> derivatives! Observe, however, that this cannot occur in characteristic <m>0</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="lem-gcd-and-field-extensions">
					<title><m>\lem</m> – GCD and Field Extensions</title>

					<p>
						Let <m>F</m> be a field. For <m>f, g \in F[x]</m>[^1], not both of which are <m>0</m>, recall that <m>\gcd_{F[x]}(f,g)</m> denotes the unique monic generator of the ideal in <m>F[x]</m> generated by <m>f</m> and <m>g</m>. 1. For any field extension <m>F \subseteq L</m>, <m>\gcd_{F[x]}(f,g) = \gcd_{L[x]}(f,g)</m>. 2. Let <m>\overline{F}</m> be an [[Mathematics/Definitions/Algebraic Closure|algebraic closure]] of <m>F</m>. <m>\gcd_{F[x]}(f,g) = 1</m> if any only if <m>f</m> and <m>g</m> have no common roots in <m>\overline{F}</m>.
					</p>

        </paragraphs>

					<paragraphs xml:id="proof.-45">
						<title><em>Proof.</em></title>

						<p>
							Let <m>h = \gcd_{F[x]}(f, g)</m>. To prove (1), we note that <m>h</m> is the unique monic polynomial such that - <m>h = f \cdot p + g \cdot q</m> for some <m>p,q \in F[x]</m>, - <m>f = h \cdot \a</m> for soem <m>\a \in F[x]</m>, and - <m>g = h \cdot \beta</m> for soem <m>\beta \in F[x]</m>. Since <m>F[x]</m> is a subring of <m>L[x]</m>, these three properties also hold when we regard <m>h,f,g</m> as belonging to <m>L[x]</m> and thus by the uniqueness property, we have <m>h = \gcd_{L[x]}(f,g)</m>.
						</p>

						<p><ol>
							<li>
							is a consequence of (1), since if two polynomials factor completely into linear factors, then they are relatively prime if and only if they have no linear factors in common, which is equivalent to their having no roots in common.
							</li>

						</ol></p>

					</paragraphs>
				

				<paragraphs xml:id="prop-criteria-for-separability">
					<title><m>\prop</m> – Criteria for Separability</title>

					<p>
						Let <m>F</m> be field and <m>\ov{F}</m> an [[Mathematics/Definitions/Algebraic Closure]] of <m>F</m>. 1. Given <m>f(x) \in F[x]</m>[^2] and <m>\a \in \ov{F}</m>, the [[Mathematics/Definitions/Multiplicity|multiplicity]] of <m>\a</m> in <m>f(x)</m> is at least <m>2</m> if and only if <m>f(\a) = 0</m> and <m>f'(\a) = 0</m>[^1]. 2. <m>f</m> is separable if and only if <m>\gcd(f(x), f'(x)) = 1</m> in <m>F[x]</m>. 3. If <m>f(x)</m> is irreducible in <m>F[x]</m>, then <m>f</m> is separable if and only if <m>f'(x) \neq 0</m>.
					</p>

        </paragraphs>

					<paragraphs xml:id="proof.-46">
						<title><em>Proof.</em></title>

						<p>
							For (1), suppose <m>\a</m> is a root of <m>f(x)</m> of multiplicity at least two. Then <m>f(x) = (x-\a)^2g(x)</m> in <m>\ov{F}[x]</m> and hence <m>f'(x) = 2(x - \a) g(x) + (x-\a)^2 g(x)</m>, by the Product Rule. It follows that <m>f'(\a) = 0</m>. Conversely, suppose <m>f(\a) = f'(\a) =0</m>. Since <m>f(\a) = 0</m>, we have <m>f(x) = (x-\a)h(x)</m> and hence <m>f'(x) = h(x) + (x-\a)h'(x)</m>. Since <m>f'(\a) = 0</m> it follows <m>h(\a) = 0</m> and thus <m>\a</m> has multiplicity at least two.
						</p>

						<p>
							For the second assertion, by (1), we have that <m>f</m> is separable if and only if <m>f</m> and <m>f'</m> has no common roots in <m>\ov{F}</m>. The result thus follows from the Lemma.
						</p>

						<p>
							For the final assertion, assume <m>f(x)</m> is irreducible. Since the degree of <m>f'(x)</m> is strictly less than the degree of <m>f(x)</m>, we have that <m>\gcd(f(x),f'(x)) \ne 1</m> if and only if <m>f'(x) = 0</m>.
						</p>

					</paragraphs>
				

				<paragraphs xml:id="exe-separability-and-x4-x3-x1">
					<title><m>\exe</m> – Separability and <m>(x^4-x^3-x+1)</m></title>

					<p>
						<m>x^4 -x^3 -x + 1</m> is not separable sine <m>x-1</m> is a double root (it factors as <m>(x^3-1)(x-1) = (x^2+x+1)(x-1)^2</m>). As predicted by the Theorem, it fails to be relatively prime to its derivative, which is <m>4x^3 - 3x^2 -1</m>, since each are divisible by <m>x-1</m>.
					</p>

					<p>
						<m>x^3-1</m> is separable in <m>\R[x]</m> because it has 3 distinct roots in <m>\C</m>, namely <m>1,\zeta_3,\zeta_3^2</m>. As predicted by the Theorem, it is relatively prime to its derivative <m>3x^2</m>.
					</p>

					<p>
						Now interpret <m>x^3 - 1</m> as belonging to <m>(\Z/3)[x]</m>. Then <m>x^3 - 1 = (x-1)^3</m> is not separable. As predicted by the Theorem, it is not relatively prime to its derivative, which is <m>0</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="exe-zpy">
					<title><m>\exe</m> – <m>\Z/p(y)</m></title>

					<p>
						Let <m>F</m> be a field of characteristic <m>p &gt; 0</m> and assume <m>a \in F</m> is an element such that <m>a \ne b^p</m> for all <m>b \in F</m>.Then <m>x^p - a</m> is irreducible but not separable. It is not separable since in <m>\ov{F}</m> we have <m>x^p - a= (x-b)^p</m> where <m>b^p = a</m>. Also note that its derivative is <m>px^{p-1} = 0</m>.
					</p>

					<p>
						It is less obvious that it is irreducible, but we can see that this is indeed the case in a specific example: Take <m>F = \Z/p(y)</m> (the field of fractions of the polynomial ring <m>\Z/p[y]</m>) and let <m>a = y</m>. In this case, <m>x^p - y</m> is seen to be irreducible, by Eisenstein, but not separable.
					</p>

				</paragraphs>

				<paragraphs xml:id="cor-separability-and-characteristic-zero">
					<title><m>\cor</m> – Separability and Characteristic Zero</title>

					<p>
						If <m>f(x)</m> is an irreducible polynomial with coefficients in a field of characteristic <m>0</m>, then <m>f(x)</m> is [[Mathematics/Definitions/Multiplicity|separable]]. More generally, if <m>f(x)</m> is irreducible of degree <m>n</m> and <m>\char(F) \nmid n</m>, then <m>f(x)</m> is separable.
					</p>

				</paragraphs>

				<paragraphs xml:id="defn-separable-field-extension">
					<title><m>\defn</m> – Separable (Field Extension)</title>

					<p>
						An algebraic field extension <m>F \subseteq L</m> is called <em><m>\defnn{separable}</m></em> if for every <m>\a \in L</m> its minimum polynomial <m>m_{\a, F}(x)</m> is [[Mathematics/Definitions/Multiplicity|separable]] (i.e., has no repeated roots in an algebraic closure of <m>F</m>).
					</p>

				</paragraphs>

				<paragraphs xml:id="cor-separability-algebraic-extensions-and-char-zero">
					<title><m>\cor</m> – Separability, Algebraic Extensions, and Char Zero</title>

					<p>
						If <m>\char(F) = 0</m>[^1], then every algebraic field extension <m>F \subseteq L</m> is [[Mathematics/Definitions/Multiplicity|separable]].
					</p>

        </paragraphs>

					<paragraphs xml:id="proof.-47">
						<title><em>Proof.</em></title>

						<p>
							Suppose <m>F\subseteq L</m> is an algebraic extension and let <m>\alpha\in L</m> be algebraic over <m>F</m>. Then the minimal polynomial <m>f(x)</m> of <m>\alpha</m> over <m>F</m> has coefficients in <m>F</m> and is of the form <me>f(x) = x^m + a_{m-1} x^{m-1} + \cdots + a_1 x + a_0</me> where <m>a_i \in F</m>. We need to show that <m>f(x)</m> is separable, i.e., has no repeated roots in its splitting field.
						</p>

						<p>
							Suppose <m>f(x)</m> has a repeated root <m>\beta</m> in some splitting field <m>K</m> of <m>f(x)</m>, i.e., <m>f(x) = (x-\beta)^2 g(x)</m> for some polynomial <m>g(x) \in K[x]</m>. Since <m>\alpha</m> is also a root of <m>f(x)</m>, we have <m>(\alpha - \beta)^2 g(\alpha) = 0</m>. Since <m>F\subseteq L</m> is an algebraic extension, <m>\alpha</m> is algebraic over <m>F</m>, so <m>g(\alpha)\neq 0</m>. It follows that <m>(\alpha-\beta)^2=0</m>, i.e., <m>\alpha=\beta</m>, which means that <m>\alpha</m> is a repeated root of <m>f(x)</m>, contradicting the assumption that <m>f(x)</m> has no repeated roots. Hence <m>f(x)</m> is separable, and since <m>\alpha</m> was an arbitrary algebraic element of <m>L</m>, we conclude that <m>F\subseteq L</m> is a separable extension.
						</p>

					</paragraphs>
				

				<paragraphs xml:id="exe-f-zpy-subseteq-zpz-l">
					<title><m>\exe</m> – <m>F = (\Z/p)(y) \subseteq (\Z/p)(z) = L</m></title>

					<p>
						Let <m>y</m> and <m>z</m> be indeterminants. The extension of fields <m>F = (\Z/p)(y) \subseteq (\Z/p)(z)= L</m> given by identifying <m>y</m> with <m>z^p</m> is not separable. Somewhat more precisely, <m>F</m> is isomorphism to the subfield of <m>L</m> consisting of elements of the form <m>\frac{\sum_i a_i z^{ip}}{\sum_j b_j z^{jp}}</m>, with the isomorphism given by sending <m>\frac{\sum_i a_i y^{i}}{\sum_j b_j y^{j}}</m> to <m>\frac{\sum_i a_i z^{ip}}{\sum_j b_j z^{jp}}</m>.
					</p>

					<p>
						Then <m>z \in L</m> is a root of the polynomial <m>x^p - y \in F[x]</m>. Moreover since <m>F</m> is the field of fractions of the PID <m>R = (\Z/p)[y]</m> and <m>y</m> is a prime element of <m>R</m>, we may apply Eisenstein (and Gauss) to conclude that <m>x^p - y</m> is irreducible in <m>F[x]</m>. This proves that <m>m_{z, F}(x) = x^p - y</m>. This polynomial is not separable since in <m>L[x]</m> it is equal to <m>(x-z)^p</m> and hence has a repeated root. (Or, you may use that its derivative is <m>0</m>.)
					</p>

				</paragraphs>

				<paragraphs xml:id="problem-9-separable-factorization-and-irreducibility">
					<title>Problem 9 – Separable, Factorization, and Irreducibility</title>

					<p>
						Assume <m>F</m> is field and let <m>f(x)\in F [x]</m>. Recall that <m>f (x)</m> is separable if <m>f (x)</m> has no repeated roots in an [[Mathematics/Definitions/Algebraic Closure|algebraic closure]] of <m>F</m>.
					</p>

					<p><ol>
						<li>
						Assume <m>\char(F) = 0</m>[^1]. Prove that <m>f(x)</m> is separable if and only if the irreducible factorization of <m>f(x)</m> in <m>F[x]</m> has no repeated factors.
						</li>

						<li>
						Fix a prime integer <m>p</m>, let <m>\F_p</m> be the field with <m>p</m> elements, and let <m>F</m> be the [[Mathematics/Definitions/Field of Fractions|field of fractions]] of the polynomial ring <m>\F_{p}[y]</m>. Prove <m>x^p-y</m> is irreducible in <m>F[x]</m> but not separable.
						</li>

					</ol></p>

        </paragraphs>
					
        <p>
          proof
        </p>
						

        
						<paragraphs xml:id="part-a-17">
							<title>Part (a)</title>

							<p>
								<m>(\Rightarrow)</m> Suppose that <m>f</m> is not separable, so <m>f</m> has a repeated root in <m>\overline{F}</m>, which we denote <m>\alpha</m>. So <m>x-\alpha</m> is a factor of <m>f(x)</m>. By Corollary 2.96, <m>F(\alpha)</m> is separable, so the minimal polynomial of <m>\alpha</m> in <m>F[x]</m> has no repeated root in <m>\overline{F}</m>. As <m>f</m> does have a repeated root (by supposition) it cannot be the minimum polynomial of <m>\alpha</m>. Thus <m>f(x)=\mp_{\alpha,F[x]}(x)g(x)</m> for some <m>g(x)\in F[x]</m> such that <m>g(x)</m> has <m>\alpha</m> as a root, otherwise <m>f</m> would not obtain its repeated root. However, this means that <m>\mp_{\alpha,F[x]}(x)\big|g(x)</m>, meaning that <m>g(x)</m> has <m>x-\alpha</m> as a factor as well. Thus we see that <m>x-\alpha</m> is a repeated factor of <m>f(x)</m>, one from the minimum polynomial, one from <m>g(x)</m>.
							</p>

							<p>
								<m>(\Leftarrow)</m> Suppose that the prime factorization of <m>f(x)</m> in <m>F[x]</m> admits a repeated factor. Thus there exists some prime (and thus irreducible) <m>g(x)</m> such that <m>g(x)g(x)|f(x)</m>. However, <m>g</m> has a root <m>\alpha</m> in <m>\overline{F}</m>, so in <m>F(\alpha)</m> we see that <m>f(x)</m> has <m>\alpha</m> as a root as well, as <m>g(a)=0</m>. But since <m>g(x)</m> has factor <m>x-\alpha</m>, it shows up twice in the factorization of <m>f</m> because <m>g(x)g(x)|f(x)</m>. So <m>\alpha</m> has multiplicity at least 2, so <m>f</m> is not separable.
							</p>

						</paragraphs>

						<paragraphs xml:id="part-b-16">
							<title>Part (b)</title>

							<p>
								Let <m>y,z</m> be indeterminants, <m>F=\F_{p}[y]</m>, and <m>L=\F_{p}[z]</m> such that <m>z^3=y</m> (as seen in Example 2.78). Note then that <m>z^3</m> is a root of the polynomial <m>x^p-y\in F[x]</m>.
							</p>

							<p>
								Moreover, since <m>F</m> is the field of fractions of the PID <m>R=\F_{p}[y]</m> and <m>y</m> is a prime element of <m>R</m>, we may apply Eisenstein’s Criterion (using <m>p=y</m>) to conclude that <m>x^p-y</m> is irreducible in <m>F[x]</m>. Thus <m>x^p-y</m> is the minimum polynomial of <m>z</m> in <m>F[x]</m>.
							</p>

							<p>
								However, as the derivative of this polynomial is <m>0</m>, we see that the <m>\mp_{z,F[x]}(x)</m> is not separable by Proposition 2.72. However, by the Freshman’s Dream, we see that <m>x^3-y=x^p-z^p=(x-z)^p\in L</m>. But as <m>z\not\in F</m>, we see that the prime factorization of <m>x^p-y</m> admits no repeated factor. #### Problem 6 – Strict Inequality of Automorphism Group Assume <m>F \sse L</m> is a finite [[Mathematics/Definitions/Field Extension|extension]] of fields and that the characteristic of <m>F</m> is <m>p</m>, where p is a prime. Prove that if there exists an element <m>\a\in L\setminus F</m> such that <m>\a^p \in F</m> , then <m>|\Aut(L/F )| &lt; [L : F ]</m>. You may use, without proof, the fact that <m>|\Aut(K/E)|\leq [K : E]</m> for any finite extension of fields <m>E \sse K.</m>
							</p>

						</paragraphs>
					

					<paragraphs xml:id="proof.-49">
						<title><em>Proof</em>.</title>

						<p>
							Let <m>F \sse L</m> is a finite extension of fields and that the characteristic of <m>F</m> is <m>p</m>, where <m>p</m> is a prime, and suppose there exists an element <m>\a\in L\setminus F</m> such that <m>\a^p \in F</m>.
						</p>

						<p>
							Consider the polynomial <m>f(x)=x^{p}-\a^p\in F[x]</m>, and notice that <m>f'(x)=px^{p-1}=0</m>, as we are in a field of characteristic <m>p</m>. However, this characteristic also yields <m>x^p-\a^p=(x-\a)^p</m>. As <m>F</m> is a field we have <m>F[x]</m> as a UFD, and thus as <m>x-\a</m> is irreducible in <m>F[x]</m> it is also prime. Therefore this is the unique factorization of <m>f</m> up to associates. If <m>f</m> was reducible <m>F</m> it would thus have to be divisible into power os <m>x-\a</m>, which will never be reducible as <m>\a\not\in F</m>. Thus <m>f</m> is irreducible in <m>F[x]</m>, making it the minimal polynomial of <m>\a</m>. However, if <m>|\Aut(L/F )|= [L : F ]</m> this would make <m>L</m> the splitting field of <m>f</m> over <m>F</m>, which it is not, given <m>\a\not\in F</m>. Thus <m>|\Aut(L/F )| &lt; [L : F ]</m>. #### Problem 5 #unfinished Let F be a field, and let <m>f (x) ∈ F [x]</m>. Recall that <m>f (x)</m> is separable provided, for every extension field <m>K/F , f (x)</m> has no multiple roots in <m>K</m>. (A multiple root is an element <m>α ∈ K</m> such that <m>(x − α)^2 | f (x)</m> in <m>K[x].)</m>
						</p>

						<p><ol>
							<li>
							Prove that <m>f (x) ∈ F [x]</m> is separable if and only if <m>f (x)</m> and its derivative <m>f'(x)</m> are relatively prime in <m>F [x]</m>.
							</li>

							<li>
							Suppose that <m>f (x)</m> is irreducible and that the degree of <m>f (x)</m> is not a multiple of the characteristic of <m>F</m>. Prove that <m>f (x)</m> is separable.
              </li>
            </ol>
          </p>
        </paragraphs>


          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
    </section>

        <section xml:id="sec-galext">
          <title>Galois Extensions</title>
          <p>
            
          

          <m>\defn</m> – Field Automorphism Group Let <m>K</m> be a field. The <em><m>\defnn{automorphism group}</m></em> of <m>K</m>, written <m>\Aut(K)</m>, is the collection of field automorphisms of <m>K</m>, with the binary operation of composition.
							
          
          </p>

						<p>
							The <em><m>\defnn{automorphism group}</m></em> of a field extension <m>F \subseteq K</m>, written <m>\Aut(K/F)</m>, is the subgroup of <m>\Aut(K)</m> consisting of those field automorphisms of <m>K</m> that restrict to the identity on <m>F</m>.
						</p>



				<paragraphs xml:id="exe-autcr">
					<title><m>\exe</m> – <m>\Aut(\C/\R)</m></title>

					<p>
						I claim <m>\Aut(\C/\R)</m> has two elements (and so is a cyclic group of order <m>2</m>): the identity map on <m>\C</m> and the element <m>\s</m> given as complex conjugation. It is easy to see each of these is an element of <m>\Aut(\C/\R)</m> — for <m>\s</m>, this amounts to the fact that complex conjugation commutes with addition and multiplication of complex numbers (and that it sends <m>1</m> to <m>1</m>).
					</p>

					<p>
						To see these are the only elements of <m>\Aut(\C/\R)</m>, suppose <m>\tau \in \Aut(\C/\R)</m>. For any <m>z = a + ib \in \C</m>, we have <m>\tau(z) = a + b \tau(i)</m> since <m>\tau|_\R = \id_\R</m>. Moreover, <m>-1 = \tau(-1) = \tau(i \cdot i) = \tau(i) \cdot \tau(i)</m> and so <m>\tau(i) = \pm 1</m>. Thus <m>\tau = \id</m> or <m>\tau = \s</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="exe-autqsqrt-dq">
					<title><m>\exe</m> – <m>\Aut(\Q(\sqrt d)/\Q)</m></title>

					<p>
						For any square-free integer <m>d</m>, <m>\Aut(\Q(\sqrt{d})/\Q)</m> has order <m>2</m>, and its two elements are the identity and the map sending <m>a + b \sqrt{d}</m> to <m>a - b \sqrt{d}</m>. Checking that each really is an element of this group and that there are the only two elements in this group is done similarly to the previous example.
					</p>

				</paragraphs>

				<paragraphs xml:id="exe-aut-and-x3-2">
					<title><m>\exe</m> – <m>\Aut</m> and <m>(x^3-2)</m></title>

					<p>
						For a more complicated example, let <m>K</m> be the splitting field of <m>x^3 - 2</m> over <m>\Q</m>. Recall <me>
K= \Q(\a_1, \a_2, \a_3)
</me> <me>
\a_1 = \sqrt[3]{2}, \a_2 = e^{2 \pi i/3}\sqrt[3]{2}, \a_3 = e^{4 \pi i/3}\sqrt[3]{2}.
</me> Let us ponder how big <m>\Aut(K/\Q)</m> could be. Pick any <m>\s \in \Aut(K/\Q)</m>. Since <m>\s</m> is a ring homomorphism, for any <m>i</m> we have <me>
\s(\a_i)^3 = \s(\a_i^3) = \s(2) = 2
</me> and thus <m>\s(\a_i)</m> is also a root of <m>x^2 - 2</m>. In other words, for each <m>i</m> we have <m>\s(\a_i) = \a_j</m> for some <m>j</m>. Moreover, since <m>K</m> is generated as a field extension of <m>\Q</m> by <m>\a_1, \a_2, \a_3</m>, the action of <m>\s</m> on the three roots completely determines the action of <m>\s</m> on all of <m>K</m>. In more detail, every element of <m>K</m> is given taking <m>\Q</m>-linear combinations of sums and products and quotients of these roots, and any element of <m>\Aut(K/\Q)</m> preserves sums, products and <m>\Q</m>-linear combinations.
					</p>

					<p>
						To summarize, we have proven that there are {} <m>3! = 6</m> possibilities for <m>\s</m>. In fact, more is true: The function <me>
\phi: \Aut(K/\Q) \to \Perm(\{\a_1, \a_2, \a_3\})
</me> given by sending <m>\s \in \Aut(K/\Q)</m> to its restriction to the subset <m>\{\a_1, \a_2, \a_3\}</m> is an injective group homomorphism. Thus <m>\Aut(K/\Q)</m> is isomorphic to a subgroup of <m>S_3</m>. I claim that <m>\# \Aut(K/\Q) = 6</m> and hence <m>\Aut(K/\Q) \cong S_3</m>. I will prove this directly – we will learn of fancier methods to do so later.
					</p>

					<p>
						First we notice that the field automorphism of <m>\C</m> given by complex conjugation, namely <m>\C \to \C, z \mapsto \overline{z}</m>, permutes the roots of <m>x^3 - 2</m> and hence it restricts to a field map from <m>K</m> into <m>K</m>. Since this map is <m>\Q</m>-linear and injective (as are all field maps) and <m>K</m> is a finite dimensional <m>\Q</m>-vector space, this map must be onto as well. So, we obtain an element <m>\a \in \Aut(K/\Q)</m> given by <m>\a(z) = \overline{z}</m> for all <m>z \in K</m>. It corresponds <m>(2 \, 3) \in S_3</m>.
					</p>

					<p>
						Next, we apply Porism , which gives <m>\gamma \in \Aut(K/\Q)</m> such that <m>\gamma(\sqrt[3]{2})=e^{2 \pi i/3} \sqrt[3]{2}</m>. So, in the numbering above, <m>\gamma</m> corresponds to either <m>(1 \, 2)</m> or <m>(1 \, 2 \, 3)</m> in <m>S_3</m>. We don’t really know which of these it is. (In fact, both will occur — the map <m>\gamma</m> is not unique.) But either way we have proven the claim: For notice that both subsets <m>\{ (2 \, 3), (1 \, 2)\}</m> and <m>\{ (2 \, 3), (1 \, 2 \, 3)\}</m> of <m>S_3</m> generated all of <m>S_3</m>.
					</p>

					<p>
						In other words, every possible permutation of roots of <m>x^3 - 2</m> arises as a field automorphism of its splitting field over <m>\Q</m>. This is what I meant before when I said that the roots of <m>x^3 - 2</m> are “as symmetric as possible’’.
					</p>

				</paragraphs>

				<paragraphs xml:id="prop-automorphisms-and-permutations-fields">
					<title><m>\prop</m> – Automorphisms and Permutations (Fields)</title>

					<p>
						Suppose <m>F \subseteq K</m> is a field extension and <m>f(x) = F[x]</m>. Let <m>R = \{\a \in K \mid f(\a) = 0\}</m>, the set of roots of <m>f</m> in <m>K</m>. 1. For any <m>\a \in K</m> and <m>\s \in \Aut(K/F)</m>[^1], we have <m>\s(f(\a)) = f(\s(\a))</m>. 2. If <m>\a \in R</m> then <m>\s(\a) \in R</m> for all <m>\s \in \Aut(K/F)</m>. 3. The function <m>\phi: \Aut(K/F) \to \Perm(R)</m>[^2] given by <m>\phi(\s)(\a) = \s(\a)</m> for <m>\a \in R</m> is a homomorphism of groups. 4. If <m>K= F(R)</m>[^3] then <m>\phi</m> is one-to-one. In particular, if <m>K</m> is the splitting field over <m>F</m> of <m>f(x)</m>, then <m>\Aut(L/K)</m> is isomorphic to a subgroup of <m>S_n</m> where <m>n</m> is the number of roots of <m>f</m> in <m>L</m>. (Note that <m>n \leq \deg(f)</m>[^4].)
					</p>


					<paragraphs xml:id="proof.-50">
						<title><em>Proof.</em></title>

						<p>
							Say <m>f = \sum_i c_i x^i</m> with <m>c_i \in F</m>. Then <me>\s(f(\a)) = \s \left(\sum_i c_i \a^i\right) = \sum_i \s(c_i) \s(\a)^i = \sum_i c_i \s(\a)^i = f(\s(\a)).</me>This proves (1) and (2) is an immediate consequence.
						</p>

						<p>
							To show (3) we need to first prove that for all <m>\s \in \Aut(K/F)</m>, the function <m>\phi(\s): R \to R</m> is a bijection. (The target of this function is indeed <m>R</m> by (2).) Since <m>R</m> is finite, we just need to show its one-to-one. This holds since <m>\s</m> itself is one-to-one. (If <m>\phi(\s)(\a) = \phi(\s)(\a')</m> for <m>\a, \a' \in R</m> then <m>\s(\a) = \s(\a')</m> and, since <m>\s</m> is one-to-one, we have <m>\a = \a'</m>.) Since the group laws of <m>\Aut(K/F)</m> and <m>\Perm(R)</m> are both given as function composition, it is clear that <m>\phi</m> is a group homomorphism. In detail, <me>\phi(\s \circ \tau)(\a) = \s(\tau(\a)) = (\phi(\s)\circ \phi(\tau))(\a)</me> for all <m>\a</m> and so <m>\phi(\s \circ \tau) = \phi(\s) \circ \phi(\tau)</m>.
						</p>

						<p>
							Now asssume <m>K = F(R)</m>. We show <m>\phi</m> is one-to-one by showing its kernel is trivial. Suppose <m>\phi(\s) = \id_{\Perm(R)}</m>; that is, suppose <m>\s(\a) = \a</m> for all <m>\a \in R</m>. We show <m>\s = \id_K</m>. Define <me>K^\sigma := \{\beta \in K \mid \s(\beta) = \beta\},</me> the so-called {} of <m>\s</m>. Note that <m>F \subseteq K^\s</m> since <m>\s \in \Aut(K/F)</m> and we also have <m>R \subseteq K^\s</m> by assumption. It is easy to see that <m>K^\s</m> is a subfield of <m>K</m>. (Here are the details: Clearly <m>1 \in K^\s</m>. For <m>\beta_1, \beta_2 \in K^\s</m>, we have <me>
\s(\beta_1 \pm \beta_2) = \s(\beta_1) \pm \s(\beta_2)  = \beta_1 \pm \beta_2,</me><me>\s(\beta_1 \cdot\beta_2) = \s(\beta_1) \cdot \s(\beta_2)  = \beta_1 \cdot \beta_2,</me>and, if <m>\beta_2 \ne 0</m>, <me>\s(1/\beta_2) =1 /\s(\beta_2)  =1/ \beta_2,</me> and thus <m>\beta_1 \pm \beta_2, \beta_1 \cdot \beta_2, 1/\beta_2 \in K^\s</m>.) So, <m>K^\s</m> is a subfield of <m>K</m> that contains <m>F</m> and <m>R</m>, but recall that <m>F(R)</m> is the {} subfield of <m>K</m> that contains <m>F</m> and <m>R</m>, and so we must have <m>K^\s = K</m>. But then <m>\s(\a) = \a</m> for all <m>\a \in K</m> and hence <m>\s = \id_K</m>.
						</p>

					</paragraphs>
				</paragraphs>

				<paragraphs xml:id="defn-fixed-field">
					<title><m>\defn</m> – Fixed Field</title>

					<p>
						Let <m>\s\in\Aut(K)</m>[^1]. Define <me>
K^\sigma := \{\beta \in K \mid \s(\beta) = \beta\},
</me> the so-called <em><m>\defnn{fixed field}</m></em> of <m>\s</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="cor-automorphisms-and-group-actions-fields">
					<title><m>\cor</m> – Automorphisms and Group Actions (Fields)</title>

					<p>
						Let <m>K</m> be the spitting field of some <m>f(x) \in F[x]</m>. 1. The group <m>\Aut(K/F)</m>[^1] acts on the set of roots <m>f(x)</m> in <m>K</m> by the rule <m>\s \cdot \a := \s(\a)</m>. 2. This action is faithful. 3. If <m>f</m> is irreducible, this action is transitive.
					</p>


					<paragraphs xml:id="proof.-51">
						<title><em>Proof.</em></title>

						<p>
							We proceed by induction on <m>[L:F]</m>. When <m>[L:F]=1</m>, we have <m>L = F</m> and <m>\Aut(L/F)</m> is the one element group.
						</p>

						<p>
							For <m>[L: F] &gt; 1</m>, pick <m>\a \in L \setminus F</m> and let <m>p(x) = m_{\a, F}(x)</m>. Consider the extension <m>F \subseteq F(\a)</m>. Note that <m>H = \Aut(L/F(\a))</m> is a subgroup of <m>G = \Aut(L/F)</m>, by definition. By induction we have <m>|H| \leq [L: F(\a)]</m>. Using the degree formula and the fact that <m>|G| = |H| \cdot [G:H]</m>, it suffices to prove <m>[G:H] \leq [F(\a):F]</m>. This follows from:
						</p>

						<p>
							{} Let <m>R</m> be the set of roots of <m>p(x)</m> that belong to <m>L</m>. Then the function <me>\begin{equation}
\theta: G/H  \to R
\end{equation}</me> given by <m>\theta(gH) = g(\a)</m> is well-defined and injective. (Note: <m>H</m> is not necessarily normal, and so <m>G/H</m> isn’t a group. By <m>G/H</m> I just mean the {} of left cosets of <m>H</m> in <m>G</m>.)
						</p>

						<p>
							By Proposition , for each <m>g \in G</m> we have <m>g(\a) \in R</m>. Moreover, by definition of <m>H</m>, for all <m>h \in H</m>, we have <m>h(\a) = \a</m> and hence <m>gh(\a)=g(h(\a))=g(\a)</m>. This proves <m>\theta</m> is a well-defined function (i.e., it is independent of left coset representative). For <m>g_1, g_2 \in G</m>, if <m>g_1(\a) = g_2(\a)</m> then <m>g_2^{-1}g_1(\a) = \a</m> which implies <m>g_2^{-1}g_1 \in H</m> (since if an automorphism of <m>L</m> fixes <m>F</m> and <m>\a</m> then it fixes <m>F(\a)</m>). Thus <m>g_1(\a) = g_2(\a)</m> implies <m>g_1H = g_2H</m> and hence <m>\theta</m> is one-to-one.
						</p>

						<p>
							Since <m>\deg(p(x)) = [F(\a): F]</m>, we conclude from the claim that <m>[G:H] =|G/H|\leq [F(\a): F]</m>. Putting all this together gives <m>|G| = |H|\cdot [G:H] \leq [L:F(\a)][F(\a):F] = [L:F]</m>.
						</p>

					</paragraphs>
				</paragraphs>

				<paragraphs xml:id="prop-automorphisms-and-finite-extension-degree">
					<title><m>\prop</m> – Automorphisms and Finite Extension Degree</title>

					<p>
						If <m>F \subseteq L</m> is a finite extension of fields, then <m>|\Aut(L/F)| \leq [L : F]</m>[^1].
					</p>


					<paragraphs xml:id="proof.-52">
						<title><em>Proof.</em></title>

						<p>
							We proceed by induction on <m>[L:F]</m>. When <m>[L:F]=1</m>, we have <m>L = F</m> and <m>\Aut(L/F)</m> is the one element group.
						</p>

						<p>
							For <m>[L: F] &gt; 1</m>, pick <m>\a \in L \setminus F</m> and let <m>p(x) = m_{\a, F}(x)</m>. Consider the extension <m>F \subseteq F(\a)</m>. Note that <m>H = \Aut(L/F(\a))</m> is a subgroup of <m>G = \Aut(L/F)</m>, by definition. By induction we have <m>|H| \leq [L: F(\a)]</m>. Using the degree formula and the fact that <m>|G| = |H| \cdot [G:H]</m>, it suffices to prove <m>[G:H] \leq [F(\a):F]</m>. This follows from:
						</p>

						<p>
							{} Let <m>R</m> be the set of roots of <m>p(x)</m> that belong to <m>L</m>. Then the function <me>\begin{equation}
\theta: G/H  \to R
\end{equation}</me> given by <m>\theta(gH) = g(\a)</m> is well-defined and injective. (Note: <m>H</m> is not necessarily normal, and so <m>G/H</m> isn’t a group. By <m>G/H</m> I just mean the {} of left cosets of <m>H</m> in <m>G</m>.)
						</p>

						<p>
							\begin{proof} By Proposition , for each <m>g \in G</m> we have <m>g(\a) \in R</m>. Moreover, by definition of <m>H</m>, for all <m>h \in H</m>, we have <m>h(\a) = \a</m> and hence <m>gh(\a)=g(h(\a))=g(\a)</m>. This proves <m>\theta</m> is a well-defined function (i.e., it is independent of left coset representative). For <m>g_1, g_2 \in G</m>, if <m>g_1(\a) = g_2(\a)</m> then <m>g_2^{-1}g_1(\a) = \a</m> which implies <m>g_2^{-1}g_1 \in H</m> (since if an automorphism of <m>L</m> fixes <m>F</m> and <m>\a</m> then it fixes <m>F(\a)</m>). Thus <m>g_1(\a) = g_2(\a)</m> implies <m>g_1H = g_2H</m> and hence <m>\theta</m> is one-to-one.
						</p>

					</paragraphs>
				</paragraphs>

				<paragraphs xml:id="exe-lq-and-x4-2">
					<title><m>\exe</m> – <m>[L:\Q]</m> and <m>(x^4-2)</m></title>

					<p>
						Let <m>L</m> be the splitting field of the irreducible polynomial <m>q(x) = x^4 - 2 \in \Q[x]</m>. So <m>L = \Q(R)</m> where <m>R = \{z_1 = \sqrt[4]{2}, z_2 = i \sqrt[4]{2}, z_3= - \sqrt[4]{2}, z_4 = - i\sqrt[4]{2})\}</m>. By the Corollary above, the action of <m>\Aut(L/\Q)</m> on <m>R</m> is faithful so that we have an injective group homomorphism <m>\Aut(L/\Q) \to \Perm(R)</m>.
					</p>

					<p>
						Note that this map cannot possibly be onto: there is no <m>\s \in \Aut(L/\Q)</m> such that <m>\s(z_1) = z_2</m>, <m>\s(z_2) = z_1</m>, <m>\s(z_3) = z_3</m>, and <m>\s(z_4) =z_4</m>; i.e., the permutation <m>(1 \, 2)</m> of these roots is not realizable by a field automorphism. To see this note that if <m>\s(z_1) = z_2</m> then <m>\s(z_3) = \s(-z_1) = - \s(z_1) = - z_2 = z_4</m>. So, any field automorphism that interchanges <m>z_1</m> and <m>z_2</m> would have to also interchange <m>z_3</m> and <m>z_4</m>. In fact, as we shall see, <m>\# \Aut(L/\Q) = 8</m>, considerably smaller than <m>4!</m>.
					</p>

					<p>
						Let us compute <m>[L:\Q]</m>. Note that <m>i \in L</m> since <m>i = z_2/z_1</m> and in fact <m>L = \Q(z_1, i)</m>. In the chain of extensions <me>
\Q \subset \Q(z_1) \subset L = \Q(z_1)(i)
</me> the first one has degree <m>4</m>, since <m>x^4 - 2</m> is irreducible by Eisenstein, and the second has degree at most <m>2</m> since <m>i</m> is a root of <m>x^2 + 1</m>. It would be less than two if <m>x^2+1</m> factors in <m>\Q(z_2)[x]</m>. But since <m>\Q(z_1) \subset \R</m> and <m>L</m> is not contained in <m>\R</m>, the second extension cannot be trivial and so must have degree exactly <m>2</m>. We conclude <m>[L:\Q] = 8</m>. It follows from Proposition  that <m>\# \Aut(L/\Q) \leq 8</m>. (In fact, since <m>L</m> is the splitting field of a separable polynomial, the Theorem below will tell us that <m>\# \Aut(L/\Q) = 8</m>. But we won’t appeal to this fact here.)
					</p>

					<p>
						We claim <m>\# \Aut(L/\Q) = 8</m> and <m>\Aut(L/\Q)</m> is isomorphic to the subgroup <m>H</m> of <m>S_4</m> generated by <m>(2 \, 4)</m> and <m>(1 \,2 \, 3 \,4)</m>. (This is isomorphic to <m>D_8</m>.)
					</p>

					<p>
						The map <m>\C \to \C</m> given by complex conjugation permutes the roots of <m>x^4 - 2</m> and it restricts to an automorphism of <m>L</m> — specially, it fixes <m>z_1, z_3</m> and interchanges <m>z_2</m> and <m>z_4</m>. It follows that complex conjugation determines an element <m>\s \in \Aut(L/\Q)</m> that corresponds to <m>(2 \, 4) \in H \leq S_4</m>.
					</p>

					<p>
						By the degree formula we get <m>[L: \Q(i)] = 4</m>. Since <m>L = \Q(i)(z_1)</m>, the degree of <m>m_{z_1, \Q(i)}(x)</m> must be <m>4</m>. This shows that <m>x^4 - 2</m> must remain irreducible as a polynomial in <m>\Q(i)[x]</m>; this is not obvious, but we have now proven it, and this fact will be useful in what we do next.
					</p>

					<p>
						To construct another element of <m>\Aut(L/\Q)</m>, we use that that <m>L</m> is the splitting field of the polynomial <m>x^4 - 2</m> over <m>\Q(i)</m> and that, as we just showed, <m>x^4 - 2</m> is irreducible in <m>\Q(i)[x]</m>. We may thus apply Porism  (also stated in the Corollary) to get that there is an element <m>\tau \in \Aut(L/\Q(i))</m> such that <m>\tau(z_1) = z_2</m>. We may regard <m>\tau</m> as an element of <m>\Aut(L/\Q)</m> since, by definition, <m>\Aut(L/\Q(i))</m> is a subgroup of <m>\Aut(L/\Q)</m>. We have <me>
\tau(z_2) = \tau(i z_1) = \tau(i) \tau(z_1) = i z_2 = z_3
</me> since <m>\tau(i) = i</m> by construction. A key point here is that if we had merely specified <m>\tau</m> to be an element of <m>\Aut(L/\Q)</m> sending <m>z_1</m> to <m>z_2</m>, then we would have no idea what <m>\tau</m> does to <m>z_2</m> — it was key to define <m>\tau \in \Aut(L/\Q(i))</m> as we did. We then also get <m>\tau(z_3) = z_4</m> and <m>\tau(z_4) = z_1</m>. So <m>\tau</m> corresponds to the permutation <m>(1 \,2 \, 3 \,4)</m>.
					</p>

					<p>
						We have proven that <m>\Aut(L/\Q)</m> is isomorphic to a subgroup of <m>S_4</m> having order at most <m>8</m> (by the Proposition above) and that it contains <m>(2 \, 4)</m> and <m>(1 \,2 \, 3 \,4)</m>. Since the subgroup generated by these two elements has order <m>8</m>, the claim follows.
					</p>

				</paragraphs>

				<paragraphs xml:id="defn-normal-extension">
					<title><m>\defn</m> – Normal Extension</title>

					<p>
						A finite extension <m>F \subseteq L</m> is called <em><m>\defnn{normal}</m></em> if <m>L</m> is the splitting field of some (non-unique) polynomial <m>f(x) \in F[x]</m>[^1].
					</p>

				</paragraphs>

				<paragraphs xml:id="exe-normal-extension">
					<title><m>\exe</m> – Normal Extension</title>

					<p>
						<m>L = \Q(\sqrt[3]{2}, e^{2 \pi i/3})</m> is a normal extension of <m>\Q</m> since it is the splitting field of <m>x^3-2</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="exe-qsqrt32-not-normal">
					<title><m>\exe</m> – <m>\Q(\sqrt[3]2)</m> not Normal</title>

					<p>
						Is <m>\Q(\sqrt[3]{2})</m> normal? It isn’t the splitting field of <m>x^3 -2</m> clearly, but maybe it somehow is the splitting field of some other polynomial. It in fact is not normal, but it is not so obvious that it isn’t. The next Theorem will allow us to prove it isn’t normal.
					</p>

				</paragraphs>

				<paragraphs xml:id="thm-galois-extension-equivalencies">
					<title><m>\thm</m> – Galois Extension Equivalencies</title>

					<p>
						Let <m>F \subseteq L</m> be a finite extension of fields. The following are equivalent. 1. <m>|\Aut(L/F)| = [L:F]</m>[^1] 2. The extension is both normal and separable. 3. <m>L</m> is the splitting field of some separable polynomial with coefficients in <m>F</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="defn-galois-extension">
					<title><m>\defn</m> – Galois Extension</title>

					<p>
						A finite extension of fields is a <m>\defnn{Galois extension}</m> if the three equivalence conditions of this theorem hold: 1. <m>|\Aut(L/F)| = [L:F]</m>[^2] 2. The extension is both normal and separable. 3. <m>L</m> is the splitting field of some separable polynomial with coefficients in <m>F</m>.
					</p>

					<p>
						In this case it is customary to write <m>\Gal(L/F)</m> for the group <m>\Aut(L/F)</m>[^1] and to refer to it as the <em><m>\defnn{Galois group}</m></em> of the extension – they are exactly the same group, but the former is used only when the extension is Galois.
					</p>

				</paragraphs>

				<paragraphs xml:id="rem-9">
					<title><m>\rem</m></title>

					<p>
						Recall from the Proposition above that <m>\# \Aut(L/F) \leq [L:F]</m> holds for any finite field extension. So, such an extension is Galois if and only if its automorphism group is as large as is allowed by the Proposition.
					</p>

				</paragraphs>

				<paragraphs xml:id="exe-galois-extension">
					<title><m>\exe</m> – Galois Extension</title>

					<p>
						<m>L = \Q(\sqrt[3]{2}, e^{2 \pi i/3})</m> is a Galois extension of <m>\Q</m>, since it the splitting field of <m>x^3 - 2</m>. We proved above that <m>\Aut(L/\Q)</m> has six elements and <m>[L: \Q] = 6</m>, as the Theorem predicts.
					</p>

				</paragraphs>

				<paragraphs xml:id="exe-qsqrt32-not-galois">
					<title><m>\exe</m> – <m>\Q(\sqrt[3]2)</m> not Galois</title>

					<p>
						I claim <m>L = \Q(\sqrt[3]{2})</m> is not a Galois extension of <m>\Q</m>. Let <m>R</m> be the set of all roots of <m>x^3-2</m> in <m>L</m>. Since <m>L \subseteq \R</m>, <m>R</m> has just one element: <m>R = \{\sqrt[3]{2}\}</m>. Since <m>L = \Q(R)</m>, the function <m>\phi: \Aut(L/\Q) \to \Perm(R)</m> is injective by Proposition  and so since <m>\# R = 1</m>, we have <m>\# \Aut(L/\Q) = 1</m>. Thus it isn’t Galois. Since it is separable, <m>L</m> must not be a normal extension of <m>\Q</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="prop-one-root-to-factor-them-all">
					<title><m>\prop</m> – One Root to Factor them All</title>

					<p>
						Suppose <m>F \subseteq L</m> is a finite extension of fields and <m>f(x)\in F[x]</m>[^1] is an irreducible polynomial. If <m>F \subseteq L</m> is a normal extension and <m>f</m> has at least one root in <m>L</m>, then <m>f</m> factors completely in <m>L</m>.
					</p>


					<paragraphs xml:id="proof.-53">
						<title><em>Proof.</em></title>

						<p>
							Say <m>L</m> is the splitting field of <m>g(x) \in F[x]</m>. Let <m>K</m> be the splitting field of <m>g(x) \cdot f(x)</m>. So <m>F \subseteq L \subseteq K</m>. Say <m>\a</m> is a root of <m>f</m> that belongs to <m>L</m>, and let <m>\beta \in K</m> be any other root. We aim to prove <m>\beta \in L</m>.
						</p>

						<p>
							Recall that we know that if <m>E</m> is a splitting field over <m>F</m> of an {} polynomial, then <m>\Aut(E/F)</m> acts transitively on the roots of this polynomial. Something more general is true: If <m>E</m> is the splitting field over <m>F</m> of some possibly reducible polynomial <m>h(x)</m>, then for each irreducible factors <m>p(x)</m>, <m>\Aut(E/F)</m> acts transitively on the roots of <m>p(x)</m>. I won’t prove this, but will apply it to <m>F \subseteq K</m>. Since <m>f(x)</m> is irreducible, we get that there is there a <m>\s \in \Aut(K/F)</m> such that <m>\s(\a) = \beta</m>. Let <m>c_1, \dots, c_m \in L</m> be all the roots of <m>g</m>, so that <m>L = F(c_1, \dots, c_m)</m>. Since <m>\s</m> fixes <m>F</m>, it must permute these roots. In particular, <m>\s(L) \subseteq L</m>. But then <m>\beta = \s(\a) \in L</m>.
						</p>

					</paragraphs>
				</paragraphs>

				<paragraphs xml:id="exe-galois-and-characteristic-p">
					<title><m>\exe</m> – Galois and Characteristic <m>p</m></title>

					<p>
						Let <m>F</m> be a field of characteristic <m>p</m>, for a prime integer <m>p</m>, and assume <m>L</m> is a finite field extension of <m>F</m> such that there exists an element <m>a</m> of <m>L</m> with <m>a \notin F</m> but <m>a^p \in F</m>. Then <m>F \subseteq L</m> is not Galois since <m>\# \Aut(L/K) &lt; [L:K]</m> in this case. You will prove this in the Homework.
					</p>

				</paragraphs>

				<paragraphs xml:id="problem-5-galois-group-and-transitive-action">
					<title>Problem 5 – Galois Group and Transitive Action</title>

					<p>
						Let <m>E/F</m> be a finite Galois extension and let <m>G</m> be the Galois group of <m>E/F</m>.<!-- linebreak -->Suppose that <m>E = F (\a)</m> and let <m>f(x)</m> be the minimum polynomial of <m>\a</m> over <m>F</m> . Prove that <me>f(x) =\prod_{\s\in G} (x-\s(\a)).</me> ###### <em>Proof</em>. Let <m>E/F</m> be a finite Galois extension and let <m>G</m> be the Galois group of <m>E/F</m>. Suppose that <m>E = F (\a)</m> and let <m>f(x)</m> be the minimal polynomial of <m>\a</m> over <m>F</m>. Thus <m>G</m> acts on the roots of <m>f</m> faithfully.[^1] Additionally, as <m>f</m> is the minimal polynomial of <m>\a</m> it is irreducible, making the action transitive as well.
					</p>

					<p>
						As <m>E</m> is Galois over <m>F</m> we know that <m>f</m> splits into linear factors, each of the form <m>x-\b</m>, where <m>\b</m> is a root of <m>f</m>. As our action is transitive, for every root <m>\b</m> there exists a <m>\s\in G</m> such that <m>\s\cdot\a=b</m>, or <m>\s(\a)=\b</m>. #### Problem 4 – Prime Degree and Splitting Field Let <m>F</m> be a field of characteristic <m>p &gt; 0</m>, <m>a \in F</m> , and consider the polynomial <m>f(x)=x^p -x-a \in F[x]</m>[^1].
					</p>

					<p><ol>
						<li>
						Prove that <m>f(x)</m> is either irreducible over <m>F</m> or it splits into distinct linear factors over <m>F</m>. (<em>Hint</em>: If <m>\alpha</m> is a root of <m>f(x)</m>, consider <m>\alpha + j</m> for <m>j \in \Z/p</m>.)
						</li>

						<li>
						Suppose <m>f(x)</m> is irreducible over <m>F</m> and let <m>L</m> be a splitting field of <m>f</m> over <m>F</m>. Prove that the Galois group of <m>L</m> over <m>F</m> is cyclic.
						</li>

					</ol></p>

        </paragraphs>
					<paragraphs xml:id="proof.-54">
						<title><em>Proof.</em></title>

						<p>
							Let <m>F</m> be a field of characteristic <m>p &gt; 0</m>, <m>a \in F</m> , and consider the polynomial <m>f(x)=x^p -x-a \in F[x]</m>.
						</p>


						<paragraphs xml:id="part-a-18">
							<title>Part (a)</title>

							<p>
								Suppose <m>f</m> has a root, <m>\alpha</m>, in <m>F</m>. Then <m>\alpha^p-\alpha-a=0</m>. Consider <m>\alpha+j</m> for some <m>j\in\Z/p</m>, and observe <m>(\alpha+j)^p-(\alpha+j)-a</m>. By The Freshman’s Dream, we have <m>\alpha^p+j^p-\alpha-j-a</m>, but as <m>\alpha^p-\alpha-a=0</m>, we really have <m>j^p-j</m>. By Fermat’s Little Theorem, <m>p|j^p-j</m>, and thus <m>f(\alpha+j)=0</m>. Thus we have found <m>p</m> roots of <m>f</m>, and thus <m>f</m> splits into linear factors.
							</p>

							<p>
								Suppose then that no root of <m>f</m> exists in <m>F</m>. Let <m>L</m> be a splitting field of <m>f</m> over <m>F</m>, and note that from the above paragraph we have <m>L=F(\alpha)</m>. As <m>f'(x)=1</m>, we see <m>\gcd(f,f')=1</m>, and thus <m>f</m> is separable. Hence <m>F(\alpha)</m> is a Galois extension. Thus there exists a <m>\sigma\in\Gal(L/F)</m> such that <m>\sigma(\alpha)\neq\alpha</m>. So <m>\sigma(\alpha)=\alpha+j</m> for some <m>j\in\Z/p</m>. Notice <m>\sigma(\alpha+j)=\sigma(\alpha)+j=\alpha+2j</m>. As <m>p</m> is prime, we see that <m>|j|=p</m>, and thus we need to apply <m>\sigma</m> to <m>\alpha</m> <m>p</m> times in order to get back to <m>\alpha</m>. Thus <m>\sigma(\alpha)^p=\alpha</m>, so <m>|\sigma|=p</m>. Thus <m>|\Gal(L/F)|=p</m>. Thus the minimum polynomial of <m>\alpha</m> must have degree <m>p</m>. As <m>\alpha</m> is a root of <m>f</m> and <m>f</m> is monic, it must be the minimal polynomial and is thus irreducible.
							</p>

						</paragraphs>

						<paragraphs xml:id="part-b-17">
							<title>Part (b)</title>

							<p>
								Suppose <m>f(x)</m> is irreducible over <m>F</m> and let <m>L</m> be a splitting field of <m>f</m> over <m>F</m>. Let <m>\alpha</m> be a root of <m>f.</m> Consider <m>F(\alpha)</m>. By part (a), <m>F(\alpha)</m> contains all the roots of <m>f</m>, hence <m>L=F(\alpha)</m>. As <m>f</m> is monic and irreducible, it is the minimum polynomial of <m>f</m>, and thus <m>[L:F]=p</m>. Hence <m>|\Gal(L/F)|=p</m>. All groups of prime order are cyclic, completing the proof. #### Problem 9 – Primitive Sixth Root of Unity Consider <m>f(x) = x^6 + 3 \in \Q[x]</m>. (a) Let <m>a</m> be a root of <m>f(x)</m> and prove <m>\Q(a)</m> is a Galois field extension of <m>\Q</m>. (Hint: First show <m>\frac{a^3+1}2</m> is primitive <m>6</m>-th root of unity.) (b) Find the Galois group <m>\Gal(\Q(a)/\Q)</m>.
							</p>

						</paragraphs>
					</paragraphs>

					<paragraphs xml:id="proof.-55">
						<title><em>Proof.</em></title>

						<p>
							Consider <m>f(x) = x^6 + 3 \in \Q[x]</m>. ###### Part (a) Let <m>a</m> be a root of <m>f(x)</m>. Note that <m>(\frac{a^3+1}{2})^2=\frac{a^3-1}{2}</m>, and so <me>(\frac{a^3+1}{2})^6=((\frac{a^3+1}{2})^2)^3=(\frac{a^3-1}{2})^3=1,</me>so yay! It’s primitive. Using one <m>6</m>th primitive root we can obtain all the others, specifically <m>e^{2\pi i/6}</m>. So we multiply each root by this to get all the others. So we have our splitting field.
						</p>


						<paragraphs xml:id="part-b-18">
							<title>Part (b)</title>

							<p>
								Since <m>\Q(a)</m> is Galois, we see that <m>[\Q(a):\Q]=6</m>. So <m>\Gal(\Q(a)/\Q)</m> is either <m>S_3</m> or <m>\Z/6.</m> The roots of <m>f</m> are <m>\alpha_i=\sqrt[6]{3}e^{{(\pi i/6)}^{2i-1}}</m> for <m>1\leq i\leq 6</m>.
							</p>

							<p>
								By the Porism there exists a <m>\sigma\in\Gal(\Q(a)/\Q)</m> such that <m>\sigma(\alpha_1)=\alpha_2</m>. Let <m>\zeta=e^{\pi i/3}</m>, and note that <m>\alpha_i=\alpha_{i-1}\zeta</m>. Additionally, note that <m>\frac{\alpha_i^3+1}{2}=\zeta</m> when <m>i=1,3,5</m> and <m>\frac{\alpha_i^3+1}{2}=\zeta^5</m> when <m>i=2,4,6</m>.
							</p>

							<p>
								Observe then that <me>\sigma(\alpha_2)=\sigma(\alpha_1)\sigma(\zeta)=\alpha_2\frac{\sigma(\alpha_1)^3+1}{2}=\alpha_2\zeta^5=\alpha_1.</me>Similarly, we see that <m>\sigma(\alpha_3)=\alpha_6</m> and <m>\sigma(\alpha_4)=\alpha_5</m>. Thus <m>\sigma</m> corresponds to the permutation <m>(12)(36)(45)</m>.
							</p>

							<p>
								Using the Porism again we see there exists a <m>\tau\in\Gal(\Q(a)/\Q)</m> such that <m>\tau(\alpha_2)=\alpha_3</m>. Using a similar process as above we see that <m>\tau</m> corresponds to <m>(14)(23)(46)</m>. However, observe that <m>\sigma\tau=(153)(264)</m>, while <m>\tau\sigma=(135)(246)</m>. Thus these elements do not commute, so we cannot be in <m>\Z/6</m>. Thus <m>\Gal(\Q(a)/\Q)\cong S_3</m>. #### Problem 6 #unfinished Let <m>\omega</m> be a primitive <m>25</m>th root of unity. (a) Find <m>[\Q(\omega):\Q]</m> and generator(s) for <m>\Gal(\Q(\omega)/\Q))</m>. (b) Draw the subfield lattice for <m>\Q(\omega)</m> and indicate the degrees of each extension. (You do not have to find generators for each of the subfields) #### Problem 6 #unfinished Let <m>L</m> be a finite Galois field extension of <m>\Q</m>. Let <m>E</m> and <m>F</m> be subfields of <m>L</m> such that <m>EF = L, E/\Q</m> is normal, and <m>E\cap F = \Q</m>. Prove that <m>[L : Q] = [E : Q][F : Q].</m> #### Problem 6 #unfinished Let <m>E</m> be the splitting field of the polynomial <m>x^7 − 12</m> over <m>\Q</m>. Find <m>[E : \Q]</m>,<!-- linebreak -->and describe the elements of <m>\Gal(E/\Q)</m> explicitly. #### Problem 6 #unfinished Let <m>K/F</m> be a finite Galois field extension, <m>G = \Gal(K/F)</m>, and <m>n = |G|</m>.<!-- linebreak -->Let <m>\alpha</m> be an element of <m>K</m> and <m>m(x)</m> its minimal polynomial over <m>F</m> ; set<!-- linebreak --><m>d = \deg m(x)</m>.
							</p>

							<p><ol>
								<li>
								Prove there are <m>d</m> distinct elements in the set <m>\{\s(\alpha) | \sigma \in G\}</m>.
								</li>

								<li>
								Prove <me>\prod_{\s\in G} (x-\s(\alpha))=m(x)^{n/d}</me> ##### <em>Proof</em>. Let <m>K/F</m> be a finite Galois field extension, <m>G = Gal(K/F)</m>, and <m>n = |G|</m>.<!-- linebreak -->Let <m>\alpha</m> be an element of <m>K</m> and <m>m(x)</m> its minimal polynomial over <m>F</m> ; set<!-- linebreak --><m>d = \deg m(x)</m>. #### Problem 8 #unfinished Let <m>L/F</m> be an extension of fields and let <m>a, b \in L</m>. Show that <me>\Aut(L/F (a, b)) = \Aut(L/F (a)) \cap \Aut(L/F (b)).</me>
              </li>
            </ol>
          </p>
          </paragraphs>
        </paragraphs>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>

        <section xml:id="sec-ftgt">
          <title>The Fundamental Theorem of Galois Theory</title>
          <paragraphs>
            <title></title>
                <p>
          <m>\defn</m> – Intermediate Field Given a field extension <m>F \subseteq L</m>, an <em><m>\defnn{intermediate field}</m></em> is a subfield <m>E</m> of <m>L</m> that contains <m>F</m>, so that <m>F \subseteq E\subseteq L</m>.
								</p>

						</paragraphs>


				<paragraphs xml:id="prop-intermediate-extensions-are-galois">
					<title><m>\prop</m> – Intermediate Extensions are Galois</title>

					<p>
						If <m>F \subseteq L</m> is a (finite) Galois extension, then so is <m>E \subseteq L</m> for any intermediate field <m>E</m>.
					</p>


					<paragraphs xml:id="proof.-56">
						<title><em>Proof.</em></title>

						<p>
							This is immediate from the definition: If <m>L</m> is the splitting field over <m>F</m> of a separable polynomial <m>f(x) \in F[x]</m>, then <m>L</m> is also the splitting field over <m>E</m> of the same polynomial, now regarded as belonging to <m>E[x]</m>.
						</p>

					</paragraphs>
				</paragraphs>

				<paragraphs xml:id="war">
					<title><m>\war</m></title>

					<p>
						In the setting of the previous Proposition, <m>E</m> need not be Galois over <m>F</m>. For <m>\ex</m>, <m>L = \Q(\sqrt[3]{2}, e^{2 \pi i/3})</m> is Galois over <m>F = \Q</m> but <m>E = \Q(\sqrt[3]{2})</m> is not Galois over <m>\Q</m>, as we noted before.
					</p>

				</paragraphs>

				<paragraphs xml:id="defn-fixed-subfield">
					<title><m>\defn</m> – Fixed Subfield</title>

					<p>
						For a field <m>L</m> and a subgroup <m>G</m> of <m>\Aut(L)</m>[^1], the <m>\defnn{subfield of}</m> <m>L</m> <m>\defnn{fixed by}</m> <m>G</m>, denoted <m>L^G</m>, is by definition <me>
L^G := \{ \a \in L \mid \s(\a) = \a, \text{ for all }\s \in G\}.
</me> <m>L^G</m> really is a subfield of <m>L</m> since <m>L^G = \bigcap_{\s \in G} L^\s</m> where <m>L^\s = \{\a \in L \mid \s(\a) = \a\}</m>, and we proved before that <m>L^\s</m> is a subfield of <m>L</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="thm-fundamental-theorem-of-galois-theory">
					<title><m>\thm</m> – Fundamental Theorem of Galois Theory</title>

					<p>
						Suppose <m>F \subseteq L</m> is a (finite) Galois field extension. Then the functions[^1] <me>\Psi:\left\{\text{intermediate fields $E$, with $F\subseteq E \subseteq L$}\right\} \to\left\{\text{subgroups $H$ of $\Gal(L/F)$}\right\}</me>given by <me>\Psi(E)=\Gal(L/E)</me>is a bijection whose inverse <me>\Psi^{-1}:
\left\{\text{subgroups $H$ of $\Gal(L/F)$}\right\} \to
\left\{\text{intermediate fields $E$, with $F \subseteq E \subseteq L$}\right\}</me>is given by <me>\Psi^{-1}(H)=L^H.</me> Moreover, this bijective correspondence enjoys the following properties: 1. <m>\Psi</m> and <m>\Psi^{-1}</m> each reverse the order of inclusions. 2. <m>\Psi</m> and <m>\Psi^{-1}</m> convert degrees of extensions to indices of subgroups: 1. <m>[\Gal(L/F) : H] = [L^H:F]</m> or, equivalently, 2. <m>[\Gal(L/F) : \Gal(L/E)] = [E:F]</m>. 3. Normal subgroups correspond to intermediate fields that are Galois over <m>F</m>: 1. If <m>N \nsg G</m> then <m>L^N/F</m> is Galois.[^2] 2. If <m>E/F</m> is Galois, then <m>\Gal(L/E) \nsg \Gal(L/F)</m>. 4. If <m>E = L^N</m> for a normal subgroup <m>N \nsg \Gal(L/F)</m>, then <m>\Gal(E/F) \cong \Gal(L/F)/N</m>. 5. If <m>H_1, H_2</m> are subgroups of <m>G</m> with corresponding fixed subfields <m>E_1=L^{H_1}</m> and <m>E_2=L^{H_2}</m>, then 1. <m>E_1\cap E_2=L^{&lt;H_1,H_2&gt;}</m> and <m>\Gal(L/E_1\cap E_2)=\langle H_1,H_2\rangle</m> 2. <m>E_1 E_2=L^{H_1\cap H_2}</m> and <m>\Gal(L/E_1E_2)=H_1\cap H_2</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="cor-galois-correspondence-and-lattices">
					<title><m>\cor</m> – Galois Correspondence and Lattices</title>

					<p>
						The Galois correspondence induces a lattice isomorphism between the lattice of intermediate fields of a Galois extension <m>F \subseteq L</m> and the “dual’’ of the lattice of subgroups of <m>\Gal(L/F)</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="exe-ftgt-and-x4-2">
					<title><m>\exe</m> – FTGT and <m>(x^4-2)</m></title>

					<p>
						Let <m>L</m> be the splitting field of <m>x^4 - 2</m> over <m>\Q</m>. Let’s use the fundamental theorem to list all intermediate fields for <m>\Q \subseteq L</m> and to determine which are Galois over <m>\Q</m>. Notice that without the theorem, it isn’t even obvious that there are only a finite number of such intermediate fields.
					</p>

					<p>
						We know <m>G := \Gal(L/\Q)</m> corresponds to the <m>8</m> element subgroup of <m>S_4</m> generated by <m>\s = (2 \, 4)</m> and <m>\tau = (1 \, 2 \,3 \, 4)</m> where we number the roots as <m>\a_1 = \sqrt[4]{2}, \a_2 = i \a_1, \a_3 = -\a_1, \a_4 =-i\a_1</m>.
					</p>

					<p>
						This group is isomorphic to <m>D_8</m> and we can make this isomorphism explicit by labeling the four corners of a square by <m>\a_1, \dots,\a_4</m>, counter-clockwise. So, <m>\tau</m> is rotation by <m>90</m> degrees and <m>\s</m> is reflection about the line joining vertices <m>1</m> and <m>3</m>.
					</p>

					<p>
						The subgroup lattice and intermediate field lattice are represented below, with normal subgroups and Galois extensions highlighted (boxed).
					</p>

					<p>
						The subgroups are <me>
\begin{aligned}
G &amp;= \langle (2 \, 4), (1 \, 2 \, 3 \,4) \rangle \\
\{e\} &amp; \\
H_1 &amp;= \langle (2 \, 4) \rangle \\
H_2 &amp;= \langle (1 \, 3)\rangle \\
H_3 &amp;= \langle (1 \, 2)(3 \, 4)\rangle \\
H_4 &amp; = \langle (1 \,4)(2 \, 3)\rangle \\
H_5 &amp;= \langle (1 \,3)(2 \, 4)\rangle \\
H_6 &amp; = \langle (1 \, 2 \, 3 \,4) \rangle \\
H_7 &amp;= \langle(1 \, 3), (2 \, 4)\rangle \\
H_8 &amp;= \langle (1 \, 2)(3 \, 4), (1 \,4)(2 \, 3)\rangle \\
\end{aligned}
</me> and the lattices are INSERT IMAGE The intermediate fields are the fixed subfields of <m>L</m> associated to each of these subgroups. In some sense, this answers the question, but let’s find explicit generators for at least some of these.
					</p>

					<p>
						<m>G</m> corresponds to <m>\Q</m> and <m>e</m> corresponds to <m>L = \Q(\a_1, i)</m>.
					</p>

					<p>
						Set <m>E_i = L^{H_i}</m>. <m>E_1</m> has degree <m>4 = [G:H_1]</m> over <m>\Q</m>. It is clear <m>\a_1</m> (and <m>\a_3</m>) belongs to <m>E_1</m> and since <m>[\Q(\a_1):\Q]=4</m>, we must have <m>E_1 = \Q(\a_1)</m>.
					</p>

					<p>
						Likewise <m>E_2 = \Q(\a_2)</m>.
					</p>

					<p>
						<m>E_3</m> also has degree four over <m>\Q</m>. Let <m>\beta = \a_1 + \a_2 = (1+i)\sqrt[4]{2}</m> and note <m>\beta \in E_3</m>. If <m>[\Q(\beta):\Q] = 2</m>, then <m>\beta</m> would be fixed by a subgroup of index <m>2</m> that contains <m>(1 \,2)(3 \, 4)</m>, and the only possibility is <m>H_8</m>. But <m>(1 \, 4)(2 \, 3)</m> sends <m>\beta</m> to <m>\a_4 + \a_3 = - \beta \ne \beta</m>. So we must have <m>[\Q(\beta):\Q] = 4</m> and hence <m>E_3 = \Q(\beta)</m>.
					</p>

					<p>
						I’ll skip the details on <m>E_4</m> and <m>E_5</m>, but they are <m>E_4 =\Q((1-i)\a_1)</m> and <m>E_5 = \Q(\sqrt{2},i)</m>.
					</p>

					<p>
						<m>E_6</m> has degree equal to <m>[G:H_6] = 2</m> over <m>\Q</m> and so we merely need to find a single, non-rational element of <m>L</m> fixed by <m>\tau</m>. Since <m>\tau(i) = i</m> (which can be seen by looking back at how we built <m>\tau</m> originally or by noting that <m>\tau(i) = \tau(\a_2/\a_1) = \a_3/\a_2= i</m>), we get <m>E_6 = \Q(i)</m>.
					</p>

					<p>
						I’ll skip the details on <m>E_7</m>, but it is <m>E_7 = \Q(\sqrt{2})</m>.
					</p>

					<p>
						<m>E_8</m> also has degree two over <m>\Q</m> and so we just need to find a single non-rational element fixed by the two generators of <m>H_8</m>. Note that <m>\a_1\a_2 = \a_3\a_4 = i \sqrt{2}</m> and so <m>i \sqrt{2}</m> is fixed by both <m>(1 \, 2)(3 \, 4)</m> and <m>(1 \,4)(2 \, 3)</m>. Thus <m>E_8 = \Q(i \sqrt{2})</m>.
					</p>

					<p>
						Finally, we note that <m>G, \{e\}, H_5, H_6, H_7, H_8</m> are normal subgroups of <m>D_8</m>, since <m>H_5</m> is the center of <m>D_8</m> and each of <m>H_6, H_7, H_8</m> has index two. Some messy checking reveals these to be the only normal subgroups. It follows from the Fundamental Theorem that <m>\Q, L, E_5, E_6, E_7, E_8</m> are the only intermediate fields that are Galois over <m>\Q</m>. As an example, to see directly that <m>E_3</m> is not Galois over <m>\Q</m>, note that <m>(1+i)\sqrt[4]{2}</m> is a root of <m>x^4 + 4</m>, which is irreducible. But <m>(1-i)\sqrt[4]{2}</m> is also a root of this polynomial and it is not in <m>E_3</m>. We conclude from Proposition  that <m>E_3</m> is not a normal extension of <m>\Q</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="exe-cyclotomic-extensions-revisited">
					<title><m>\exe</m> – Cyclotomic Extensions Revisited</title>

					<p>
						Let <m>F</m> be a field, let <m>n</m> be a positive integer such that <m>\char(F)</m> does not divide <m>n</m>, and let <m>\ov{F}</m> be the algebraic closure of <m>F</m>. If <m>\zeta\ov{F}</m> is a primitive <m>n</m>-th root of 1 over <m>F</m>, then <m>F(\zeta)/F</m> is a finite Galois extension, and <m>\Gal(F(\zeta)/F)</m> is a cyclic group that is isomorphic to a subgroup of <m>(\Z/n,+)</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="rem-10">
					<title><m>\rem</m></title>

					<p>
						If <m>F</m> is a field of prime characteristic <m>p</m> and <m>n</m> is an integer that is divisible by <m>p</m>, then <m>x^n - 1</m> is not separable.
					</p>

				</paragraphs>

				<paragraphs xml:id="exe-two-complex-roots-and-s_n">
					<title><m>\exe</m> – Two Complex Roots and <m>S_n</m></title>

					<p>
						Let <m>f</m> be an irreducible, fifth degree polynomial in <m>\Q[x]</m> with exactly three real roots and let <m>L</m> be its splitting field over <m>\Q</m>. Let the real roots be <m>\a_1, \a_2, \a_3</m> and the imaginary ones be <m>\a_4</m> and <m>\a_5</m>. Then <m>G = \Gal(L/\Q)</m> is isomorphic to <m>S_5</m>. You will prove this on one of the final exam problems (for a specific case), but let me get you started with a couple {}: Show that <m>G</m> is isomorphic to a subgroup <m>H</m> of <m>S_5</m> with <m>5 \mid \#H</m>. You may use (without justifying it) that if <m>\tau</m> is a <m>2</m>-cycle in <m>S_5</m> and <m>\s</m> is any five cycle in <m>S_5</m>, then the subgroup of <m>S_5</m> they generate is all of <m>S_5</m>.
					</p>

				</paragraphs>

				<paragraphs xml:id="exe-finding-unique-intermediate-field">
					<title><m>\exe</m> – Finding Unique Intermediate Field</title>

					<p>
						If <m>F \subseteq L</m> is a finite Galois extension of degree <m>21</m>, then I claim there is a unique intermediate field <m>E</m> with <m>[E:F] = 3</m> and that <m>E</m> must be a Galois extension over <m>F</m>.
					</p>

					<p>
						To see this, set <m>G= \Gal(L/F)</m>. Then <m>\#G = 21 = 3 \cdot 7</m> and by the Sylow theorems, there is a unique Sylow <m>7</m>-subgroup, call it <m>H</m>, and hence <m>H</m> is normal in <m>G</m>. It follows from the Fundamental Theorem that <m>E := L^H</m> is an intermediate field that (a) is Galois over <m>F</m> and (b) satisfies <m>[E: F] = [G :H] = 3</m>. Moreover, it is unique since <m>G</m> has just one subgroup of index <m>3</m>.
					</p>

					<p>
						In fact, there are exactly two groups of order <m>21</m> up to isomorphism, the cyclic one and one that is a (non-trivial) semi-direct product of <m>\Z/7</m> by <m>\Z/3</m>. So, there are just two possible lattices of intermediate fields for such a field extension. #### Problem 4 – All Primitive Generators and Galois Let <m>L</m> be the splitting field over <m>\Q</m> of the polynomial <m>x^3-7</m>. (a) Find all intermediate fields <m>E</m> with <m>\Q \sse E \sse L</m> (including possibly <m>L</m> and <m>\Q</m>) such that <m>E</m> is Galois over <m>\Q</m>. (b) For each field <m>E</m> you found in (a), find with justification a primitive generator (i.e., find <m>\a \in E</m> so that <m>E = \Q(\a))</m>.
					</p>


					<paragraphs xml:id="proof.-57">
						<title><em>Proof.</em></title>

						<p>
							Let <m>L</m> be the splitting field over <m>\Q</m> of the polynomial <m>f(x)=x^3-7</m>
						</p>


						<paragraphs xml:id="part-a-19">
							<title>Part (a)</title>

							<p>
								First, notice that <m>f</m> is irreducible in <m>\Q[x]</m> by Eisenstein’s Criterion <m>(p=7)</m>. Let <m>\z</m> denote a primitive third root of unity. The roots of <m>f</m> are the following: 1. <m>\a_1=\sqrt[3]{7}\z</m>, 2. <m>\a_2=\sqrt[3]{7}\z^2</m>, and 3. <m>\a_3=\sqrt[3]{7}\z^3=\sqrt[3]{7}</m>. As <m>f</m> is irreducible and monic we see that it is the minimum polynomial of <m>\a_3</m> over <m>\Q</m>. Let <m>E_1=\Q(\a_3)</m> and notice <m>[E_1:\Q]=3</m>.
							</p>

							<p>
								Recall that <m>G=\Gal(L/\Q)</m> is isomorphic to a subgroup of <m>S_3</m>. As <m>E_1\sse \R</m> and <m>\a_1\not\in\R</m>, we see another extension is needed, and that extension will have at least degree <m>2</m>. Thus, due to size constraints, we see <m>G\cong S_3</m>.
							</p>

							<p>
								By the FTGT each Galois intermediate extension between <m>\Q</m> and <m>L</m> corresponds to a normal subgroup of <m>G</m>, which are the normal subgroups of <m>S_3</m>.
							</p>

							<p>
								The elements of <m>S_3</m> are the following: 1. <m>(1)</m> 2. <m>(12)</m> 3. <m>(13)</m> 4. <m>(23)</m> 5. <m>(123)</m> 6. <m>(132)</m> The subgroup <m>F=\langle(123)\rangle=\{(123),(132)\}</m> has index <m>2</m> in <m>G</m> and is thus normal. None of the order <m>2</m> subgroups are normal in <m>S_3</m>, so <m>F</m> is the only strictly intermediate extension.
							</p>

							<p>
								Recall <m>[E_1:\Q]=3</m>, meaning <m>|\Gal(L/E_1)|=3</m>, so <m>E</m> corresponds to a subgroup of order <m>2</m> in <m>G</m>, so its not Galois unfortunately. However, <m>E_2=\Q(\z)</m> is a degree <m>2</m> extension that is an intermediate field, as <m>\z</m> is a root of the irreducible polynomial <m>x^2+x+1</m>.
							</p>

							<p>
								With all this in mind, notice: - <m>\Q</m> is a splitting field of <m>\Q</m>, and has the primitive generator by <m>1</m>. - <m>E_2</m> is our only strictly intermediate field, and has the primitive generator <m>\z</m> - Finally, <m>L</m> is Galois over <m>\Q</m>, and has the primitive generator <m>\z+\sqrt[3]{7}</m>.
							</p>

						</paragraphs>

						<paragraphs xml:id="part-b-19">
							<title>Part (b)</title>

						</paragraphs>
					</paragraphs>
				</paragraphs>

				<paragraphs xml:id="problem-8-cubic-polynomial-with-two-complex-roots">
					<title>Problem 8 – Cubic Polynomial with Two Complex Roots</title>

					<p>
						Let <m>F</m> be the splitting field over <m>\Q</m> of the polynomial <m>x^3-2</m>. Prove that the Galois group <m>\Gal(F : \Q)</m> is isomorphic to <m>S_3</m>.
					</p>


					<paragraphs xml:id="proof.-58">
						<title><em>Proof</em>.</title>

						<p>
							Let <m>F</m> be the splitting field over <m>\Q</m> of the polynomial <m>f(x)=x^3-2</m>, the roots of which are: 1. <m>\sqrt[3]2</m> 2. <m>\sqrt[3]2e^{2\pi i/3}</m>, and 3. <m>\sqrt[3]2e^{4\pi i/3}</m>. Using Eisenstein’s Criterion]] with <m>p=2</m> we see that <m>f</m> is irreducible in <m>\Q[x]</m>. As <m>f</m> is monic and irreducible it is the minimum polynomial of <m>\sqrt[3]2\in\Q[x]</m> and <m>[\Q(\sqrt[3]2):\Q]=3</m>. Thus, by the FTGT we know there exists an element of order <m>3</m> in <m>\Gal(F:\Q)</m>.
						</p>

						<p>
							Notice now that <m>f</m> has exactly two complex roots, making complex conjugation correspond to a transposition in <m>S_3</m>. Thus we have an element of order <m>2</m> and an element of order <m>3</m>, so the order of <m>\Gal(F:\Q)</m> must be at least <m>6</m> by Lagrange’s Theorem.
						</p>

						<p>
							As <m>\deg f=3</m> we know <m>\Gal(F:\Q)</m> is isomorphic to a subgroup of <m>S_3</m>, which has order <m>6</m>. Thus <m>\Gal(F : \Q)</m> is isomorphic to <m>S_3</m>.
						</p>

					</paragraphs>
				</paragraphs>

				<paragraphs xml:id="problem-9-cubic-with-two-complex-roots">
					<title>Problem 9 – Cubic with Two Complex Roots</title>

					<p>
						Let <m>f (x) \in \Q[x]</m> be an irreducible cubic (degree <m>3</m>) polynomial having exactly one real root. Let <m>L</m> be the splitting field of <m>f(x)</m> over <m>\Q</m>. Show that <m>\Gal(L/\Q) \cong S_3</m>.
					</p>


					<paragraphs xml:id="proof.-59">
						<title><em>Proof.</em></title>

						<p>
							Let <m>f(x) \in \Q[x]</m> be an irreducible cubic (degree <m>3</m>) polynomial having exactly one real root, and let <m>L</m> be the splitting field of <m>f(x)</m> over <m>\Q</m>.
						</p>

						<p>
							As <m>L</m> is the splitting field of <m>f</m>, it is a normal extension]. As <m>\Q</m> has characteristic 0, <m>L</m> is separable (because <m>L</m> is algebraic extension, as its the extension caused by adjoining each root of <m>f</m>, and algebraic extensions of algebraic extensions are algebraic). Thus by we see <m>|\Aut(L/\Q)|=[L:F]</m>.
						</p>

						<p>
							By Proposition 2.83 we see that <m>\Aut(L/F)</m> is isomorphic to some subgroup of <m>S_m</m>, where <m>m</m> is the number of distinct roots of <m>f</m>. As <m>f</m> cubic and irreducible we know that the real root must be irrational, which we will denote <m>\alpha</m>. Consider the extension <m>\Q(\alpha)</m>. As <m>\alpha\not\in\Q</m>, we see that <m>[\Q(\alpha):\Q]\geq 2</m>. However, neither of our complex roots are in this extension, and so another extension is needed to reach <m>L</m>. But this extension would also have a degree larger than <m>1</m>, so <m>[L:\Q]\geq4</m>. As <m>|S_2|=2</m>, there exists no subgroup of it that <m>\Aut(L/F)</m> can be isomorphic to, given that <m>|\Aut(L/\Q)|=[L:F]</m>. Thus we see that <m>m=3</m>, meaning that our complex roots are distinct.
						</p>

						<p>
							Then <m>|\Aut(L/F)|=1,2,3,</m> or <m>6</m>, the only possible sizes of subgroups of <m>S_3</m>. However, by the previous argument we see that as <m>[L:\Q]\geq4</m>, the only viable subgroup of <m>S_3</m> is <m>S_3</m> itself. Thus <m>\Aut(L/\Q) \cong S_3</m>.
						</p>

					</paragraphs>
				</paragraphs>

				<paragraphs xml:id="problem-9-galois-extensions-of-degree-5-and-10">
					<title>Problem 9 – Galois Extensions of Degree 5 and 10</title>

					<p>
						Let <m>K</m> be a Galois extension of <m>F</m> with <m>|\Gal(K/F)| = 20</m>[^1]. (a) Prove that there exists a subfield <m>E</m> of <m>K</m> containing F with <m>[E : F ] = 5</m>[^2]. (b) Determine whether there must also exist a subfield <m>L</m> of <m>K</m> containing <m>F</m> with <m>[L : F ] = 10</m>.
					</p>


					<paragraphs xml:id="proof.-60">
						<title><em>Proof.</em></title>

						<p>
							Let <m>K</m> be a Galois extension of <m>F</m> with <m>|\Gal(K/F )| = 20</m>.
						</p>


						<paragraphs xml:id="part-a-20">
							<title>Part (a)</title>

							<p>
								Let <m>P</m> be a Sylow <m>2</m>-subgroup of <m>\Gal(K/F)</m>. Notice that <m>|P|=4</m>. By the FTGT there exists an intermediate field extension <m>E</m> such that <m>|\Gal(K/E)|=4</m>. By the Degree Formula, we have <m>[K:F]=[K:E][E:F]</m>, with <m>[K:F]=20</m> and <m>[K:L]=4</m>. Thus <m>[L:F]=5</m>.
							</p>

						</paragraphs>

						<paragraphs xml:id="part-b-20">
							<title>Part (b)</title>

							<p>
								Notice that as <m>2</m> is a prime dividing the order of <m>\Gal(K/F )</m> there must exist an element of order <m>2</m> by Cauchy’s Theorem. The cyclic subgroup, <m>H</m>, generated by this element has order <m>2</m>. By the FTGT there exists an intermediate field extension <m>L</m> such that <m>|\Gal(K/L)|=2</m>. By the Degree Formula, we have <m>[K:F]=[K:L][L:F]</m>, with <m>[K:F]=20</m> and <m>[K:L]=2</m>. Thus <m>[L:F]=2</m>. #### Problem 8 – Galois Group and Transitivity Suppose that <m>F \sse L</m> is a finite Galois extension with Galois group <m>G</m>, and that <m>\a \in L</m>. Prove that <m>L = F (\a)</m> if and only if the images of <m>\a</m> under elements of <m>G</m> are distinct.
							</p>

						</paragraphs>

						<paragraphs xml:id="proof.-61">
							<title><em>Proof</em>.</title>

							<p>
								First, suppose that <m>L=F(\a)</m>. As <m>L</m> is a Galois extension the minimum polynomial of <m>\a</m> in <m>F</m> splits completely into linear factors. Thus <m>G</m> acts faithfully on the roots of <m>\mp_\a(x)</m>, which includes <m>\a</m>. Thus the images of <m>\a</m> under elements of <m>G</m> are distinct.
							</p>

							<p>
								Now suppose that the images of <m>\a</m> under elements of <m>G</m> are distinct, and suppose by way of contradiction that there exists some <m>\b\in L</m> that is not in <m>F(\a)</m>. Consider the intermediate field <m>F(\a,\b)</m>. By the FTGT there exists a nontrivial subgroup of <m>G</m> whose elements fix elements of <m>F(\a,\b)</m>, including <m>\a</m>, a contradiction.
							</p>

						</paragraphs>
					</paragraphs>
				</paragraphs>

				<paragraphs xml:id="problem-5-minimal-polynomial-of-sqrt-2-sqrt-5">
					<title>Problem 5 – Minimal Polynomial of <m>\sqrt{ 2 }+\sqrt{ 5 }</m></title>

					<p>
						Let <m>\a = \sqrt2 + \sqrt5 \in\R</m>. (a) Find the minimum polynomial <m>f(x)</m> of <m>\a</m> over <m>\Q</m>. (b) Let <m>E</m> be the splitting field of <m>f(x)</m> over <m>\Q</m>. Find the Galois group <m>G</m> of <m>E/\Q</m>. (c) Find all subgroups of <m>G</m> and generators for the corresponding intermediate fields of <m>E/\Q</m>.
					</p>


					<paragraphs xml:id="proof.-62">
						<title><em>Proof</em>.</title>

						<p>
							Let <m>\a = \sqrt2+\sqrt5 \in\R</m>.
						</p>


						<paragraphs xml:id="part-a-21">
							<title>Part (a)</title>

							<p>
								Let <me>\begin{align}
f(x)&amp;=(x+\sqrt2 + \sqrt5)(x+\sqrt2 - \sqrt5)(x-\sqrt2 + \sqrt5)(x-\sqrt2 - \sqrt5)\\ &amp;=(x^2-2\sqrt{ 2 }-3)(x^2+2\sqrt{ 2 }-3)\\&amp;=x^4-14x^2+9,
\end{align}</me> which factors as two irreducible polynomials and has no roots in <m>\Q</m>, making it irreducible. Thus <m>f</m> is the minimal polynomial of <m>\a</m>.
							</p>

						</paragraphs>

						<paragraphs xml:id="part-b-21">
							<title>Part (b)</title>

							<p>
								Notice that <m>E=\Q(\sqrt{ 2 },\sqrt{ 5 })</m> which also has degree <m>4</m>. Let <m>G</m> denote the Galois group of <m>E/\Q</m>. Thus <m>G</m> is a group of order <m>4</m>, making it isomorphic to <m>\Z_2\times\Z_2</m> or <m>\Z_4</m>. Notice that the elements of <m>G</m> are the following: - <m>\id(\sqrt{ 2 })=\sqrt{ 2 }</m> and <m>\id(\sqrt{ 5 })=\sqrt{ 5 }</m>, - <m>\tau(\sqrt{ 2 })=\sqrt{ 2 }</m> and <m>\s(\sqrt{ 5 })=-\sqrt{ 5 }</m>, - <m>\g(\sqrt{ 2 })=-\sqrt{ 2 }</m> and <m>\s(\sqrt{ 5 })=-\sqrt{ 5 }</m>, and - <m>\l(\sqrt{ 2 })=-\sqrt{ 2 }</m> and <m>\s(\sqrt{ 5 })=\sqrt{ 5 }</m>. All of these automorphisms have degree <m>2</m>, making <m>G\cong\Z_2\times\Z_2</m>.
							</p>

						</paragraphs>

						<paragraphs xml:id="part-c-3">
							<title>Part (c)</title>

							<p>
								Thus are only two subgroups of <m>G</m>, <m>H\cong\Z_2</m> and <m>K\cong\Z_2</m>. The first, <m>H</m>, corresponds to <m>\Q(\sqrt{ 2 })</m> and is generated by <m>\sqrt{ 2 }</m>, where <m>K</m> corresponds to <m>\Q(\sqrt{ 5 })</m> and is generated by <m>\sqrt{ 5 }</m>. #### Problem 8 – Quartic and Unique Galois Extension Let <m>L</m> be the splitting field of <m>x^4-2022</m> over <m>\Q</m>. Prove there exists a unique intermediate field <m>\Q\subseteq K\subseteq L</m> such that <m>[K : \Q] = 4</m>[^1] and <m>\Q\subseteq K</m> is a Galois extension.
							</p>

						</paragraphs>

						<paragraphs xml:id="proof.-63">
							<title><em>Proof</em>.</title>

							<p>
								Let <m>L</m> be the splitting field of <m>f(x)=x^4-2022</m> over <m>\Q</m>. Let <m>\z</m> be a primitive fourth root of unity. Thus <m>\z=e^{2\pi i/4}=e^{\pi/2}=\cos(\pi/2)+i\sin(\pi/2)=i</m>.
							</p>

							<p>
								Notice that the roots of <m>f</m> are the following: - <m>\sqrt[4]{2022} i</m>, - <m>\sqrt[4]{2022}i^2=-\sqrt[4]{2022}</m>, - <m>\sqrt[4]{2022}i^3=-\sqrt[4]{2022}i</m>, and - <m>\sqrt[4]{2022}i^4=\sqrt[4]{2022}</m>. Thus <m>L=\Q(\sqrt[4]{2022},i)</m>.
							</p>

							<p>
								Using Eisenstein’s Criterion]] with <m>p=2</m> we see that <m>f</m> is irreducible in <m>\Q[x]</m>. Let <m>\a=\sqrt[4]{2022}</m>, and notice that <m>f</m> is the minimum polynomial of <m>\a</m>. Let <m>K=\Q(\a)</m> and observe <m>[K:\Q]=4</m>.
							</p>

							<p>
								As <m>i</m> is the root of the monic irreducible polynomial <m>x^2+1</m> we have <m>[L:F]=2</m> and <m>[L:\Q]=[L:F][F:\Q]=2\cdot4=8</m>. Thus <m>G=\Gal(L/\Q)</m> is isomorphic to a subgroup of <m>S_4</m> of order <m>8</m>, making it <m>D_8</m>.
							</p>

							<p>
								Notice that <m>K</m> is an extension of degree <m>24</m>, and thus by the FTGT we have a subgroup <m>H\leq G</m> such that <m>H=\Gal(L/K)</m>. As <m>[L:\Q]=[L:K][K:\Q]</m> we have <m>8=[L:K]\cdot 4</m>, and thus <m>|H|=2</m>. This makes <m>H</m> the cyclic subgroup of <m>D_8</m> generated by a reflection, the only element of order <m>2</m> in <m>D_8</m>, making it unique. This is also a normal subgroup of <m>D_8</m>, making <m>K</m> Galois over <m>\Q</m>.
							</p>

						</paragraphs>
					</paragraphs>
				</paragraphs>

				<paragraphs xml:id="problem-9-quintic-with-two-complex-roots">
					<title>Problem 9 – Quintic with Two Complex Roots</title>

					<p>
						Consider <m>f (x) = x^5 -20x - 2 \in \Q[x].</m> This polynomial has exactly three real roots, a fact that you may use without proof.
					</p>

					<p><ol>
						<li>
						Show that <m>f</m> is irreducible in <m>\Q[x]</m>.
						</li>

						<li>
						Let <m>L</m> be a splitting field of <m>f</m> $over <m>\Q</m>. Show that <m>L/\Q</m> is a Galois extension and find the isomorphism class of the Galois group <m>\Gal(L/\Q)</m>.
						</li>

					</ol></p>


					<paragraphs xml:id="proof.-64">
						<title><em>Proof.</em></title>


						<paragraphs xml:id="part-a-22">
							<title>Part (a)</title>

							<p>
								Notice that <m>p=2</m> is prime in <m>\Q[x]</m>, and thus <m>f(x)</m> is irreducible in <m>\Q[x]</m> by Eisenstein’s Criterion.
							</p>

						</paragraphs>

						<paragraphs xml:id="part-b-22">
							<title>Part (b)</title>

							<p>
								Let <m>\alpha</m> be a real root of <m>f(x)</m>. As <m>f</m> is irreducible this root is not in <m>\Q</m>. As <m>\alpha</m> is the root of a monic irreducible polynomial of degree <m>5</m>, we see that <m>[\Q(\alpha):\Q]=5</m>. By the FTGT there exists a subgroup of <m>\Gal(L/\Q)</m> with order <m>5</m>, making it a cyclic subgroup generated by some element of order <m>5</m>.
							</p>

							<p>
								However, as <m>f</m> only has <m>2</m> complex roots we see that complex conjugation corresponds to an element of order <m>2</m> in <m>\Gal(L/\Q)\cong H\leq S_5</m>. Thus we have a transposition and a <m>5</m>-cycle, meaning we can generate all of <m>S_5</m>. <m>\Gal(L/\Q)\cong S_5</m>.
							</p>

						</paragraphs>
					</paragraphs>
				</paragraphs>

				<paragraphs xml:id="problem-9-quintic-with-two-complex-roots-1">
					<title>Problem 9 – Quintic with Two Complex Roots</title>

					<p>
						Consider <m>f (x) = x^5-4x-2 \in \Q[x]</m>. This polynomial has exactly three real roots, a fact that you may use without proof.
					</p>

					<p><ol>
						<li>
						Show that <m>f</m> is irreducible in <m>\Q[x]</m>.
						</li>

						<li>
						Let <m>L</m> be a splitting field of <m>f</m> over <m>\Q</m>. Show that <m>L/\Q</m> is a Galois extension with Galois group <m>\Gal(L/\Q)</m> isomorphic to the symmetric group <m>S_5</m>.
						</li>

					</ol></p>


					<paragraphs xml:id="proof.-65">
						<title><em>Proof.</em></title>

						<p>
							Consider <m>f (x) = x^5-4x-2 \in \Q[x]</m>.
						</p>


						<paragraphs xml:id="part-a-23">
							<title>Part (a)</title>

							<p>
								Using Eisenstein’s Criterion with <m>p=2</m> we see that <m>f</m> is indeed irreducible in <m>\Q[x]</m>.
							</p>

						</paragraphs>

						<paragraphs xml:id="part-b-23">
							<title>Part (b)</title>

							<p>
								Let <m>L</m> be a splitting field of <m>f</m> over <m>\Q</m>. As <m>f</m> is monic and irreducible, it is the minimum polynomial for some <m>\a\in L</m> such that <m>[\Q(\a):\Q]=5</m>. As <m>f</m> has complex roots, we know that <m>\Q(\a)\neq L</m>, and is thus an intermediate field. By the FTGT there exists a subgroup <m>H</m> of <m>\Gal(L/\Q)</m> such that <m>H=\Gal(\Q(\a)/\Q)</m>. As <m>[\Q(\a):\Q]=5</m> we know <m>|H|=5</m>, making <m>H=\langle\s\rangle</m> for some <m>\s\in H</m>. Thus <m>\s</m> is an element of order <m>5</m> in <m>\Gal(L/\Q)</m>. As <m>f</m> is a degree <m>5</m> polynomial, we know <m>\Gal(L/\Q)</m> is isomorphic to a subgroup of <m>S_5</m>. Thus <m>\s</m> must be a <m>5</m>-cycle.
							</p>

							<p>
								Recall that <m>f</m> has exactly two complex roots. Thus <m>\tau</m>, the complex conjugation automorphism, has order <m>2</m>, making it a transposition. From the Gospel of Mark we were told that we did not need to prove that a transposition and an <m>n</m>-cycle generate all of <m>S_n</m>, and thus <m>\Gal(L/\Q)\cong S_5</m> #### Problem 4 – Unique Intermediate Field of Degree 5 Let <m>L/F</m> be a finite Galois field extension of degree <m>45</m>. Prove there exists a unique intermediate field <m>E</m> (i.e., <m>F \sse E \sse L)</m> such that <m>[E : F ] = 5</m>[^1].
							</p>

						</paragraphs>
					</paragraphs>

					<paragraphs xml:id="proof.-66">
						<title><em>Proof.</em></title>

						<p>
							Let <m>F \subseteq L</m> be a finite Galois field extension of degree <m>45</m>.
						</p>

						<p>
							Note that as <m>45=5\cdot 9</m>, we see that the number of Sylow-<m>5</m> subgroups of <m>G</m> divides <m>9</m> and is congruent to <m>1\mod{5}</m>. Thus there is exactly one Sylow-<m>5</m> subgroup of <m>G</m>, which we denote <m>H</m>. By the Fundamental Theorem of Galois Theory, <m>H</m> corresponds to an intermediate field extension <m>E</m>. Note that as <m>H</m> has order 5, we see that <m>[G:H]=9</m>, and thus <m>[L:E]=9</m> as well. By the Degree Formula we see that <m>[L:F]=[L:E][E:F]</m>. As <m>[L:F]=45</m> and <m>[L:E]=9</m>, we see that <m>[E:F]=5</m>, as desired. As <m>E</m> corresponds to the unique subgroup of <m>G</m> or order 5, we see that this extension must be unique as well.
						</p>

					</paragraphs>
				</paragraphs>

				<paragraphs xml:id="problem-2">
					<title>Problem</title>

					<p>
						Let <m>L</m> be the splitting field over <m>\Q</m> of the polynomial <m>x^3 - 7</m>. (a) Find all intermediate fields <m>E</m> with <m>\Q \subseteq E \subseteq L</m> (including possibly <m>L</m> and <m>\Q</m>) such that <m>E</m> is Galois over <m>\Q</m>. (b) For each field <m>E</m> you found in (a), find with justification a primitive generator (i.e., find <m>\alpha \in E</m> so that <m>E = \Q(\alpha)</m>).
					</p>


					<paragraphs xml:id="proof.-67">
						<title><em>Proof.</em></title>


						<paragraphs xml:id="part-a-24">
							<title>Part (a)</title>

							<p>
								Invoking Eisenstein’s Criterion and setting <m>p=7</m> we see that <m>x^3 - 7</m> is an irreducible cubic with exactly one real root. Thus <m>\Gal(L/\Q)\cong S_3</m>. Thus the subgroups of <m>\Gal(L/\Q)</m> are as follows: <m>H_1\cong\langle(12)\rangle, H_2\cong\langle(13)\rangle, H_3\cong\langle(23)\rangle,</m> and <m>H_4\cong\langle(123)\rangle.</m> By the Fundamental Theorem of Galois Theory there exist four intermediate field extensions: <m>E_1=L^{H_1}, E_2=L^{H_2}, E_3=L^{H_3}</m>, and <m>E_4=L^{H_4}</m>. Note that as <m>H_1,H_2,</m> and <m>H_3</m> are all Sylow-2 subgroups of <m>S_3</m>, by Sylow’s Theorem none are normal in <m>S_3</m>. However, as <m>[G:H_4]=2</m>, the smallest prime dividing 6, we see that <m>E_4</m> is the only strictly intermediate field that is Galois over <m>\Q</m>.
							</p>

							<p>
								As the identity map is the only automorphism from <m>\Q\to\Q</m> that fixes <m>\Q</m> and <m>[\Q:\Q]=1</m>, we see that <m>\Q</m> is Galois over itself. As all finite extensions of fields with characteristic 0 are separable, we see that <m>L</m> is also Galois over <m>\Q</m>. Thus <m>\Q, L,</m> and <m>E_4</m> are the only intermediate fields <m>\Q \subseteq E \subseteq L</m> that are Galois over <m>\Q</m>. #### Problem 5 #unfinished Let <m>L</m> be the splitting field of the polynomial <m>x^7-18</m> over <m>\Q</m>. Give, with full justification, a presentation for the Galois group <m>\Gal(L/\Q)</m> that has two generators. #### Problem 5 #unfinished Let <m>\zeta</m> denote a primitive <m>17</m>th root of unity, over <m>\Q</m>. (a) Prove that <m>\Q(\zeta)/\Q</m> is a Galois extension, of degree <m>16</m> (b) Describe an explicit isomorphism <m>\Z/16 \to\Gal(\Q(\zeta)/\Q).</m> (c) Describe a primitive generator for the fixed subfield of the subgroup <m>\Z/4</m> of <m>\Gal(\Q(\zeta)/\Q)</m>.
							</p>

						</paragraphs>
					</paragraphs>

					<paragraphs xml:id="proof.-68">
						<title><em>Proof</em>.</title>


						<paragraphs xml:id="part-a-25">
							<title>Part (a)</title>

							<p>
								Notice <m>\zeta</m> is a root of the polynomial <m>x^{17}-1</m>, and we can factor out an <m>(x-1)</m> to see that <m>\zeta</m> is a root of the cyclotomic polynomial <m>f(x)=x^{16}+x^{15}+\dots+x+1</m>. Thanks to Mark, this is a monic irreducible polynomial in <m>\Q[x]</m>, making <m>f</m> the minimal polynomial of <m>\zeta</m>, and thus the degree of the extension is <m>16</m>.
							</p>


            </paragraphs>
          </paragraphs>
        </paragraphs>

          <exercises>

            <exercisegroup>
              <title>Computations and Examples</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          
            <exercisegroup>
              <title>Qualifying Exam Problems</title>
              <introduction>
                <p>
                </p>
              </introduction>
          
              <exercise>
                <title></title>
                
                
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
  
                <hint>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </hint>
  
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
          
            </exercisegroup>
          </exercises>
          
        </section>
        
      </chapter>

      <chapter xml:id="ch-field-extras">
        <title>Extras</title>

        <section xml:id="sec-solvable-by-radicals">
          <title>Solvability by Radicals</title>

          <p></p>
          
        </section>

        <section xml:id="sec-transcendental-elements">
          <title>Transcendental Elements</title>

          <p></p>
          
        </section>

        <section xml:id="sec-advanced-galois">
          <title>Advanced Galois Theory</title>

          <p></p>
          
        </section>

        <section xml:id="sec-inverse-galois">
          <title>The Inverse Galois Problem</title>

          <p></p>
          
        </section>
        
      </chapter>

    </part>

<!--
    <part xml:id="part-representation">
      <title>Representation Theory</title>
      
      <chapter xml:id="ch-grouprings">
        <title>Group Rings and Representations</title>
        
        <section xml:id="sec-representations">
          <title>Representations</title>

          <definition xml:id="def-representation">
            <statement>
              <p>
                Let <m>G</m> be a group. A <em>representation</em> of <m>G</m> over a field <m>K</m> is a <m>K</m>-vector space <m>V</m> equipped with a group homomorphism <m>\rho:G\to \operatorname{Aut}_K(V)</m>. More generally, a representation of <m>G</m> over a ring <m>R</m> is an <m>R</m>-module <m>V</m> equipped with group homomorphism <m>\rho:G\to \operatorname{Aut}_R(V)</m>. We may also say that <m>G</m> <em>acts linearly</em> on <m>V</m>.
              </p>
            </statement>
          </definition>

          <remark>
            <p>
              
            We can think of this data in a number of different ways.
				</p>

				<p><ol>
					<li>
									<p>
					Given a representation <m>(V,\rho)</m>, the map 
          <me>
            xy matrix
</me> satisfies the properties
				</p>

				<p><ol>
					<li>
									<p>
					<m>e \cdot v= v</m>
				</p>
					</li>

					<li>
									<p>
					<m>gh \cdot v = g \cdot (h \cdot v)</m>
				</p>
					</li>

					<li>
									<p>
					<m>g \cdot (v+w)=(g \cdot v) + (g \cdot w)</m>
				</p>
					</li>

					<li>
									<p>
					<m>g \cdot rv = r (g\cdot v)</m>,
				</p>
					</li>

				</ol></p>

				<p>
					In particular, the first two conditions say that <m>G</m> acts on <m>V</m> in the sense of group action on a set, and the last two say that the action of any element is by an <m>R</m>-linear map. Conversely, any such function <m>\psi</m> yields a representation <m>(V,\rho)</m>.
				</p>
					</li>

					<li>
									<p>
					If <m>V=R^n</m> is free, then <m>\operatorname{Aut}_R(V) \cong \operatorname{GL}_n(R)</m>, where <m>\operatorname{GL}_n(R)</m> is the group of <m>n\times n</m> invertible matrices with entries in <m>R</m>. By a slight abuse of notation, we will say that a group homomorphism <m>G\to \operatorname{GL}_n(V)</m> is a representation of <m>G</m>.
				</p>
					</li>

				</ol></p>
          </remark>

          <example>
            <p><ol>
              <li>
                      <p>
              For any group <m>G</m>, and any <m>R</m>-module <m>V</m>, there is the <em>trivial representation</em> <m>\rho:G\to \operatorname{Aut}_R(V)</m> where <m>\rho(g)=1_V</m> for all <m>g\in G</m>. In this action, every element acts trivially on <m>M</m>.
            </p>
              </li>
    
              <li>
                      <p>
              Any representation on <m>V = R</m> is determined by specifying a group homomorphism <m>\rho:G \to \operatorname{Aut}_R(R)\cong R^\times</m>.
            </p>
    
            <p>
              For example, if <m>G = C_n=\langle g\rangle</m> (the multiplicative cyclic group of order <m>n</m>) and <m>R = \mathbb{C}</m>, there are <m>n</m> possible such homomorphisms, determined by <m>\rho(g)=e^\frac{2\pi k i}{n}</m> where <m>0 \leq k \leq n-1</m>.
            </p>
    
            <p>
              Another important example of a rank 1 representation is the <em>sign representation</em> of the symmetric group <m>S_n</m>, given by the group homomorphism which assigns to each permutation its sign, regarded as an element of the arbitrary ring <m>R</m>.
            </p>
              </li>
    
              <li>
                      <p>
              The symmetric group <m>S_n</m> acts on a free <m>R</m>-module with basis <m>b_1,\dots,b_n</m> by permuting coordinates: <m>\rho(\sigma)(b_i) = b_{\sigma(i)}</m>. For a concrete example, <m>S_3</m> acts on <m>{\mathbb{R}}^3</m>, where, for example <m>(132) \cdot (a_1,a_2,a_3) = (a_2,a_3,a_1)</m>.
            </p>
              </li>
    
              <li>
                      <p>
              Let <m>G = D_{2n}</m>, symmetries of the equilateral polygon on <m>n</m> vertices. Then <m>G</m> acts linearly on <m>V={\mathbb{R}}^2</m> by rotations and reflections. If <m>G</m> is generated by <m>r</m> (rotation by <m>2\pi/n</m>) and <m>l</m> (reflection about the <m>y</m>-axis), then the associated group homomorphism <m>\rho: G \to\operatorname{GL}_2({\mathbb{R}})</m> maps <me>\rho(r)=\begin{bmatrix}
          \cos(2 \pi/n) &amp; -\sin(2 \pi/n)  \\ \sin(2 \pi/n)  &amp; \cos(2 \pi/n)  \end{bmatrix}
          \qquad
    \rho(l)=\begin{bmatrix}
          -1  &amp; 0 \\ 0 &amp; 1 \end{bmatrix}.</me>
            </p>
              </li>
    
              <li>
                      <p>
              Let <m>R=K</m> be a field,<m>V =K^2</m>, and let <m>G=(K,+)</m>. We see that the assignment <me>\rho:G\to  \operatorname{GL}_2(K) \quad 
    \rho(\lambda)=\begin{bmatrix} 
    1 &amp; 0  \\ \lambda &amp; 1
    \end{bmatrix}</me> is a representation. In particular, if <m>K=\mathbb{F}_p</m>, this is a representation of <m>C_p</m>.
            </p>
              </li>
    
            </ol></p>
          </example>

          <definition xml:id="def-equivariant-map">
            <statement>
              <p>
                If <m>\rho:G \to \operatorname{Aut}_R(V)</m> and <m>\omega:G\to \operatorname{Aut}_R(W)</m> are <m>R</m>-linear representations of <m>G</m> on <m>V</m> and <m>W</m> respectively then a <em><m>G</m>-equivariant map</em> from <m>V</m> to <m>W</m> is an <m>R</m>-module homomorphism <m>f: V \to W</m> such that <m>f(gv)=gf(v)</m> for all <m>v\in V</m>. Equivalently the following diagram commutes: 
                <me>
                  xy matrix
                </me>
              </p>
            </statement>
          </definition>

          <definition xml:id="def-stable-mod">
            <statement>
              <p>
                If <m>\rho:G \to \operatorname{Aut}_R(V)</m> is a representation, a submodule <m>W\leq V</m> is <em><m>G</m>-stable</em> if <m>\rho(g)(W)\subseteq W</m> for all <m>g\in G</m>.
				</p>
            </statement>
          </definition>
    
          <example>
            <p>
              For <m>G=S_n</m> acting by permuting a basis as above, <m>\{(\lambda,\dots,\lambda) \ | \ \lambda\in K\}</m> and <me>\{(\lambda_1,\dots,\lambda_n) \ | \ \lambda_1+\cdots+\lambda_n=0\}</me> are stable subspaces.
            </p>
          </example>

          <example>
            <p>
              For <m>G=(K,+)</m> acting on <m>K^2</m> as above, <m>\{(0,\lambda) \ | \ \lambda\in K\}</m> is a stable subspace.
            </p>
          </example>

          <proposition xml:id="prop-equimaps-category">
            <statement>
              <p>
                Fix a group <m>G</m> and a ring <m>R</m>. The collection of left <m>R</m>-linear representations of <m>G</m> and <m>G</m>-equivariant maps between them forms a category which we will denote <m>\mathbf{Rep}_{R}(G)</m>.
              </p>
            </statement>
          </proposition>

          <problem>
            <p>
              Prove that if <m>\varphi:G\to GL(V)</m> is any representation, then <m>varphi</m> gives a faithful representation of <m>G/\ker\varphi</m>.
            </p>
          </problem>

          <problem>
            <p>
              Let <m>\varphi:G\to GL_n(F)</m> be a matrix representation. Prove that the map <m>g\mapsto\det(\varphi(g))</m> is a
              degree <m>1</m> representation.
            </p>
          </problem>

          <problem>
            <p>
              Prove that the degree <m>1</m> representations of <m>G</m> are in bijective correspondence with the degree
              <m>1</m> representations of the abelian group <m>G/G'</m> (where <m>G'</m> is the commutator subgroup of <m>G</m>).
            </p>
          </problem>

          <problem>
            <p>
              Let <m>V</m> be a (possibly infinite dimensional) <m>FG</m>-module (<m>G</m> is a finite group). Prove that for each <m>v\in V</m> there is an <m>FG</m>-submodule containing <m>v</m> of dimension <m>/leq|G|</m>.
            </p>
          </problem>

          <problem>
            <p>
              Prove that if <m>|G|>1</m> then every simple <m>FG</m>-module has dimension less than <m>|G|</m>.
            </p>
          </problem>

        </section>

        <section xml:id="sec-group-rings-and-modules">
          <title>Group Rings and Modules</title>

          <definition xml:id="def-groupring">
            <statement>
              <p>
                For any ring <m>R</m> and group <m>G</m>, we define the <em>group ring</em> <m>R[G]</m> as follows: As a set, <m>R[G]</m> is the free left <m>R</m>-module with basis <m>G</m>; that is, 
                <me>
                  R[G]=\left\{ \sum_g r_g g \mid  r_g = 0_R \text{ for all by a finite number of }g's \right\}.
                </me> 
                We define addition as module addition; that is, 
                <me>
                  \left(\sum_g r_g g \right) + \left(\sum_h s_h h \right) = \sum_{f \in G} \left(r_f + s_f \right) f.
                </me> 
                Multiplication is the unique pairing that obeys the distributive laws and is such that <m>R</m> is a subring, <m>1_RG</m> is a subgroup of <m>(R[G]^\times, \cdot)</m>, and every element of <m>R</m> commutes with every element of <m>G</m>. In general, we have 
                <me>
                  \left(\sum_g r_g g\right) \cdot \left(\sum_h s_h h \right) = \sum_{f \in G} \left(\sum_{\substack{(g,h) \in G \times G\\ gh= f}} r_gs_h\right)   f.
                </me> 
                where the inner sum is over pairs of group elements whose product is <m>f</m>.
              </p>
            </statement>
          </definition>

          <remark>
            As a matter of notation, the element <m>1_R g</m> will be written as just <m>g</m> and the element <m>r e_G</m> as just <m>r</m>, so that we will regard <m>G</m> and <m>R</m> as subsets of <m>R[G]</m>. They overlap in the one element <m>1_R e_G</m> which will be written as just <m>1</m>.
          </remark>

          <remark>
            When <m>R</m> is commutative (in particular when <m>R</m> is a field), <m>R[G]</m> is an <m>R</m>-algebra called the <em>group <m>R</m>-algebra</em> of <m>G</m>.
          </remark>

          <problem>
            <p>
              For any ring <m>R</m> and <m>G = C_n</m>, prove there is a ring isomorphism 
              <me>
                R[C_n] \cong R[x]/(x^n -1).
              </me>
            </p>
          </problem>

          <proposition xml:id="prop-ump-groupring">
            <title>Universal Mapping Property of Group Rings</title>
            
            <statement>
              <p>
                Let <m>R, A</m> be rings and <m>G</m> a group. Given a ring homomorphism <m>\iota: R \to A</m> and a group homomorphism <m>f: G \to (A^\times, \cdot)</m>, such that for every <m>r\in R, g\in G</m> we have that <m>\iota(r)</m> and <m>f(g)</m> commute in <m>(A,\cdot)</m>, there is a unique ring homomorphism <m>\alpha: R[G] \to A</m> such that <m>\alpha|_R = \iota</m> and <m>\alpha|_G = f</m>. Explicitly, <m>\alpha</m> is given by 
                <me>
                  \alpha\left(\sum_g r_g g\right) = \sum_g \iota(r_g) f(g).
                </me>
              </p>
            </statement>

            <proof>
              <p>
                Most of this follows from noticing that <m>R[G]</m> is a coproduct. Indeed, we can vie <m>R[G]</m> as an internal direct sum <m>R[G]=\bigoplus_{g\in G} Rg</m> and hence it is the coproduct for the family <m>\{Rg\}_{g\in G}</m> where each <m>Rg\cong R</m>. For each <m>g\in G</m> set up an <m>R</m>-module homomorphism <m>f_g: Rg\to A</m> by mapping <m>f_g(r_gg)=\iota(r_g)f(g)</m>. Then the definition of coproduct gives a unique <m>R</m>-module homomorphism <me>\alpha: R[G]=\bigoplus_{g\in G} Rg \to A \text{ such that } \alpha|_{Rg}=f_g.</me> From the way we defined the maps <m>f_g</m> we can deduce that <m>\alpha|_R = \iota</m> and <m>\alpha|_G = f</m> and <me>\alpha\left(\sum_g r_g g\right) = \sum_g \iota(r_g) f(g).</me> It remains to check that this map is in fact a ring homomorphism, i.e. it preserves multiplication. This can be done using the formula for <m>\alpha</m> above and the fact that <m>\iota(R)</m> and <m>f(G)</m> commute in <m>A</m>.
              </p>
            </proof>
          </proposition>

          <remark>
            If we assumed that <m>A</m> is an <m>R</m>-algebra in the proposition above, then we would not need the commutativity condition as <m>\iota(R)</m> is in the center of <m>A</m> so it commutes with everything.
          </remark>

          <lemma xml:id="lem-repsmods">
            <statement>
              <p>
                Let <m>R</m> be a ring, <m>V</m> a left <m>R</m>-module, and <m>G</m> a group. There is a bijection <me>xy matrix</me>
              </p>
              <p>
                Moreover, if <m>V</m> and <m>W</m> are representations, then <m>\psi:V\to W</m> is <m>G</m>-equivariant if and only if it is <m>R[G]</m>-linear.
              </p>

            </statement>

            <proof>
              <p>
                Given an <m>R[G]</m>-module structure on <m>V</m>, for every <m>g\in G</m>, there is a map <m>m_g: V\to V</m> given by <m>v\mapsto g\cdot v</m>. We have <m>m_g  (rv) = g (rv) = rg (v) = r m_g(v)</m>, so <m>m_g</m> is <m>R</m>-linear. Moreover, the map <m>\rho:G\to \operatorname{End}_R(V)</m> that sends <m>g\mapsto m_g</m> preserves multiplication and identity: <m>\rho(gh)(v) = gh v = g(hv) = \rho(g) \rho(h) (v)</m> and <m>\rho(e)(v) = v</m>. Thus, we obtain an <m>R</m>-linear representation <m>\rho:G\to \operatorname{Aut}_R(V)</m>.
              </p>

              <p>
                Conversely, recall that a module structure on an abelian group is equivalent to a ring homomorphism to its endomorphism ring over <m>\mathbb{Z}</m>. Given a representation <m>\rho:G\to \operatorname{Aut}_R(V)</m> by considering <m>\operatorname{Aut}_R(V)\subseteq \operatorname{End}_\mathbb{Z}(V)</m> we get a group homomorphism <m>f</m> to the unit subgroup of <m>\operatorname{End}_\mathbb{Z}(V)</m>. The action of <m>R</m> on <m>V</m> gives a ring homomorphism <m>\iota: R \to \operatorname{End}_\mathbb{Z}(V)</m>. For <m>r\in R</m> and <m>g\in G</m>, we have <me>(f(g) \circ \iota(r))(v) = f(g)(rv) = \rho(g)(rv) = r \rho(g)(v) = (\iota(r) \circ f(g))(v)</me> for all <m>v\in V</m>. Thus, by the universal property, we get a well-defined ring homomorphism <m>R[G] \to \operatorname{End}_\mathbb{Z}(V)</m>, and hence an <m>R[G]</m>-module structure, which is easily seen to follow the formula above.
              </p>
      
              <p>
                We leave the final claim as an exercise.
              </p>
            </proof>
          </lemma>

          <remark>
            We can think of these bijections as yielding mutually inverse functors <m>F:\mathbf{Rep}_{R}(G) \to R[G]-\mathbf{Mod}</m> and <m>F^{-1}:R[G]-\mathbf{Mod} \to \mathbf{Rep}_{R}(G)</m>.
          </remark>
        </section>

        

      </chapter>

      <chapter xml:id="ch-simple-modules">
        <title>Simple Modules</title>

        <section xml:id="sec-simple-modules">
          <title>Simple Modules</title>
          

          <p>
            Now we proceed to discuss some smallness conditions on modules. The first key notion is that of a simple module. Simple modules are the atoms in module theory.
          </p>

          <definition xml:id="def-simple-module">
            <statement>
              <p>
                An <m>R</m>-module <m>M</m> is <em>simple</em> if there are no nonzero proper submodules of <m>M</m>.
              </p>
            </statement>
          </definition>

          <lemma xml:id="lem-equiv-simple-mod">
            <statement>
              <p>
                Let <m>M</m> be a nonzero <m>R</m>-module. The following are equivalent:
                <ol>
                  <li>
                    <p>
                      <m>M</m> is simple
                    </p>
                  </li>

                  <li>
                    <p>
                      <m>Rm= M</m> for all <m>m\in M\smallsetminus 0</m>
                    </p>
                  </li>

                  <li>
                    <p>
                      <m>M\cong R/I</m> for some maximal left ideal <m>I</m>.
                    </p>
                  </li>
                </ol>
              </p>
            </statement>

            <proof>
              <p>
                For a left ideal <m>I</m>, the submodules of <m>R/I</m> are in bijective correspondence with the left <m>R</m>-submodules of <m>R</m> that contain <m>I</m>, i.e., the left ideals that contain <m>I</m>. It is then clear that if <m>I</m> is a maximal left ideal, then <m>R/I</m> is simple, so (3) implies (1). On the other hand, if <m>M</m> is simple then it is cyclic (since (1) implies (2)), so <m>M\cong R/I</m> for some left ideal <m>I</m>, and if <m>I \subsetneqq J</m> for some proper left ideal <m>J</m>, then <m>0 \neq J/I \subsetneqq R/I</m>; thus (1) implies (3).
              </p>
            </proof>
          </lemma>

          <example>
            <ol>
              <li>
                <p>
                  If <m>K</m> is a field, a <m>K</m>-vector space is simple if and only if it is <m>1</m>-dimensional. Moreover, if <m>R</m> is a <m>K</m>-algebra, then any <m>R</m>-module that is <m>1</m>-dimensional as a vector space is a simple <m>R</m>-module as well.
                </p>
              </li>

              <li>
                <p>
                  If <m>R</m> is commutative, then an <m>R</m>-module <m>M</m> is simple if and only if <m>M</m> is isomorphic to a field.
                </p>
              </li>

              <li>
                <p>
                  Let <m>R={\mathbb{R}}[D_{2n}]</m>, and <m>V</m> be the natural <m>2</m>-dimensional representation by reflections and rotations. Then <m>V</m> is a simple <m>R</m>-module, since there are no <m>D_{2n}</m>-stable subspaces.
                </p>
              </li>

              <li>
                <p>
                  Let <m>K</m> be a field, or more generally a division ring, and let <m>R=M_n(K) \cong \operatorname{End}_K(K^n)</m>. The module <m>M=K^n</m> of column vectors is a simple <m>R</m>-module Indeed, if <m>v=(a_1,\dots,a_n)\neq 0</m>, say <m>a_i\neq 0</m>; then <m>a_i^{-1} E_{ij} v = e_i \in M</m>, and since <m>M</m> is generated by the standard vectors <m>e_i</m>, <m>M = Rv</m>.
                </p>
              </li>
            </ol>
          </example>

          <lemma xml:id="lem-schurs">
            <title>Schur's Lemma</title>
            
            <statement>
              <p>
                Let <m>R</m> be a ring, and <m>M,N</m> be two simple <m>R</m>-modules. Then every nonzero <m>R</m>-module homomorphism <m>\phi:M\to N</m> is an isomorphism. In particular, <m>\operatorname{End}_R(M)</m> is a division ring.
              </p>
            </statement>

            <proof>
              <p>
                For the first assertion, let <m>f:M\to N</m> be <m>R</m>-linear and nonzero. Then <m>\operatorname{ker}(f)\neq M</m>, so <m>\operatorname{ker}(f)=0</m> by simplicity, and <m>\operatorname{im}(f) \neq 0</m>, so <m>\operatorname{im}(f) = N</m>.
              </p>

              <p>
                For the second, recall that <m>\operatorname{End}_R(M)</m> is a ring. If <m>f\in \operatorname{End}_R(M)</m> is nonzero, then by the first part, it is an isomorphism, so it has a two-sided inverse in <m>\operatorname{End}_R(M)</m>.
              </p>
            </proof>
          </lemma>
        </section>

        <section xml:id="sec-fl-modules">
          <title>Finite Length Modules</title>

          <definition xml:id="def-finite-length">
            <statement>
              <p>
                Given a short exact sequence 
                <me>
                  0 \to A \to B \to C \to 0
                </me> 
                we may think of the middle module <m>B</m> as built out of <m>A</m> and <m>C</m>; we call <m>B</m> an <em>extension</em> of <m>A</m> and <m>C</m>. Suppose that a module has a finite sequence of submodules 
                <me>
                  0 = M_0 \subseteq M_1 \subseteq M_2 \subseteq \cdots \subseteq M_n = M
                </me> we call such a sequence a <em>filtration</em>. Then <m>M_1</m> is an extension of <m>M_0</m> and <m>M_1/M_0</m>, <m>M_2</m> is an extension of <m>M_1=M_1/M_0</m> and <m>M_2/M_1</m>, and so on. We might think of <m>M</m> as built from <m>M_1/M_0, M_2/M_1,\dots,M_n/M_{n-1}</m> like so.
              </p>
            </statement>
          </definition>

          <definition xml:id="def-">
            <statement>
              <p>
                A module <m>M</m> has <em>finite length</em> if it has a filtration of the form 
                <me>
                  0 = M_0 \subseteq M_1 \subseteq M_2 \subseteq \cdots \subseteq M_n = M
                </me> 
                with <m>M_{i+1}/M_i</m> simple for each <m>i</m>; such a filtration is called a <em>composition series</em> of <em>length</em> <m>n</m>. We say a composition series is <em>strict</em> if <m>M_i \neq M_{i+1}</m> for all <m>i</m>. Two composition series are <em>equivalent</em> if the collections of composition factors <m>M_{i+1}/M_i</m> are the same up to reordering. The <em>length</em> of a finite length module <m>M</m>, denoted <m>\ell(M)</m> , is the minimum of the lengths of a composition series of <m>M</m>. If <m>M</m> has does not have finite length, we say that <m>M</m> has infinite length, or <m>\ell(M) = \infty</m>.
              </p>
            </statement>
          </definition>

          <example>
            Let <m>K</m> be a field and <m>V=K^2</m>. Then any filtration of the form <m>0 \subseteq W \subseteq V</m> where <m>W</m> is a line through the origin is a strict composition series.
          </example>

          <remark>
            <p>
              Let 
              <me>
                0 \to A \xrightarrow{i} B \xrightarrow{p} C\to 0
              </me> 
              be a short exact sequence. Given filtrations / composition series / strict composition series 
              <me>
                A_\bullet: \qquad 0 = A_0 \subseteq A_1 \subseteq A_2 \subseteq \cdots \subseteq A_n = A
              </me> 
              and 
              <me>
                C_\bullet: \qquad 0 = C_0 \subseteq C_1 \subseteq C_2 \subseteq \cdots \subseteq C_n = C
              </me> 
              we can make a filtration / composition series / strict composition series of <m>B</m> by 
              <me>
                0 = i(A_0) \subseteq i(A_1) \subseteq i(A_2) \subseteq \cdots \subseteq i(A_n) = i(A) = p^{-1}(C_0) \subseteq p^{-1}(C_1) \subseteq p^{-1}(C_2) \subseteq \cdots \subseteq p^{-1}(C_n) = B.
              </me>
            </p>

            <p>
              Conversely, given a filtration / composition series / strict composition series of <m>B</m> that contains <m>i(L)</m> as a term, we can obtain filtrations / composition series / strict composition series of <m>A</m> and <m>C</m> by applying <m>i^{-1}</m> to the terms up through <m>i(L)</m> and applying <m>p</m> to the terms from <m>i(L)</m> on. However, not every filtration / composition series of a module will contain a fixed submodule as a term.
            </p>
          </remark>

          <theorem xml:id="thm-jordan-holder">
            <title>JOrdan-Holder Theorem</title>
            
            
            <statement>
              <p>
                Let <m>M</m> be a module of finite length.
              </p>

              <p>
                <ol>
                  <li>
                    <p>
                      If <m>L\subseteq M</m> is a proper submodule, then <m>\ell(L) &lt; \ell(M)</m>.
                    </p>
                  </li>
                  
                  <li>
                    <p>
                      If <m>L\subseteq M</m> is a nonzero submodule and <m>\overline{M}=M/L</m>, then <m>\ell(\overline{M})&lt; \ell(M)</m>.
                    </p>
                  </li>

                  <li>
                    <p>
                      Any filtration of <m>M</m> can be refined to a composition series.
                    </p>
                  </li>

                  <li>
                    <p>
                      All strict composition series for <m>M</m> are equivalent, and hence have the same length.
                    </p>
                  </li>
                </ol>
              </p>
            </statement>

            <proof>
              <p>
                If <m>m := \ell(M)</m>, consider a strict composition series of <m>M</m> of length <m>m</m>, say <me>M_\bullet: \qquad  0 = M_0 \subsetneqq M_1 \subsetneqq M_2 \subsetneqq \cdots \subsetneqq M_m = M.</me>
              </p>
              
              <p>
                <ol>
                  <li>
                    <p>
                      Consider the filtration 
                      <me>
                        L_\bullet: \qquad  0 = M_0 \cap L \subseteq M_1 \cap L \subseteq M_2 \cap L \subseteq \cdots \subseteq M_m \cap L = L.
                      </me> 
                      By the Second Isomorphism Theorem, its composition factors satisfy 
                      <me>
                        \frac{M_{i+1} \cap L}{M_i \cap L} = \frac{M_{i+1} \cap L}{(M_{i+1} \cap L) \cap M_i} \cong \frac{M_{i+1} \cap L + M_i} {M_i}.
                      </me> 
                      The right hand side is a submodule of <m>M_{i+1}/M_i</m>, which by assumption is simple, so our filtration is in fact a composition series of length <m>n</m>. Then for any <m>i</m> either 
                      <me>
                        \frac{M_{i+1} \cap L}{M_i \cap L} = 0 \quad \textrm{ or } \quad \frac{M_{i+1} \cap L}{M_i \cap L}\cong \frac{M_{i+1}}{M_i}.
                      </me> 
                      We claim that the latter case does not hold for all <m>i</m>: if it did, we would have <m>0=M_0 = M_0 \cap L</m>, and inductively <m>M_{i+1} \cap L = M_{i+1}</m> for all <m>i</m> and in particular for <m>i=m-1</m>, we have <m>M = M\cap L</m>, contradicting that <m>L</m> is proper. Thus, for some <m>i</m>, the first case holds. We can then skip that <m>i</m> and obtain a composition series of length less than <m>n</m>, so <m>\ell(L)&lt;m</m>.
                    </p>
                  </li>

                  <li>
                    <p>
                      Consider the filtration <me>\overline{M}_\bullet: \qquad 0 = \frac{M_0 + L }{L} \subseteq \frac{M_1 + L }{L} \subseteq \cdots \subseteq \frac{M_n + L }{L} = \overline{M}.</me> The factors satisfy <me>\frac{(M_{i+1} + L )/ L }{(M_i + L) / L} \cong \frac{M_{i+1} + L}{M_{i} + L} \cong \frac{M_{i+1} + (M_i+L)}{M_i+L}\cong \frac{M_{i+1} }{M_{i+1} \cap( M_i + L)},</me> and since <m>M_i \subseteq M_{i+1} \cap (M_i+L)</m>, these are quotient modules of the simple module <m>M_{i+1}/M_i</m>, so this is a composition series. Then for any <m>i</m> either <me>\frac{(M_{i+1} + L )/ L }{(M_i + L) / L}  = 0 \quad \textrm{ or } \quad \frac{M_{i+1} }{M_{i+1} \cap( M_i + L)}  \cong  \frac{M_{i+1}}{M_i}.</me>
                    </p>

                    <p>
                      We claim that the latter case does not hold for all <m>i</m>: if it did, we would have then <m>M_{i+1} \cap (M_i+L) = M_{i}</m> for all <m>i</m>, so <me>\frac{M_{i+1} + L}{M_{i+1}} \cong \frac{L+M_i}{(L+M_i) \cap M_{i+1}} =\frac{L+M_i}{M_i}</me> for all <m>i</m>, and hence <m>L \cong (L+M_0)/M_0 \cong (L+M_n)/M_n \cong 0</m>, contradicting that <m>L\neq 0</m>. Thus, for some <m>i</m>, the first case holds, and we can skip that <m>i</m> to obtain a composition series of length less than <m>n</m>, so <m>\ell(\overline{M})&lt;m</m>.
                    </p>
                  </li>

                  <li>
                    <p>
                      We proceed by induction on length again. Given a filtration of <m>M</m>, we can suppose that there is some nonzero proper submodule <m>L</m> in the filtration, since otherwise we could just take any composition series. Then <m>L</m> and <m>\overline{M}</m> has length less than <m>M</m>. The filtration up to <m>L</m> can be refined to a strict composition series by the induction hypothesis, and the filtration from <m>L</m> to <m>M</m> taken mod <m>L</m> can be refined to a strict composition series for <m>\overline{M}</m>; pulling back as in the remark above, we get the strict composition series we want.
                    </p>
                  </li>

                  <li>
                    <p>
                      We show by induction on <m>m</m> that for any module of length <m>m</m>, all of its strict composition series are equivalent. Assume that <m>\ell(M)=m</m>. If <m>m=1</m>, the claim is clear since we are dealing with a simple module. Suppose that <me>N_\bullet: \qquad 0 = N_0 \subsetneqq N_1 \subsetneqq \cdots \subsetneqq N_n = M</me> is another strict composition series for <m>M</m>, so <m>n\geq m</m>. If <m>N_{n-1}=M_{m-1}</m>, then since <m>\ell(M_{m-1})\leq m-1</m> the two composition series we have for <m>M_{m-1}</m> are equivalent by induction, so the two given series are equivalent.
                    </p>

                    <p>
                      If <m>N_{n-1} \neq M_{m-1}</m>, since <m>M/M_{m-1}</m> is simple, <m>M_{m-1}</m> is not properly contained in <m>N_{n-1}</m>, so the image of <m>M_{m-1}</m> in <m>M/N_{n-1}</m> is nonzero, so equals all of <m>M</m>, which means that <m>N_{n-1}+M_{m-1} = M</m>. Set <m>K=N_{n-1}\cap M_{m-1}</m>. By the second isomorphism theorem, we then have <me>\frac{M}{M_{m-1}} = \frac{M_{m-1}+N_{n-1}}{M_{m-1}} \cong \frac{N_{n-1}}{K}</me> and similarly <m>M/N_{n-1} \cong M_{m-1} / K</m>, and both of these modules are simple.
                    </p>

                    <p>
                      Fix a strict composition series for <m>K</m>: <me>K_{\bullet}: \qquad 0 = K_0 \subsetneqq K_1 \subsetneqq \cdots \subsetneqq K_k = K</me> and extend to a strict composition series for <m>M_{m-1}</m>: <me>K'_{\bullet}: \qquad 0 = K_0 \subsetneqq K_1 \subsetneqq \cdots \subsetneqq K_k = K \subsetneqq M_{m-1}.</me> Since we also have the strict composition series <me>M_{\bullet \leq m-1} : \qquad 0 = M_0 \subsetneqq M_1 \subsetneqq M_2 \subsetneqq \cdots \subsetneqq M_{m-1}</me> of length <m>m-1</m>, we must have that <m>k=m-2</m> and <m>K'_{\bullet}</m> is equivalent to <m>M_{\bullet \leq n-1}</m>. Thus, the composition factors of <m>M_{\bullet \leq m-1}</m> are those of <m>K</m> plus one copy of <m>M_{m-1} / K \cong M/N_{n-1}</m>.
                    </p>

                    <p>
                      Now, <me>K''_{\bullet}: \qquad  0 = K_0 \subsetneqq K_1 \subsetneqq \cdots \subsetneqq K_{m-2} = K \subsetneqq N_{n-1}</me> is a strict composition series for <m>N_{n-1}</m>, so <m>n=m</m>. Then, <m>K''_{\bullet}</m> is equivalent to the strict composition series <me>N_{\bullet \leq n-1}: \qquad 0 = N_0 \subsetneqq N_1 \subsetneqq \cdots \subsetneqq N_{n-1}.</me> Thus, the composition factors of <m>N_{\bullet \leq n-1}</m> are those of <m>K</m> plus one copy of <m>N_{n-1} / K \cong M/M_{n-1}</m>.
                    </p>

                    <p>
                      It follows that the composition series <m>M_\bullet</m> and <m>N_\bullet</m> are equivalent.
                    </p>
                  </li>
                </ol>
              </p>
            </proof>
          </theorem>

          <example>
            <p>
              <ol>
                <li>
                        <p>
                If <m>K</m> is a field, then a <m>K</m>-vector space of dimension <m>n</m> is a <m>K</m>-module of length <m>n</m>.
              </p>
                </li>
      
                <li>
                        <p>
                If <m>R</m> is a <m>K</m>-algebra, and <m>M</m> is an <m>R</m>-module that as a <m>K</m>-vector space has dimension <m>n</m>, then <m>\ell(M)\leq n</m>, since the vector space dimension of a proper submodule is strictly smaller.
              </p>
                </li>
      
                <li>
                        <p>
                The ring <m>R=K[x]</m> does not have finite length as a module over itself.
              </p>
                </li>
      
                <li>
                        <p>
                <m>\mathbb{Z}/p^n</m> has length <m>n</m> as a <m>\mathbb{Z}</m>-module, with strict composition series <me>0 \subseteq \langle \overline{p^{n-1}} \rangle \subseteq \cdots \subseteq \langle \overline{p} \rangle \subseteq \mathbb{Z}/p^n.</me>
              </p>
                </li>
      
              </ol>
            </p>
          </example>
          
          
        </section>
      </chapter>


      <chapter xml:id="sec-chain-conditions">
        <title>Chain Conditions</title>

        <definition xml:id="def-acc">
          <statement>
            <p>
              We say a poset <m>(P,\leq)</m> satisfies the <em>ascending chain condition</em> or <em>ACC</em> if every totally ordered nonempty subset of <m>P</m> has a maximum element.
            </p>
          </statement>
        </definition>

        <definition xml:id="def-dcc">
          <statement>
            <p>
              We say a poset <m>(P,\leq)</m> satisfies the <em>descending chain condition</em> or <em>DCC</em> if every totally ordered nonempty subset of <m>P</m> has a minimum element.
            </p>
          </statement>
        </definition>

        <remark>
          <p>
            For a poset <m>(P,\leq)</m>, the following are equivalent:
          </p>

          <p>
            <ol>
              <li>
                <p>
                  Every totally ordered nonempty subset has a maximum element (i.e., <m>P</m> has ACC)
                </p>
              </li>

              <li>
                <p>
                  Every totally ordered subset indexed by <m>\mathbb{N}</m>, <m>p_1 \leq p_2 \leq p_3 \leq \cdots</m> has a maximum element (i.e., <m>\exists k: p_k=p_{k+1} = \cdots</m>)
                </p>
              </li>

              <li>
                <p>
                  Every nonempty subset of <m>P</m> has a maximum element.
                </p>
              </li>
            </ol>
          </p>

          <p>
            Indeed, (3) <m>\Rightarrow</m> (1) <m>\Rightarrow</m> (2) is clear. Given a totally ordered nonempty subset with no maximum, one can inductively keep choosing larger elements and obtain a countable such subset, so (2) <m>\Rightarrow</m> (1). If any totally ordered nonempty subset of <m>P</m> has a maximum element, then the same property holds for any nonempty subset <m>Q</m> of <m>P</m>, so by Zorn’s Lemma, such a <m>Q</m> has a maximum element. The analogous equivalences hold with DCC.
          </p>
        </remark>

        <p>
          Note that the condition (3) asserts that any nonempty subset of <m>P</m> has an element that is maximal <em>within the subset</em>, not maximal <em>within <m>P</m></em>.
        </p>

        <definition xml:id="def-noeth-art-mod">
          <statement>
            <p>
              Let <m>R</m> be a ring and <m>M</m> be an <m>R</m>-module.

              <ol>
                <li>
                  <p>
                    We say that <m>M</m> is <em>Noetherian</em> if the poset of submodules of <m>M</m> partially ordered by containment has ACC.
                  </p>
                </li>

                <li>
                  <p>
                    We say that <m>M</m> is <em>Artinian</em> if the poset of submodules of <m>M</m> partially ordered by containment has DCC.
                  </p>
                </li>

                <li>
                  <p>
                    We say that <m>R</m> is <em>left Noetherian</em> if <m>R</m> is Noetherian as a left <m>R</m>-module; i.e., the poset of left ideals of <m>R</m> under containment has ACC.
                  </p>
                </li>

                <li>
                  <p>
                    We say that <m>R</m> is <em>left Artinian</em> if <m>R</m> is Artinian as a left <m>R</m>-module; i.e., the poset of left ideals of <m>R</m> under containment has DCC.
                  </p>
                </li>
              </ol>
            </p>

            <p>
              If <m>R</m> is commutative, left ideals and right ideals are the same, so we will just say <m>R</m> is Noetherian or Artinian.
            </p>
          </statement>
        </definition>

        <example>
          <p>
            <ol>
              <li>
                    <p>
              A division ring <m>D</m> is both left Noetherian and left Artinian.
            </p>
              </li>
      
              <li>
                    <p>
              If <m>R</m> is a PID but not a field (e.g., <m>R=\mathbb{Z}</m> or <m>R=K[x]</m>), then <m>R</m> is Noetherian but not Artinian. To see <m>R</m> is Noetherian, note that any ideal is of the form <m>I=(p_1^{e_1} \cdots p_t^{e_t})</m> for some irreducible elements <m>p_i</m> and positive integers <m>e_i</m>. An ideal contains <m>I</m> if it corresponds to a product of the same irreducibles with smaller or equal multiplicities; there are only finitely many of these so an ascending chain must stablilize. To see <m>R</m> is not Artinian, take some irreducible <m>p</m> and take the chain <me>(p) \supsetneqq (p^2) \supsetneqq (p^3) \supsetneqq (p^4) \supsetneqq \cdots .</me>
            </p>
              </li>
      
              <li>
                    <p>
              A polynomial ring in infinitely many variables is neither Noetherian nor Artinian: there is an ascending chain <me>(x_1) \subsetneqq (x_1,x_2)  \subsetneqq (x_1,x_2,x_3)  \subsetneqq (x_1,x_2,x_3,x_4) \subsetneqq \cdots</me> and take a descending chain as in the last example.
            </p>
              </li>
      
              <li>
                    <p>
              The <m>\mathbb{Z}</m>-module <m>M=\displaystyle \frac{\mathbb{Z}[\frac12]}{\mathbb{Z}}</m>, where <m>\mathbb{Z}[\frac12]</m> is the subring of <m>\mathbb{Q}</m> generated by <m>\mathbb{Z}</m> and <m>\frac12</m>, is Artinian but not Noetherian. Suppose that <m>N\subseteq M</m> is generated by <m>\displaystyle \big\{ \big[\frac{a_\lambda}{2^{n_\lambda}}\big]\big\}</m>, where each <m>a_\lambda</m> is odd (we can write any element in <m>\mathbb{Z}[\frac12]</m> like so). Observe that for each <m>\lambda</m>, there are integers <m>s,t</m> such that <m>\displaystyle s a_\lambda + t 2^{n_\lambda} = 1</m>, so <m>\displaystyle s \big[\frac{a_\lambda}{2^{n_\lambda}}\big] = \big[\frac{1}{2^{n_\lambda}}\big]</m>. Thus, <m>N</m> is generated by <m>\displaystyle \big\{ \big[\frac{1}{2^{n_\lambda}}\big]\big\}</m>. Thus, the submodules of <m>M</m> are <m>M</m> itself, <m>0</m>, and <m>M_i = \displaystyle \frac{\mathbb{Z}\cdot \frac{1}{2^n}}{\mathbb{Z}}</m> for <m>i&gt;0</m>. We have <m>0 \subsetneqq M_1 \subsetneqq M_2 \subsetneqq \cdots</m> so <m>M</m> is not Noetherian. However, any descending chain is either always equal to <m>M</m>, or else has some <m>M_i</m> as a term, and there are finitely many submodules of such an <m>M_i</m>, so must stabilize.
            </p>
              </li>
      
              <li>
                    <p>
              The subring of <m>M_2(\mathbb{Q})</m> given as <me>\left\{ \begin{bmatrix} a &amp; 0 \\ b &amp; c \end{bmatrix} \ \big| \ a\in \mathbb{Z}, b,c\in \mathbb{Q}\right\}</me> is left Noetherian but not right Noetherian.
            </p>
              </li>
      
            </ol>
          </p>
        </example>

        <problem>
          <p>
            Let <m>0 \to M' \to M \to M''\to0</m> be a short exact sequence. Then <m>M</m> has ACC (resp DCC) if and only if <m>M'</m> and <m>M''</m> have ACC (resp. DCC).
          </p>
        </problem>

        <p>
          The Noetherian condition is intimately tied to finite generation.
        </p>

        <proposition xml:id="prop-noeth-fg-mod">
          <statement>
            <p>
              Let <m>M</m> be an <m>R</m>-module. Then <m>M</m> has ACC if and only if every submodule of <m>M</m> is finitely generated.
            </p>
          </statement>

          <proof>
            <p>
              Suppose that <m>N\subseteq M</m> is not finitely generated. Then we can construct an ascending chain of submodules of <m>M</m> given by setting <m>N_0=0</m>, and <m>N_{i+i} = N_i + n_{i+1}</m> for some <m>n_{i+1}\in N\smallsetminus N_i</m>; we can do this since each <m>N_i</m> is a finitely generated submodule of <m>N</m>, so is not equal to <m>N</m>.
            </p>

            <p>
              Now suppose that every submodule of <m>M</m> is finitely generated. Given a countable ascending chain of submodules <me>M_1 \subseteq M_2 \subseteq M_3 \subseteq M_4 \subseteq \cdots</me> let <m>N=\bigcup _{n\in \mathbb{N}} M_n</m>; this is a submodule of <m>M</m>. Take a finite generating set <m>\{n_1,\dots,n_t\}</m> for <m>N</m>. For each <m>i=1,\dots,t</m>, we have <m>n_i \in M_j</m> for some <m>j</m>. Since there are finitely many <m>n_i</m>'s there is some <m>M_j</m> that contains them all. But then <m>M_j=N</m>, so the chain stabilizes (i.e., achieves a maximum element).
            </p>
          </proof>
        </proposition>

        <proposition xml:id="prop-lnoeth-fg-mod">
          <statement>
            <p>
              Let <m>R</m> be left Noetherian. Then a module is finitely generated if and only if it is left Noetherian. In particular, in a left Noetherian ring, every submodule of a finitely generated module is finitely generated.
            </p>
          </statement>

          <proof>
            <p>
              For the first statement, the <q>if</q> implication holds in general without the hypothesis on <m>R</m>. For the other implication, observe that there are short exact sequences <me>0 \to R^{n-1} \to R^{n} \to R \to 0</me> for all <m>n&gt;0</m>. So, by the exercise above and induction on <m>n</m>, every finitely generated free module is Noetherian. Now, if <m>M</m> is finitely generated, there is a short exact sequence of the form <me>0 \to K \to R^{n} \to M \to 0</me> so by the exercise above again, <m>M</m> is Noetherian.
            </p>

            <p>
              The second statement follows from the first as a submodule of a Noetherian module is Noetherian, again by the exercise
            </p>
          </proof>
        </proposition>

        <p>
          Now we tie these chain conditions to length.
        </p>

        <proposition xml:id="prop-fl-mod-iff">
          <statement>
            <p>
              A module <m>M</m> has finite length if and only if it is both Noetherian and Artinian.
            </p>
          </statement>

          <proof>
            <p>
              Assume that <m>M</m> has finite length. Suppose that <m>M</m> is not Noetherian. Then there is a chain <me>M_0 \subsetneqq M_1 \subsetneqq M_2 \subsetneqq  \cdots</me> Since each <m>M_i</m> is a submodule of <m>M</m>, its length is finite, and is a nonnegative integer. Then <m>\ell(M_0) &lt; \ell(M_1) &lt; \ell(M_2) &lt; \cdots \leq \ell(M)</m>, which yields a contradiction. The argument that <m>M</m> is Artinian is similar.
            </p>

            <p>
              Now assume that <m>M</m> is both Noetherian and Artinian. We will construct a composition series for <m>M</m>. We can assume that <m>M\neq 0</m>. Consider the collection of proper submodules of <m>M</m>. This is nonempty, so has a maximal element <m>M^1</m> by the Noetherian hypothesis. We must have <m>M/M^1</m> is simple, or else there is a module in between <m>M^1</m> and <m>M</m>. Using Noetherianity again, if <m>M^1\neq 0</m> (we’re done otherwise), there is a maximal proper submodule of <m>M^1</m>; call it <m>M^2</m>. This process yields a descending chain with simple quotients, and this must stop (i.e., yield <m>M^i=0</m> for some <m>i</m>) by the Artinian hypothesis. Thus, there is a composition series for <m>M</m>.
            </p>
          </proof>
        </proposition>
        
      </chapter>

      <chapter xml:id="ch-semisimple-mod">
        <title>Semisimple Modules</title>
        
        <p>
          We now study an important condition that is somewhat orthogonal (yet somewhat related) to our chain conditions. The condition of finite length, and to some extent the Noetherian and Artinian conditions, were related to how a module is made out of building blocks, or how big it is in terms of its pieces. The condition of semisimplicity says that a module is composed of basic building blocks in the simplest possible way.
        </p>

        <definition xml:id="def-semisimple-mod">
          <statement>
            <p>
              For any ring <m>R</m>, a left <m>R</m>-module <m>M</m> is called <em>semisimple</em> if it is a (possibly infinite) direct sum of simple modules. The empty direct sum is allowed, so that the <m>0</m> module <em>is</em> considered to be semisimple.
            </p>
          </statement>
        </definition>

        <example>
          <p>
            Let <m>M</m> be a finitely generated <m>\mathbb{Z}</m>-module. Then by the FTFGAG, <m>M</m> is isomorphic to <m>\mathbb{Z}^r \oplus \mathbb{Z}/p_1^{e_1} \oplus \cdots \oplus \mathbb{Z}/p_n^{e_n}</m> for some <m>r \geq 0</m>, <m>n \geq 0</m>, primes <m>p_i</m> and positive integers <m>e_i</m>. Such a module is semisimple if and only if <m>r = 0</m> and <m>e_i = 1</m> for all <m>i</m>.
          </p>
        </example>

        <example>
          <p>
            Every module over a division ring <m>D</m> is semisimple because any such module has a basis, hence it is a free module.
          </p>
        </example>

        <lemma xml:id="lem-matrixsemisimple">
          <statement>
            <p>
              Let <m>D</m> be a division ring and set <m>R = M_n(D)</m> for some <m>n \geq 1</m>. I claim <m>R</m> is semisimple as a left module over itself.
            </p>
          </statement>

          <proof>
            <p>
              For each <m>1 \leq i \leq n</m>, let <m>I_i</m> denote the subset of <m>R</m> consisting of matrices whose only nonzero entires belong to the <m>i</m>-th column. The rules for matrix addition and multiplication show that <m>I_i</m> is a left ideal (i.e., a left submodule) of <m>R</m>. Moreover, there is evident bijection between <m>I_i</m> and <m>D^n</m> (column vectors) and this bijection is an isomorphism of left <m>R</m>-modules. We proved <m>D^n</m> is simple as an <m>R</m>-module and hence so is <m>I_i</m>. Finally, <m>R</m> is the internal direct sum of <m>I_1, \dots, I_n</m>: <me>R=I_1\oplus \cdots \oplus I_n</me> because each matrix <m>X</m> is uniquely a sum of the form <m>X_1 + \cdots + X_n</m> with <m>X_i \in M_i</m>.
            </p>
          </proof>
        </lemma>

        <problem>
          <p>
            Let <m>\{M_\lambda\}_{\lambda\in \Lambda}</m> be an infinite collection of nonzero modules. Then <m>\bigoplus_{\lambda\in \Lambda} M_\lambda</m> is not finitely generated.
          </p>
        </problem>

        <remark>
          <p>
            As a consequence of the above exercise, a module is a finitely generated semisimple module if and only if it is a finite direct sum of simple modules. In this case if we write <m>M=M_1 \oplus \cdots \oplus M_n</m> as a sum of simple modules, there is a strict composition series <me>0\subset M_1 \subset M_1 \oplus M_2 \subset \cdots \subset M_1 \oplus \cdots \oplus M_{n-1} \subset M</me> so <m>M</m> has finite length, namely length <m>n</m>, and the composition factors are the modules <m>M_i</m>.
          </p>
        </remark>

        <proposition xml:id="prop-ks-semisimple-mod">
          <title>Krull-Schmidt for Semisimple Modules</title>
            
          <statement>
            <p>
              Let <m>M</m> be a finitely generated semisimple module. Given two direct sum decompositions as simple modules <me>M = M_1 \oplus  \cdots  \oplus M_m = N_1 \oplus  \cdots  \oplus N_n</me> then <m>m=n</m>, and there is a permutation <m>\sigma</m> such that <m>M_{\sigma(i)} \cong N_i</m> for all <m>i</m>
            </p>
          </statement>

          <proof>
            <p>
              Follows from the previous remark and the Jordan-Holder theorem.
            </p>
          </proof>
        </proposition>

        <theorem xml:id="thm-equiv-semisimp-mod">
          <title>Equivalent Conditions for Semisimple Modules</title>
          
          <statement>
            <p>
              For any ring <m>R</m> and left <m>R</m>-module <m>M</m>, the following are equivalent:
            </p>

            <p>
              <ol>
                <li>
                  <p>
                    <m>M</m> is semisimple,
                  </p>
                </li>

                <li>
                  <p>
                    very submodule of <m>M</m> is a summand; i.e., for every submodule <m>N</m> of <m>M</m> there is a submodule <m>N'</m> such that <m>M=N\oplus N'</m> is the internal direct sum of <m>N</m> and <m>N'</m>,
                  </p>
                </li>

                <li>
                  <p>
                    every injective <m>R</m>-map <m>i: M' \to M</m> is split has a left inverse,
                  </p>
                </li>

                <li>
                  <p>
                    every SES of the form <m>0 \to M' \to M \to M'' \to 0</m> is split exact,
                  </p>
                </li>

                <li>
                  <p>
                    every surjective <m>R</m>-map <m>p: M \to M''</m> has a right inverse.
                  </p>
                </li>
              </ol>
            </p>
          </statement>

          <proof>
            <p>
              The equivalence of (3), (4), and (5) is given by the Splitting Theorem.
            </p>

            <p>
              <m>(2) \Rightarrow (3)</m> holds since given an injective map <m>i</m> as in (3), we have by (2) that <m>i(M')</m> is a summand of <m>M</m>, hence there is a projection homomorphism <m>\pi:M\to i(M')</m> that splits the inclusion of the summand into <m>M</m>, that is <m>\pi|_{i(M')}=\mathrm{id}_{i(M')}</m>. Now <m>i:M'\to i(M')</m> is an isomorphism so we may consider the <m>R</m>-module homomorphism <m>i^{-1}:i(M')\to M'</m> and set <m>q:M\to M'</m> to be <m>q=i^{-1}\circ \pi</m>. Then <me>q\circ i=i^{-1}\circ \pi \circ i=i^{-1}\circ \pi_{i(M')} \circ i= i^{-1}\circ i=\mathrm{id}_{M'}.</me>
            </p>

            <p>
              <m>(3) \Rightarrow (2)</m> holds since we can split the inclusion <m>N\to M</m> and thus also the SES <me>0\to N\to M\to M/N \to 0.</me> Therefore the Splitting Theorem yields <m>M=N\oplus s(M/N)</m> where <m>s</m> denotes the splitting of the quotient map <m>M\to M/N</m>.
            </p>

            <p>
              The hard part is proving <m>(1) \Leftrightarrow (2)</m>. <m>(1) \Rightarrow (2)</m> Assume (1), so that <m>M = \oplus_{\lambda \in \Lambda} M_\lambda</m> for some collection of simple submodules <m>M_\lambda</m>, and let <m>N \subseteq M</m> be any submodule. (It is important to note that it does not necessarily follow that <m>N</m> is a sum of some subcollection of the <m>M_\lambda</m>) . Consider the collection <m>\mathcal S</m> of subsets <m>\Gamma</m> of <m>\Lambda</m> such that <m>N \cap M_\Gamma = 0</m> where we define <m>M_\Gamma := \oplus_{\lambda \in \Gamma} M_\lambda</m>. View <m>\mathcal S</m> as a poset by inclusion. It is nonempty since <m>J = \emptyset</m> belongs to <m>\mathcal S</m>. If <m>\{\Gamma_\alpha\}</m> is a totally ordered subcollection of <m>\mathcal S</m>, let <m>\Gamma = \cup_\alpha \Gamma_\alpha</m>. I claim <m>M_\Gamma \cap N = 0</m>. If not, there is a nonzero element <m>(m_\gamma) \in M_\Gamma \cap N</m>. But since <m>m_\gamma = 0</m> for all but a finite number of <m>\gamma</m>’s and since the collection of <m>\Gamma_\alpha</m>’s was totally ordered, there is some <m>\alpha</m> such that <m>(m_\gamma) \in M_{\Gamma_\alpha} \cap N</m>, a contradiction. We may thus apply Zorn’s Lemma to get a maximal <m>\Gamma \in \mathcal S</m>.
            </p>

            <p>
              I claim <m>M</m> is the internal direct sum of <m>N</m> and <m>M_\Gamma</m>. We have <m>N \cap M_\Gamma = 0</m> since <m>\Gamma\in\mathcal S</m> and so it suffices to prove <m>N + M_\Gamma = M</m>. Since <m>M = \sum_{\lambda \in \Lambda} M_\lambda</m>, the latter is equivalent to proving that <m>M_\lambda \subseteq N + M_\Gamma</m> for all <m>\lambda \in \Lambda</m>. If this fails for some <m>\lambda</m>, then since <m>M_\lambda \cap (N + M_\Gamma)</m> is a proper submodule of <m>M_\lambda</m>, which is simple, and hence <m>M_\lambda \cap (N + M_\Gamma) = 0</m>. But then <m>N \cap (M_{\Gamma} \oplus M_\lambda) = 0</m> (if <m>n \in N</m> and <m>n = m + m'</m>, with <m>m\in M_\Gamma</m> and <m>m'\in M_\lambda</m>, then <m>m'=n-m</m> so <m>m'=0</m> and <m>n=m</m>, and then <m>n=0</m>.) So, <m>\Gamma\cup\{ \lambda\}</m> is a member of <m>\mathcal S</m> that strictly contains <m>\Gamma</m>, a contradiction. It must be thar <m>M = N \oplus M_\Gamma</m>.
            </p>

            <p>
              <m>(2) \Rightarrow (1)</m> Now assume that every submodule of <m>M</m> is a summand. We proceed in three steps:
            </p>

            <p>
              (i) We claim that every submodule <m>T</m> of <m>M</m> inherits this property; i.e., every submodule of <m>T</m> is a summand of <m>T</m>. For say <m>U \subseteq T</m> is a submodule. By assumption on <m>M</m>, we have <m>M = U \oplus V</m> (internal direct sum) for some <m>V</m>. Since <m>U \subseteq T</m>, it follows that <m>T = U + (V \cap T)</m>. (Given <m>t \in T</m>, we have <m>t  =  u +v</m> for some <m>u \in U, v \in V</m>. Since <m>U \subseteq T</m>, <m>v =
              t-u \in V \cap T</m>.) Since <m>U \cap (V \cap T) = 0</m>, this shows <m>T = U \oplus (V \cap T)</m>.
            </p>

            <p>
              (ii) We claim that every nonzero submodule <m>T</m> of <m>M</m> contains a simple summand. Pick <m>0 \ne x \in T</m> and apply Zorn’s Lemma to show that there is a maximal submodule <m>U</m> of <m>T</m> with respect to the property that <m>x \notin U</m>. We have <m>T = U \oplus W</m> by (i) for some <m>W \neq 0</m>. If <m>W</m> is not simple, then <m>W</m> contains a nonzero, proper submodule <m>W_1</m> and hence, by using (i) again, we get that <m>W = W_1 \oplus W_2</m> for some proper nonzero submodule <m>W_2</m>.
            </p>

            <p>
              These properties implies that <m>(U \oplus W_1) \cap (U \oplus W_2) = U</m>. One containment is clear. If <m>v</m> belongs to the left side, then <m>v = u + w_1 = u' + w_2</m>. It follows that <m>w_1 - w_2 = u - u' \in U \cap W = 0</m> and so <m>w_1 = w_2 \in W_1 \cap W_2 = 0</m>, and hence <m>w_1  = w_2 = 0</m>. So, either <m>x \notin U \oplus W_1</m> or <m>x \notin U \oplus W_2</m>, and either way we reach a contradiction to the maximality of <m>U</m>.
            </p>

            <p>
              (iii) Let <m>\mathscr G</m> be the set of all simple submodules of <m>M</m>, and let <me>\mathscr  F= \{ \Omega \subseteq \mathscr G\ | \ \textrm{for all distinct} \ \omega_0, \omega_1, \dots, \omega_t \in \Omega, \omega_0 \cap (\omega_1 + \cdots + \omega_t) = 0\}.</me> Equivalently, the module generated by the modules in <m>\Omega</m> their direct sum. The set <m>\mathscr  F</m> is partially ordered by inclusion. It is nonempty, since <m>\varnothing \in \mathscr  F</m> (or some singleton is in there by (ii)). Given a chain <m>\{ \Omega_\alpha\}</m> in <m>\mathscr  F</m>, <m>\bigcup_\alpha \Omega_\alpha</m> is again an element of <m>\mathscr  F</m>, so there is a maximal element in <m>\mathscr  F</m>; call it <m>\Omega</m>. Let <m>U</m> be the direct sum of <m>\Omega</m>.
            </p>

            <p>
              We claim that <m>U=M</m>. By hypothesis we have <m>M = U \oplus V</m> for some <m>V</m>. If <m>V = 0</m> we are done. Otherwise by (ii) (and (i) again) we have <m>V = S \oplus V'</m> for some simple submodule <m>S</m>. But then <m>\Omega \cup \{ U\} \in \mathscr  F</m>, contradicting maximality of <m>\Omega</m>.
            </p>
          </proof>
        </theorem>

        <corollary xml:id="cor-semisimple-quotient">
          <statement>
            <p>
              If <m>M</m> semisimple, so is every submodule and quotient module of <m>M</m>.
            </p>
          </statement>

          <proof>
            <p>
              Say <m>N \subseteq M</m> is a submodule. By the claim marked (i) in the proof of Theorem <xref ref="prop93b" /> every submodule of <m>N</m> is a summand, and hence <m>N</m> is semisimple by Theorem <xref ref="prop93b" /> <m>(2)\Rightarrow (1)</m>.
            </p>

            <p>
              Given a surjection <m>M \twoheadrightarrow P</m>, it splits by Theorem <xref ref="prop93b" />, so that <m>P</m> is isomorphic to a submodule of <m>M</m>, namely the image of <m>P</m> under the splitting map. Hence <m>P</m> is semisimple by the case already proven.
            </p>
          </proof>
        </corollary>

        <p>
          A major source of semisimple modules comes from group rings.
        </p>
      </chapter>

      <chapter xml:id="ch-artin-wedd">
        <title>Artin-Wedderburn Theorem</title>

        <section xml:id="sec-semisimple-rings">
          <title>Semisimple Rings</title>

          <definition xml:id="def-semisimple-ring">
            <statement>
              <p>
                A ring <m>R</m> is <em>left semisimple</em> if <m>R</m> is semisimple as a left module over itself. <m>R</m> is <em>right semisimple</em> if <m>R</m> is semisimple as a right modules over itself.
              </p>
            </statement>
          </definition>

          <remark>
            <p>
              Recall that submodules of <m>R</m> are left ideals and the simple ones are the minimal (nonzero) left ideals. So, <m>R</m> is left semisimple if and only if <m>R</m> is the internal direct sum of some collection of minimal left ideals <m>I_j</m>: <me>R = \bigoplus_{j \in J} I_j.</me> Moreover, <m>R</m> is f.g. as a module over itself, and so this must be a finite direct sum. So, <m>R</m> is left semisimple if and only if <m>R</m> decomposes as an internal direct sum of the form <m>R = I_1 \oplus \cdots \oplus I_m</m> for some finite collection <m>I_1, \dots, I_m</m> of minimal left ideals.
            </p>
          </remark>

          <example>
            <p>
              For any <m>n \geq 0</m> and division ring <m>D</m>, the matrix ring <m>M_n(D)</m> is left semisimple. This was shown earlier. It is also right semisimple.
            </p>
          </example>

          <example>
            <p>
              If <m>R=K_1 \times \cdots \times K_t</m> is a finite product of fields, then each <m>K_i</m> is a simple <m>R</m>-module, and <m>R</m> is the direct sum of these, so <m>R</m> is (left) semisimple.
            </p>
          </example>

          <proposition xml:id="prop-equivcond-semisimple-ring">
            <statement>
              <p>
                For a ring <m>R</m>, the following conditions are equivalent:

                <ol>
                  <li>
                    <p>
                      <m>R</m> is a left semisimple ring.
                    </p>
                  </li>

                  <li>
                    <p>
                      Every left <m>R</m>-module is semisimple.
                    </p>
                  </li>

                  <li>
                    <p>
                      Every SES of left <m>R</m>-modules is split.
                    </p>
                  </li>

                  <li>
                    <p>
                      Every injection <m>i: M' \hookrightarrow M</m> of left <m>R</m>-modules splits
                    </p>
                  </li>

                  <li>
                    <p>
                      Every surjection <m>p: M \twoheadrightarrow M''</m> of left <m>R</m>-modules splits.
                    </p>
                  </li>

                  <li>
                    <p>
                      Every left <m>R</m>-module is projective.
                    </p>
                  </li>

                  <li>
                    <p>
                      Every left <m>R</m>-module is injective.
                    </p>
                  </li>
                </ol>
              </p>
            </statement>

            <proof>
              <p>
                The equivalence of (2)-(5) follows from Proposition <xref ref="prop93b" />. The equivalence of (4) and (7) follows from the characterization of injective modules in Proposition <xref ref="prop:injective" /> and the equivalence of (5) and (6) follows from the characterization of projective modules in Proposition <xref ref="prop1111" />. The implication (2) <m>\Rightarrow</m> (1) is obvious.
              </p>

              <p>
                Now for <m>(1)\Rightarrow (2)</m>: Assume (1) and let <m>M</m> be any left <m>R</m>-module. It follows from the definition that an arbitrary coproduct of semisimple modules is again semisimple, and so the free module <m>\bigoplus_\Lambda R</m> is semisimple for any indexing set <m>\Lambda</m>. By choosing a generating set of <m>M</m> , we may find a surjection of the form <m>p: \oplus_\Lambda R \twoheadrightarrow M</m>. By Corollary <xref ref="cor93" />, it follows that <m>M</m> is semisimple since it is a quotient of a semisimple module <m>M\cong \oplus_\Lambda R/\operatorname{ker}(p)</m>.
              </p>
            </proof>
          </proposition>

          <proposition xml:id="prop-ds-semisimple">
            <statement>
              <p>
                Let <m>R</m> be a left semisimple ring and write <m>R = I_1 \oplus \cdots \oplus I_m</m> as an internal direct sum with <m>I_1, \dots, I_m</m> minimal left ideals. Let <m>J_1, \dots, J_n</m> be a complete list of representatives of isomorphism classes as left <m>R</m>-modules taken from the list <m>I_1, \dots, I_m</m>; so, for each <m>i</m> with <m>1 \leq i \leq m</m>, there is a unique <m>j</m> with <m>1 \leq j \leq l</m> so that <m>I_i \cong J_j</m> as left <m>R</m>-modules.
              </p>

              <p>
                Then every <m>R</m>-module is isomorphic to <m>J_1^{\oplus \Lambda_1} \oplus \cdots \oplus J_n^{\oplus \Lambda_n}</m> for some index sets <m>\Lambda_1,\dots,\Lambda_n</m>.
              </p>

              <p>
                If <m>M</m> is finitely generated, <m>M</m> is isomorphic to <m>J_1^{\oplus e_1} \oplus \cdots \oplus J_n^{\oplus e_n}</m> for a unique list <m>e_1, \dots, e_n</m> of nonnegative integers.
              </p>
            </statement>

            <proof>
              <p>
                If <m>M</m> is finitely generated there is a surjection <m>R^a \twoheadrightarrow M</m>. Using Proposition <xref ref="prop916" /> this surjection splits, so that <m>R^n \cong M \oplus N</m> for some <m>N</m>, and each of <m>M</m> and <m>N</m> is semisimple and finitely generated. So <m>M = \oplus_{i=1}^s M_i</m> and <m>N = \oplus_{j=1}^s N_j</m> with <m>M_i, N_j</m> simple. Clearly <m>R^n</m> is isomorphic to a finite direct sum of copies of the <m>J_i</m>’s, and so the result follows from the Krull-Schmidt Theorem for semisimple modules.
              </p>

              <p>
                In the general case, we know that <m>M</m> is a direct sum of simple modules; if some simple summand <m>N</m> of <m>M</m> is not isomorphic to one of the <m>J_i</m>, then <m>N</m> is a finitely generated counterexample to the f.g. case.
              </p>
            </proof>
          </proposition>

          <p>
            In short, if <m>R</m> is left semisimple, and we know the simple decomposition of <m>R</m> itself, then we have a complete classification of <em>all</em> <m>R</m>-modules: they are just direct sums of the simple summands of <m>R</m>!
          </p>

          <p>
            Much of the interest in semisimple rings arises from the following:
          </p>

          <theorem xml:id="thm-Maschke">
            <title>Maschke's Theorem</title>
            
            
            <statement>
              <p>
                If <m>K</m> is a field and <m>G</m> is a finite group such that <m>\mathrm{char}(K)</m> does not divide <m>|G|</m>, then the group ring <m>K[G]</m> is left semisimple.
              </p>
            </statement>

            <proof>
              <p>
                Let <m>i: N \to M</m> be any injection of left <m>K[G]</m>-modules. It suffices to prove that there is an <m>K[G]</m>-linear map <m>p: M \to N</m> such that <m>p \circ i = \mathrm{id}_N</m>. By restriction of scalars along the inclusion <m>K \subseteq K[G]</m>, we may regard <m>i</m> as a <m>K</m>-linear map between <m>K</m>-vector spaces. As such it admits a <m>K</m>-linear splitting <m>f: M \twoheadrightarrow N</m> (since <m>K</m> is semisimple). There is no reason that <m>f</m> will be <m>K[G]</m>-linear, but we can modify it so that it becomes so: Define <m>p: M \to N</m> by <me>p(m) = \frac{1}{|G|} \sum_{g \in G} g^{-1} f(g m).</me> Note that the formula makes sense since <m>|G|</m> is invertible in <m>K</m> by assumption.
              </p>

              <p>
                Then <m>p</m> is still a <m>K</m>-linear map (since <m>f</m> is <m>K</m>-linear and the group action is <m>K</m>-linear). For any <m>h \in G</m> we have <me>p(hm) = \frac{1}{|G|} \sum_{g \in G} g^{-1} f(g hm) = \frac{1}{|G|} \sum_{x \in G} hx^{-1} f(xm)
                  = h p(m),</me> where the second equality is given by identifying <m>x</m> with <m>gh</m>. These conditions ensure that <m>p</m> is <m>K[G]</m>-linear. Finally, <me>p(i(n))= \frac{1}{|G|} \sum_{g \in G} g^{-1} f(g i(n))
                  = \frac{1}{|G|} \sum_{g \in G} g^{-1} f(i(gn)) 
                  = \frac{1}{|G|} \sum_{g \in G} g^{-1} gn
                  = \frac{1}{|G|} \sum_{g \in G} n  = n</me> where the second equality uses that <m>i</m> is <m>K[G]</m>-linear and the third one uses that <m>f \circ i =
                  \mathrm{id}</m>.
              </p>
            </proof>
          </theorem>

          <remark>
            <p>
              The proof actually shows that <m>K[G]</m> is semisimple provided <m>K</m> is and <m>|G|</m> is invertible in <m>K</m>.
            </p>
          </remark>

          <example>
            <p>
              The group ring <m>R=\mathbb{F}_p[C_p]</m> does not satisfy the hypotheses of Maschke’s theorem, since the order of the group is zero in the field. In fact, <m>\mathbb{F}_p[C_p]</m> is not semisimple: let <m>V=\mathbb{F}_p^2</m> be the <m>{C_p= \langle g\rangle}</m> representation <m>g^n \mapsto \begin{bmatrix} 1 &amp; 0 \\ \lambda n &amp; 1\end{bmatrix}</m>; i.e., as a <m>\mathbb{F}_p[C_p]</m>-module, we have <m>g \cdot \begin{bmatrix} a \\ b\end{bmatrix} =  \begin{bmatrix} a \\ a+b\end{bmatrix}</m>. We claim that <m>{U=\left\{ \begin{bmatrix} 0 \\ b \end{bmatrix}\right\}}</m> is the unique nonzero proper submodule of <m>V</m>. Let <m>W\subseteq V</m> be a nonzero submodule and suppose that <m>U\neq W</m>. Then, there is some element <m>v=\begin{bmatrix} a \\ b\end{bmatrix} \in W</m> with <m>a\neq 0</m>. Then <m>v</m> and <m>gv</m> are linearly independent, so we must have <m>W=V</m>. It follows that <m>V</m> is not semisimple: it is not simple since <m>0\subsetneqq U \subsetneqq V</m>, but <m>V</m> is not a direct sum of simple modules.
            </p>
          </example>

          <p>
            Let <m>G</m> be a finite group and <m>K</m> a field. The representation of <m>G</m> corresponding to <m>K[G]</m> viewed as a left module over itself can be described explicitly as following: As a <m>K</m>-vector space, <m>K[G]</m> has <m>G</m> as a basis: <m>K[G] = \oplus_{g \in G} k \cdot g</m>. <m>G</m> acts on this vector space by permuting the basis via left multiplication: <m>h \cdot (\sum_g c_g g) = \sum_g c_g (hg)</m>. This is sometimes called the <em>(left) regular representation of <m>G</m></em>.
          </p>

          <corollary xml:id="cor-Maschke">
            <title>Corollary of Maschke's Theorem</title>
            
            <statement>
              <p>
                If <m>G</m> is a finite group and <m>K</m> is a field such that <m>\mathrm{char}(K) \nmid |G|</m>, then every <m>K</m>-linear representation of <m>G</m> is a direct sum of irreducible representations, and every finite dimensional representation is uniquely a finite direct sum of irreducible ones.
              </p>

              <p>
                Moreover, every irreducible representation arises as a summand of the left regular representation.
              </p>
            </statement>
          </corollary>

          <example>
            <p>
              Let <m>G=C_3</m>. We can use Maschke’s Theorem and the theory of semisimple rings so far to classify every representation of <m>G</m> over <m>{\mathbb{R}}</m> or over <m>\mathbb{C}</m> (or more generally over any field of characteristic not equal to 3). In any case, the left regular representation <m>V</m> of <m>C_3</m> is the three-dimensional representation with basis <m>\{1,g,g^2\}</m> such that <m>g\cdot 1 = g, g\cdot g=g^2, g\cdot g^2 =1</m>, i.e., <me>g \mapsto \begin{bmatrix} 0 &amp; 0 &amp; 1 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0\end{bmatrix}</me> in this basis. Also in any case, the subspace <m>W</m> spanned by <m>1+g+g^2</m>, which is the vector <m>(1,1,1)</m> in these coordinates, is a 1-dimensional <m>G</m>-stable subspace, so a simple subrepresentation. Moreover, this is the trivial representation, since this vector is fixed by <m>g</m>. Then <m>V/W</m> obtains the structure of a representation. We can take <m>1+W,g+W</m> as a basis for <m>V/W</m>, and <m>g\cdot (1+W) = g+W</m>, and <m>g\cdot (g+W) = g^2+W = -1-g +W</m>, i.e., in our coordinates, <me>g\mapsto \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; -1 \end{bmatrix}.</me> A <m>G</m>-stable subspace must correspond to an eigenvector of <m>g</m> (which is equivalently an eigenvector of <m>g^{-1}</m>). The characteristic equation of this matrix shows that the eigenvalues are precisely the primitive cube roots of unity.
            </p>

            <p>
              If <m>K={\mathbb{R}}</m> (or more generally if there are no primitive cube roots of unity), then the <m>2</m>-dimensional representation <m>V/W</m> we just found is simple since there are no stable subspaces. If <m>K=\mathbb{C}</m> (or more generally if there are primitive cube roots of unity), let <m>\omega</m> be a primitive cube root of unity. The <m>2</m>-dimensional representation <m>V/W</m> has <m>\omega</m> and <m>\omega^2</m> as eigenvalues, and there are corresponding eigenvectors, so <m>V/W</m> is a direct sum of two 1-dimensional stable subspaces <m>T, T'</m> such that <m>g\cdot t= \omega t</m> for all <m>t\in T</m> and <m>g\cdot t' = \omega^2 t'</m> for all <m>t'\in T'</m>.
            </p>

            <p>
              We conclude that <em>every</em> real representation of <m>C_3</m> is isomorphic to a direct sum of copies of the trivial representation and copies of <m>V/W</m>. That is, for any such representation <m>U</m>, there is a basis of <m>U</m>, <m>\{e_\alpha \}_{\alpha\in A}</m>, <m>\{e'_\beta, e''_\beta\}_{\beta\in B}</m>, such that <m>g\cdot e_\alpha = e_\alpha</m>, <m>g\cdot e'_\beta = e''_\beta</m>, and <m>g\cdot e''_\beta = - e'_\beta - e''_\beta</m>.
            </p>
    
            <p>
              We conclude that <em>every</em> complex representation of <m>C_3</m> is isomorphic to a direct sum of copies of the trivial representation, <m>T</m>, and <m>T'</m>. That is, for any such representation <m>U</m>, there is a basis of <m>U</m>, <m>\{e_\alpha \}_{\alpha\in A}</m>, <m>\{e'_\beta\}_{\beta\in B}</m>, <m>\{e''_\gamma\}_{\gamma\in C}</m>, such that <m>g\cdot e_\alpha = e_\alpha</m>, <m>g\cdot e'_\beta = \omega e'_\beta</m>, and <m>g\cdot e''_\gamma = e''_\gamma</m>.
            </p>
          </example>
        </section>

        <section xml:id="sec-art-wedd">
          <title>Artin-Wedderburn Theorem</title>

          <p>
            We will now give a classification of <em>all</em> left semisimple rings. To start, we collect some examples.
          </p>

          <lemma xml:id="lem-lsemisimple-product">
            <statement>
              <p>
                If <m>R</m> and <m>S</m> are left semisimple, so is the product ring <m>R \times S</m>.
              </p>
            </statement>

            <proof>
              <p>
                Say we have internal direct sum decompositions <m>R = I_1 \oplus \cdots \oplus I_m</m> and <m>S = J_1 \oplus \cdots \oplus J_n</m> involving minimal left ideals. Then for all <m>a</m> and <m>b</m>, <m>I_a \times \{0\}</m> and <m>\{0\} \times J_b</m> are minimal left ideals of <m>R \times S</m> and they determine an internal direct sum decomposition of <m>R \times S</m>.
              </p>
            </proof>
          </lemma>

          <example>
            <p>
              The previous lemma and Lemma <xref ref="lem:matrixsemisimple" /> show that for any integer <m>m \geq 0</m>, list of division rings <m>D_1, \dots, D_m</m> and positive integers <m>n_1, \dots, n_m</m>, the ring <me>R =\operatorname{Mat}_{n_1}(D_1) \times \cdots \times \operatorname{Mat}_{n_m}(D_m)</me> is left semisimple.
            </p>
          </example>

          <p>
            The Artin-Wedderburn Theorem asserts that the last example accounts for <em>all</em> examples!
          </p>

          <theorem xml:id="thm-art-wedd">
            <title>Artin-Wedderburn Theorem</title>
            
            <statement>
              <p>
                Let <m>R</m> be a left semisimple ring. Then for some <m>m \geq 0</m>, positive integers <m>n_1, \dots, n_m</m>, and division rings <m>D_1, \dots, D_m</m>, there is a ring isomorphism <me>R \cong \operatorname{Mat}_{n_1}(D_1) \times \cdots \times \operatorname{Mat}_{n_m}(D_m).</me>
              </p>

              <p>
                Moreover,
                <ol>
                  <li>
                    <p>
                      <m>m</m> is the number of isomorphism classes of simple left <m>R</m>-modules.
                    </p>
                  </li>

                  <li>
                    <p>
                      Say <m>M_1, \dots, M_m</m> are simple modules forming a complete set of representatives of these isomorphism classes. Then, after reordering, <m>D_i\cong \operatorname{End}_R(M_i)^{\mathrm{op}}</m> and
                    </p>
                  </li>

                  <li>
                    <p>
                      <m>n_j</m> is the number of times summands isomorphic to <m>M_j</m> occur in the decomposition of <m>R</m> into a direct sum of simple left modules.
                    </p>
                  </li>
                </ol>
                Moreover, the data <m>(m; n_1, \dots, n_m; D_1, \dots, D_m)</m> is unique up to a permutation of <m>\{1, \dots, m\}</m> and isomorphisms of division rings.
              </p>
            </statement>
          </theorem>

          <example>
            <p>
              We saw before that the module decomposition in terms of simple modules is <m>{\mathbb{R}}[C_3] = W \oplus U</m>, where <m>W</m> is the one-dimensional trivial representation, and <m>U</m> is the <m>2</m>-dimensional representation given by <m>g \mapsto \begin{bmatrix} 0  &amp; -1 \\ 1 &amp; -1\end{bmatrix}</m>. On the other hand, as rings, <me>{\mathbb{R}}[C_3] \cong {\mathbb{R}}[g]/(g^3-1) \cong {\mathbb{R}}[g]/(g-1) \times {\mathbb{R}}[g]/ (g^2+g+1) \cong {\mathbb{R}}\times \mathbb{C}.</me>
            </p>

            <p>
              To reconcile these decompositions by the Artin-Wedderburn Theorem, one can check that <m>\operatorname{End}_{{\mathbb{R}}[C_3]}(W) \cong {\mathbb{R}}</m> and <m>\operatorname{End}_{{\mathbb{R}}[C_3]}(V/W) \cong \mathbb{C}</m>.
            </p>

            <p>
              We have <m>\operatorname{End}_{{\mathbb{R}}[C_3]}(W) \cong {\mathbb{R}}</m>. To compute the endomorphism ring of <m>V/W</m>, observe that an <m>{\mathbb{R}}</m>-linear endomorphism of <m>V/W</m> is <m>{\mathbb{R}}[C_3]</m>-linear if and only if it commutes with the action of <m>g</m>. We can write any <m>{\mathbb{R}}</m>-linear endomorphism of <m>V/W</m> as a <m>2\times 2</m> matrix; for it to commute with <m>g</m> means it commutes with <m>\begin{bmatrix} 0  &amp; -1 \\ 1 &amp; -1\end{bmatrix}</m>. We have <me>\begin{bmatrix} a &amp; b \\ c&amp; d \end{bmatrix} \begin{bmatrix} 0  &amp; -1 \\ 1 &amp; -1\end{bmatrix} - \begin{bmatrix} 0  &amp; -1 \\ 1 &amp; -1\end{bmatrix}  \begin{bmatrix} a &amp; b \\ c&amp; d \end{bmatrix} = \begin{bmatrix} b + c &amp; -a-b+d \\ d -a + c &amp; -b-c \end{bmatrix},</me> so the matrices we seek are of the form <me>\begin{bmatrix} a &amp; -c \\ c &amp; a-c \end{bmatrix}= a \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{bmatrix} + c \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; -1\end{bmatrix}.</me> Any pair of matrices in this set commutes (since the two vectorspace generators do) so they form a commutative ring and hence a field by Schur’s Lemma; any matrix in this collection is algebraic over the subring of scalar matrices (since both generators are). It follows that this collection of matrices is isomorphic as a ring to <m>\mathbb{C}</m>.
            </p>
          </example>

          <lemma xml:id="lem-semisimple-end">
            <statement>
              <p>
                Let <m>M</m> be an <m>R</m>-module. The map 
                <me>\xymatrix@R=1mm{ \operatorname{End}_R(M^{\oplus n}) \ar[r]^-{\Theta} &amp; \operatorname{Mat}_n(\operatorname{End}_R(M)) \\
                  \phi \ar@{|-&gt;}[r] &amp; [\pi_i \phi  \iota_j]_{i,j} }
                </me> 
                is a ring isomorphism, where <m>\iota_k</m> and <m>\pi_k</m> denote the natural inclusion and projection maps.
              </p>
            </statement>

            <proof>
              <p>
              It is clear that this map is additive, as each <m>\iota_i</m> and <m>\pi_j</m> is. Observe that <m>\pi_j  \iota_i</m> is the identity on <m>M</m> if <m>i=j</m>, and the zero map otherwise and that <m>1_{M^{\oplus n}} = \sum_{k} \iota_k  \pi_k</m>.
              </p>

              <p>
                The map <me>\xymatrix@R=1mm{ \operatorname{End}_R(M^{\oplus n}) &amp;\ar[l]_-{\zeta}  \operatorname{Mat}_n(\operatorname{End}_R(M)) \\
                \sum_{i,j} \iota_i  \alpha_{i,j}  \pi_j &amp; \ar@{|-&gt;}[l]  [\alpha_{i,j}]_{i,j} }</me> is a two-sided inverse for <m>\Theta</m>: <me>\zeta (\Theta(\phi)) = \zeta ( [\pi_i  \phi  \iota_j]_{i,j} ) = \sum_{i,j} \iota_i \pi_i  \phi  \iota_j \pi_j =(\sum_i \iota_i \pi_i) \phi (\sum_j \iota_j \pi_j)=\phi, \quad\text{and}</me> <me>\Theta(\zeta([\alpha_{i,j}]_{i,j}]))_{k,\ell} = \Theta ( \sum_{i,j} \iota_i  \alpha_{i,j}  \pi_j)_{k,\ell} = \pi_k (\sum_{i,j} \iota_i  \alpha_{i,j}  \pi_j) \iota_\ell = \sum_{i,j}  (\pi_k \iota_i) \alpha_{i,j} (\pi_j \iota_\ell) = \alpha_{k,\ell}.</me>
              </p>

              <p>
                To see that <m>\Theta</m> respects multiplication, we have <me>[\Theta (\psi) \Theta(\phi) ]_{i,j}= \sum_k (\pi_i  \psi  \iota_k) (\pi_k  \phi  \iota_j) = \pi_i  \psi\phi  \iota_j = \Theta(\psi \phi)_{i,j}.\qedhere</me>
              </p>
            </proof>
          </lemma>

          <problem>
            <p>
              Let <m>D</m> be a division ring. Then <m>\operatorname{End}_{\operatorname{Mat}_n(D)}(D^n)\cong D^{\mathrm{op}}</m>, where <m>D^n</m> is the simple module of column vectors.
            </p>
          </problem>

          <theorem xml:id="thm-whoknows">
            <statement>
              <p>
                ???
              </p>
            </statement>

            <proof>
              <p>
                Since <m>R</m> is left semisimple, we have <m>R \cong I_1 \oplus \cdots \oplus I_t</m> with each <m>I_i</m> is simple (in fact a minimal ideal). Group by isomorphism to rewrite this as <m>R \cong M_1^{\oplus n_1} \oplus \cdots \oplus M_m^{\oplus n_m}</m> with each <m>M_i</m> simple, <m>n_j \geq 1</m>, and such that <m>M_i</m> is not isomorphic to <m>M_j</m> for all <m>i \ne j</m>. We compute the endomorphisms of both sides:
              </p>

              <p>
                <me>\begin{aligned}
      \label{E912}
        \operatorname{End}_R(R) &amp;=&amp; \mathrm{Hom}_R(\bigoplus_{i=1}^m M_i^{\oplus n_i}, \prod_{j=1}^m M_j^{\oplus n_j}) \cong  \prod_j \mathrm{Hom}_R(M_i^{\oplus n_i}, \prod_i  M_i^{\oplus n_j})\\
        &amp;\cong&amp;  \prod_{i=1}^m \prod_{j=1}^m \mathrm{Hom}_R(M_i^{\oplus n_i}, M_j^{\oplus n_j}) \\
        &amp;=&amp; \prod_{i=1}^m \mathrm{Hom}_R(M_i^{\oplus n_i}, M_i^{\oplus n_i}) \\
        &amp;=&amp; \prod_{i=1}^m  \operatorname{End}_R(M_i^{\oplus n_i})\cong \prod_{i=1}^m  \operatorname{Mat}_{n_i}(\operatorname{End}_R(M_i)).
      \end{aligned}</me>
              </p>

              <p>
                Above the second line follows from the first by properties of <m>\mathrm{Hom}</m>, the third follows because Schur’s lemma gives that <m>\mathrm{Hom}_R(M_i,M_j)=0</m>, and consequently <m>\mathrm{Hom}_R(M_i^{\oplus n_i},M_j^{\oplus n_j})=0</m>, when <m>i\neq j</m>. The final isomorphism is the previous lemma.
              </p>

              <p>
                On the one hand, we have <m>\operatorname{End}_R(R) \cong R^{\mathrm{op}}</m> by a problem from the homework. On the other hand, applying Schur’s Lemma again, <m>D'_i := \operatorname{End}_R(M_i)</m> is a division ring for all <m>i</m>.
              </p>

              <p>
                Combining these gives <me>R^{\mathrm{op}} \cong \operatorname{Mat}_{n_1}(D'_1) \times \cdots \times \operatorname{Mat}_{n_m}(D'_m)</me> and hence, also by a homework problem, we have <me>R \cong
      \left(\operatorname{Mat}_{n_1}(D'_1) \times \cdots \times \operatorname{Mat}_{n_m}(D'_m)\right)^{\mathrm{op}} \cong
      \operatorname{Mat}_{n_1}(D_1) \times \cdots \times \operatorname{Mat}_{n_m}(D_m)</me> with <m>D_i := (D_i')^{\mathrm{op}}=\operatorname{End}_R(M_i)^\mathrm{op}</m>.
              </p>

              <p>
                This shows that given a decomposition of <m>R</m> as a left semisimple module, there is a ring decomposition as a product of matrix rings over division rings, and the data of division rings and matrix sizes is related to the data of simple modules and multiplicities by the formulas (1)–(3). We just need to prove uniqueness.
              </p>

              <p>
                Say we are given an isomorphism of rings <m>R \cong \prod_{i=1}^k \operatorname{Mat}_{t_i}(Q_i)</m> for some division rings <m>Q_1,\dots,Q_k</m>. Then since <m>\operatorname{Mat}_{t_i}(Q_i)</m> decomposes as a direct sum of <m>t_i</m> copies of <m>N_i:=Q_i^{t_i}</m>, and <m>N_i</m> is a simple <m>\operatorname{Mat}_{t_i}(Q_i)</m>-module, hence also a simple <m>R</m>-module, we have a semisimple <m>R</m>-module decomposition of <m>R</m> as <me>M_1^{\oplus n_1} \oplus  \cdots \oplus M_m^{\oplus n_m} \cong R \cong N_1^{\oplus t_1} \oplus \cdots \oplus N_k^{\oplus t_k}.</me> By Krull-Schmidt, we must have <m>m=k</m>, and after a permutation, <m>M_i \cong N_i = Q_i^{\oplus t_i}</m> and <m>n_i=t_i</m> for each <m>i</m>.
              </p>

              <p>
                Moreover, we have <me>D_i\cong  \operatorname{End}_R(M_i)^{\mathrm{op}} \cong  \operatorname{End}_R(N_i)^{\mathrm{op}}.</me> We recall that <m>N_i\cong Q_i^{\oplus n_i}</m>, with the natural column vector action from <m>\operatorname{Mat}_{n_i}(Q_i)</m>, and the trivial action from the other factors. Thus, <me>\operatorname{End}_R(N_i)^{\mathrm{op}} = \operatorname{End}_{\operatorname{Mat}_{n_i}(Q_i)} (Q_i^{\oplus n_i}) ^{\mathrm{op}}\cong Q_i,</me> using the exercise above.
              </p>
            </proof>
          </theorem>
          
          <corollary xml:id="cor-lr-semisimple">
            <statement>
              <p>
                A ring is left semisimple if and only if it is right semisimple.
              </p>
            </statement>

            <proof>
              <p>
                The claim is equivalent to showing <m>R</m> is left semisimple if and only <m>R^\mathrm{op}</m> is, which in turn follows from just one of the implications. If <m>R</m> is left semisimple, then <m>R\cong \prod_{i} \operatorname{Mat}_{n_i}(D_i)</m>, so <m>R^{\mathrm{op}} \cong \prod_i \operatorname{Mat}_{n_i}(D_i)^{\mathrm{op}} \cong \prod_i \operatorname{Mat}_{n_i}(D_i^{\mathrm{op}})</m> so <m>R^{\mathrm{op}}</m> is left semisimple.
              </p>
            </proof>
          </corollary>
          
          <p>
            Henceforth, we just say that <m>R</m> is <em>semisimple</em> if it is left semisimple.
          </p>
          
        </section>
        
      </chapter>

      <chapter xml:id="ch-applications">
        <title>Applications to Representation Theory</title>

        <section xml:id="sec-spplications">
          <title>Applications</title>
          
          <p>
            Let us start by restating the Artin-Wedderburn theorem in the context of group rings.
          </p>

          <theorem xml:id="thm-art-wedd-gr">
            <title>Artin-Wedderburn for Group Rings</title>
            
            
            <statement>
              <p>
                If <m>G</m> is a finite group and <m>K</m> is a field such that <m>\mathrm{char}(K) \nmid |G|</m>, then there is an isomorphism of rings <me>K[G] \cong \operatorname{Mat}_{n_1}(D_1) \times \cdots \times \operatorname{Mat}_{n_m}(D_m),</me> where <m>D_1, \dots, D_m</m> are division rings. Furthermore, each <m>D_i</m> contains <m>K</m> (up to isomorphism) as a subring of its center and the above isomorphism is <m>K</m>-linear. In particular, <m>\dim_K(D_i) &lt; \infty</m>.
              </p>

              <p>
                Moroever, we have:
                <ol>
                  <li>
                    <p>
                      <m>m</m> is the number of irreducible <m>k</m>-linear representation of <m>G</m> (up to isomorphism),
                    </p>
                  </li>

                  <li>
                    <p>
                      the <m>D_i</m>'s are the opposite rings of the endomorphism rings of these representations,
                    </p>
                  </li>

                  <li>
                    <p>
                      the <m>n_j</m>'s give the number of times each irreducible representation occurs in the decomposition of the regular representation of <m>G</m>,
                    </p>
                  </li>

                  <li>
                    <p>
                      the numbers <m>n_1 \cdot \dim_k(D_1), \dots, n_m \cdot \dim_k(D_m)</m> give the dimensions of these representations, and
                    </p>
                  </li>

                  <li>
                    <p>
                      <m>n_1^2 \cdot \dim_k(D_1) + \cdots + n_m^2 \cdot \dim_k(D_m) = |G|</m>.
                    </p>
                  </li>
                </ol>
              </p>
            </statement>

            <proof>
              <p>
                This mostly follows from Artin-Wedderburn and Maschke's Theorem. What needs to be noted is that each division ring here contains a copy of <m>K</m> in its center. Indeed, we recall that each <m>D_i</m> is given as the opposite ring of <m>\operatorname{End}_{K[G]}(M_i)</m> for some simple module <m>M_i</m>. For <m>\lambda\in K</m>, we have the map <m>M_i \xrightarrow{\lambda \cdot} M_i</m> which commutes with any <m>K[G]</m>-linear map from <m>M_i</m> to itself.
              </p>
            </proof>
          </theorem>

          <corollary xml:id="cor-ab-iff-gr-fieldprod">
            <statement>
              <p>
                Let <m>G</m> be a finite group, and <m>K</m> be a field such that <m>\mathrm{char}(K) \nmid |G|</m>. Then <m>G</m> is abelian if and only if <m>K[G]</m> is isomorphic to a product of fields.
              </p>
            </statement>
          </corollary>

          <lemma xml:id="lem-gring-and-iso">
            <statement>
              <p>
                Let <m>G</m> be any group and <m>K</m> any field. Given two group homomorphisms <m>\rho_1, \rho_2: G \to K^\times = \operatorname{GL}_1(K)</m>, the associated <m>K[G]</m>-modules <m>M_1</m> and <m>M_2</m> are isomorphic if and only if <m>\rho_1 =\rho_2</m>.
              </p>
            </statement>

            <proof>
              <p>
                Suppose that <m>\alpha:M_1 \to M_2</m> is an isomorphism of <m>K[G]</m>-modules. Identifying <m>M_1=M_2=K</m> as vector spaces, we have <m>\alpha(k) = ck</m> for some <m>c\neq 0</m>. Then, <me>c \rho_1(g) (k) = \alpha \rho_1(g) (k) = \rho_2(g) \alpha(k) = \rho_2(g) (ck) = c \rho_2(g) (k)</me> for all <m>k\in K</m>, so <m>\rho_1(g)(k)=\rho_2(g)(k)</m> for all <m>k\in K</m>.
              </p>
            </proof>
          </lemma>

          <proposition xml:id="prop-dring-and-C">
            <statement>
              <p>
                If <m>D</m> is a division ring that contains <m>{\mathbb{R}}</m> in its center and <m>\dim_{\mathbb{R}}(D) =2</m>, then <m>D \cong \mathbb{C}</m> .
              </p>
            </statement>

            <proof>
              <p>
                Pick <m>x \in D \smallsetminus {\mathbb{R}}</m>. Then <m>{\mathbb{R}}\subsetneqq {\mathbb{R}}[x] \subseteq D</m>, and since <m>{\mathbb{R}}[x]</m> is an <m>{\mathbb{R}}</m>-vectorspace, we must have <m>{\mathbb{R}}[x] = D</m> for dimension reasons. Thus <m>D</m> is commutative and is a field. Since <m>D</m> is a finite extension of <m>{\mathbb{R}}</m>, it is algebraic, so <m>{\mathbb{R}}\subsetneqq D \subseteq \mathbb{C}</m>, and we must have <m>D=\mathbb{C}</m>.
              </p>
            </proof>
          </proposition>

          <example>
              <p>
            Let <m>k = {\mathbb{R}}</m> and <m>G = S_3</m>. We find all the simple modules over the ring <m>{\mathbb{R}}[S_3]</m> or, equivalently, all irreducible <m>{\mathbb{R}}</m>-linear representations of <m>S_3</m>. We also find the Artin-Wedderburn decomposition of <m>{\mathbb{R}}[S_3]</m>.
              </p>

              <p>
                The one dimensional represenatations are given by group homomorphisms of the form <m>S_3 \to {\mathbb{R}}^\times</m>, and any such map factors as <me>S_3 \twoheadrightarrow S_3^{\mathrm{ab}} \to {\mathbb{R}}^\times.</me> Note that <m>S_3^{\mathrm{ab}} = S_3/A_3 \cong C_2</m> and there are two group homomorphisms <m>C_2 \to {\mathbb{R}}^\times</m>, sending the generator to either <m>1</m> or <m>-1</m> (the only elements of <m>{\mathbb{R}}^\times</m> of order <m>1</m> or <m>2</m>). This gives two representations: <m>M_1 = {\mathbb{R}}</m> with <m>S_3</m> acting trivially and <m>M_2 = {\mathbb{R}}</m> with <m>S_3</m> acting by the sign representation. These are not isomorphic by the previous lemma.
              </p>

              <p>
                We have that <m>1= \dim_{{\mathbb{R}}}(M_1) = n_1 \cdot \dim_{{\mathbb{R}}}(D_1)</m> so <m>n_1=1</m> and <m>\dim_{{\mathbb{R}}}(D_1)=1</m>, and likewise <m>1= \dim_{{\mathbb{R}}}(M_2) = n_2 \cdot \dim_{{\mathbb{R}}}(D_2)</m> so <m>n_2=1</m> and <m>\dim_{{\mathbb{R}}}(D_2)=1</m>. So, the Artin-Wedderburn decomposition starts as
              </p>

              <p>
                <me>{\mathbb{R}}[S_3] \cong {\mathbb{R}}\times {\mathbb{R}}\times \cdots.</me>
              </p>

              <p>
                Note that there are no further factors of <m>{\mathbb{R}}</m>, since we found all of the one-dimensional simple modules.
              </p>

              <p>
                Recall also that <m>S_3</m> acts on <m>{\mathbb{R}}^3</m> by permuting the basis (corresponding to the group homomorphism <m>S_3 \to \operatorname{GL}_3({\mathbb{R}})</m> sending a permutation to its associated permutation matrix). The subspace <m>M_3 = \{(a,b,c) \in {\mathbb{R}}^3 \mid a + b + c = 0\}</m> is a subrepresenation of <m>{\mathbb{R}}^3</m> of dimension <m>2</m>. We claim it is irreducible: Say <m>0 \ne (a,b,c) \in M_3</m>.
              </p>

              <p>
                By applying a permutation and scaling appropriately we obtain an element of the form <m>(1,x,-1-x) \in M_3</m> and hence <m>(1, -1-x, x) \in M_3</m>. Adding these gives <m>(2,-1,-1) \in M_3</m> and hence <m>(-1, 2,-1) \in M_3</m>. The latter two are linearly independent and so must span <m>M_3</m>. This proves <m>(a,b,c)</m> generates <m>M_3</m> as a left <m>{\mathbb{R}}[S_3]</m>-module and hence that <m>M_3</m> is simple. Note that <m>M_3</m> is not isomorphic to either <m>M_1</m> nor <m>M_2</m> by dimension considerations.
              </p>

              <p>
                We have that <m>2=\dim{{\mathbb{R}}}(M_3) = n_3 \cdot \dim_{{\mathbb{R}}}(D_3)</m>, so there are two possibilities.
              </p>

              <p><ol>
                <li>
                      <p>
                One possibility is <m>n_3=1</m> and <m>\dim_{{\mathbb{R}}}(D_3)=2</m>, in which case <m>D_3\cong \mathbb{C}</m>, so the Artin-Wedderburn decomposition reads as <me>{\mathbb{R}}[S_3] \cong {\mathbb{R}}\times {\mathbb{R}}\times \mathbb{C}\times S</me> for some <m>S</m>. We must have <m>\dim_{{\mathbb{R}}}(S)=2</m>. We know that <m>S</m> cannot have any one-dimensional simple modules (since we already accounted for all of the one-dimensional simple modules for <m>{\mathbb{R}}[S_3]</m>), so <m>S</m> cannot be <m>{\mathbb{R}}\times {\mathbb{R}}</m>. Then, for dimension reasons, we must have that <m>S\cong D_4</m> with <m>\dim_{{\mathbb{R}}}(D_4)=2</m>, so <m>S\cong \mathbb{C}</m>. But then <me>{\mathbb{R}}[S_3] \cong {\mathbb{R}}\times {\mathbb{R}}\times \mathbb{C}\times \mathbb{C}</me> would be commutative, which it is not, as <m>S_3</m> is not abelian.
              </p>
                </li>

                <li>
                      <p>
                The other possibility is <m>n_3=2</m> and <m>D_3={\mathbb{R}}</m>. We obtain the AW decomposition <me>{\mathbb{R}}[S_3] \cong {\mathbb{R}}\times{\mathbb{R}}\times \operatorname{Mat}_2({\mathbb{R}}).</me>
              </p>
                </li>

              </ol></p>

              <p>
                (Alternatively, we could compute the endomorphism ring of <m>M_3</m> and see that it contains only scalars.)
              </p>

              <p>
                We have found the AW decomposition of <m>{\mathbb{R}}[S_3]</m>. As a consequence, we have identified all of the irreducible real representations of <m>S_3</m>.
              </p>
          </example>

        </section>

        <section xml:id="sec-rep-closed-fields">
          <title>Algebraically Closed Fields</title>

          <p>
            When working over an algebraically closed field, the Artin-Wedderburn Theorem takes a simpler form.
          </p>

          <corollary xml:id="cor-art-wedd-gring-alg-closed">
            <title>Artin-Wedderburn for Group Rings over Algebraically Closed Fields</title>
            
            
            <statement>
              <p>
                If <m>G</m> is a finite group and <m>K</m> is an algebraically closed field such that <m>\mathrm{char}(K) \nmid |G|</m>, then there is an isomorphism of rings <me>k[G] \cong \operatorname{Mat}_{n_1}(K) \times \cdots \times \operatorname{Mat}_{n_m}(K).</me>
              </p>

              <p>
                Moroever, we have:

                <ol>
                  <li>
                    <p>
                      <m>m</m> is the number of irreducible <m>K</m>-linear representation of <m>G</m> (up to isomorphism),
                    </p>
                  </li>

                  <li>
                    <p>
                      the <m>D_i</m>'s are the opposite rings of the endomorphism rings of these representations,
                    </p>
                  </li>

                  <li>
                    <p>
                      the <m>n_j</m>'s give the number of times each irreducible representation occurs in the decomposition of the regular representation of <m>G</m>,
                    </p>
                  </li>

                  <li>
                    <p>
                      the <m>n_j</m>'s also give the dimensions of these representations, and
                    </p>
                  </li>

                  <li>
                    <p>
                      <m>n_1^2  + \cdots + n_m^2 = |G|</m>.
                    </p>
                  </li>
                </ol>
              </p>
            </statement>

            <proof>
              <p>
                The point is that in this setting, for each irreducible representation <m>M_i</m>, <m>D_i \cong \operatorname{End}_{K[G]}(M_i)^{\mathrm{op}}</m> is equal to <m>K</m>. Let <m>\theta\in \operatorname{End}_{K[G]}(M_i)</m>. In particular, <m>\theta</m> is a <m>K</m>-linear endomorphism of the finite dimensional vector space <m>M_i</m>. Since <m>K</m> is algebraically closed, <m>\theta</m> has an eigenvaluse, say <m>\lambda</m>. Then <m>\theta-\lambda 1_{M_i}</m> is a <m>K[G]</m>-linear endomorphism of <m>M_i</m> that is not injective, so by Schur’s Lemma is must be <m>0</m>. Thus, <m>\theta = \lambda 1_{M_i}</m>
              </p>
            </proof>
          </corollary>

          <example>
            <p>
              Let <m>k = \mathbb{C}</m> and consider the alternating group <m>G = A_4</m> of order 12. We find all the simple modules over the ring <m>\mathbb{C}[A_4]</m> or, equivalently, all irreducible <m>\mathbb{C}</m>-linear representations of <m>A_4</m>. We also find the Artin-Wedderburn decomposition of <m>\mathbb{C}[A_4]</m>.
            </p>

            <p>
              As before we start by finding 1-dimensional representations given by group homomorphisms of the form <m>A_4 \to \mathbb{C}^\times</m>. Any such map factors as <me>A_4 \twoheadrightarrow A_4^{\mathrm{ab}}\cong C_3 \to \mathbb{C}^\times</me> and thus there are three nonisomorphic 1-dimensional representations given by <m>\rho_i:C_3=\langle g\rangle \to \mathbb{C}^\times</m>, <m>\rho_i(g)=e^\frac{2\pi i}{3}</m>, with <m>i=0,1,2</m>. Note that <m>\rho_0</m> corresponds to the trivial representation. Also <m>\rho_1</m> and <m>\rho_2</m> make essential use of the fact that we are working over <m>\mathbb{C}</m> as opposed to, say, <m>{\mathbb{R}}</m> where there are no primitive cubic roots of 1.
            </p>

            <p>
              With respect to the Artin-Wedderburn decomposition we have so far <me>\mathbb{C}[A_4] \cong \mathbb{C}\times \mathbb{C}\times \mathbb{C}\times \operatorname{Mat}_{n_4}(\mathbb{C})\times \cdots \times \operatorname{Mat}_{n_m}(\mathbb{C}).</me> where <m>n_3, \ldots, n_m\geq 2</m> because we have already found all the 1-dimensional representations (<m>n_i=1</m>) above. Counting dimensions we obtain <me>12= 1+ 1 + 1 +\sum_{i=4}^m {n_i}^2.</me> It is easy to see there is only one solution: <m>m=4</m> and <m>n_4=3</m>. Hence there is a unique up to isomorphism <m>\mathbb{C}</m>-linear irreducible representation of <m>A_4</m> which is a 3 dimensional <m>\mathbb{C}</m>-vector space.
            </p>

            <p>
              To exhibit such a representation, let <m>A_4</m> act on <m>V=\mathbb{C}^4</m> by permuting the standard basis elements and thus any vector in <m>V</m>. The subspace <m>W\subseteq V</m> given by <me>W = \{ (a,b,c,d) \ | \ a+b+c+d=0\}</me> is an <m>A_4</m>-stable subspace. This is an irreducible representation: if <m>v\in W\smallsetminus 0</m>, after permuting and scaling, we can write <m>v=(1,x,y,-1-x-y)</m>. We also have <m>(1,-1-x-y,x,y)</m> and <m>(1,y,-1-x-y,x)</m> in <m>\langle v \rangle</m>, so the sum <m>(3,-1,-1,-1)\in \langle v \rangle</m>. Then <m>(-1,3,-1,-1)</m> and <m>(-1,-1,3,-1)</m> are also in <m>\langle v \rangle</m>, and these are three linearly independent vectors, so we must have <m>\langle v \rangle=W</m>.
            </p>
          </example>

          <remark>
            <p>
              Let's consider what the Artin-Wedderburn Theorem says about complex representations of finite abelian groups: the group ring must be a product of copies of <m>\mathbb{C}</m>, so every irreducible representation is one-dimensional. Thus, every representation is a sum of one-dimensional representations. Concretely, this means that there is a basis in which every group element acts as a diagonal matrix.
            </p>

            <p>
					    This special case actually just follows from basic facts in linear algebra. Let <m>\rho:G\to \operatorname{GL}_n(\mathbb{C})</m> be a representation. Then every <m>g\in G</m> has finite order, so <m>g^k-1</m> for some <m>k</m>. This implies that the matrix <m>\rho(g)</m> satisfies <m>\rho(g)^k = I_n</m>, so its minimal polynomial divides <m>x^k-1</m>. This polynomial splits into distinct linear factors over <m>\mathbb{C}</m>, so <m>\rho(g)</m> is diagonalizable for every <m>g\in G</m>. (So far, we’ve only used that <m>G</m> is finite.) Now, since <m>g</m> is abelian, we have <m>gh=hg</m> for all <m>g,h\in G</m>, so <m>\rho(g) \rho(h) = \rho(h) \rho(g)</m>; i.e., the matrices commute. Commuting diagonalizable matrices are simultaneously diagonalizable; i.e., there is a basis as above.
            </p>
          </remark>

          <proposition xml:id="prop-art-wedd-decomp">
            <statement>
              <p>
                Let <m>G</m> be a finite group. The number of one-dimensional complex representations of <m>G</m> (up to isomorphism) is <m>|G^{\mathrm{ab}}|</m>. Thus, in the Artin-Wedderburn decomposition of <m>\mathbb{C}[G]</m>, there are exactly <m>|G^{\mathrm{ab}}|</m> copies of <m>\mathbb{C}</m>.
              </p>
            </statement>

            <proof>
              <p>
                We have that <m>\mathrm{Hom}_{\mathbf{Grp}}(G,\mathbb{C}^\times) \cong \mathrm{Hom}_{\mathbf{Ab}}(G^{\mathrm{ab}},\mathbb{C}^\times)</m>, and by the discussion above, there are <m>|G^{\mathrm{ab}}|</m> distinct one-dimensional representations of <m>G^{\mathrm{ab}}</m>.
              </p>
            </proof>
          </proposition>

          <proposition xml:id="prop-reps-and-conjclass">
            <statement>
              <p>
                For any finite group <m>G</m>, the number of irreducible complex representations (up to isomorphism) is equal to the number of conjugacy classes.
              </p>
            </statement>

            <proof>
              <p>
                We have <me>\mathbb{C}[G] \cong \operatorname{Mat}_{n_1}(\mathbb{C}) \times \cdots \times \operatorname{Mat}_{n_m}(\mathbb{C})</me> and <m>m</m> is the number of irreducible complex representations up to isomorphism. A key point is that the center of the right side is <m>\mathbb{C}I_{n_1} \times \cdots \times \mathbb{C}I_{n_m}</m>, which has dimension <m>m</m> as a complex vector space. Since this ring isomorphism is <m>\mathbb{C}</m>-linear, it induces a <m>\mathbb{C}</m>-linear isomorphism of the centers, and thus we just need to show that <m>\dim_\mathbb{C}(Z(\mathbb{C}[G]))</m> is equal to the number of conjugacy classes.
              </p>

              <p>
                Let <m>C_1, \dots, C_h</m> denote the conjugacy classes of <m>G</m> (i.e., the orbits for the action of <m>G</m> on itself by conjugation). For each <m>i</m> set <m>z_i = \sum_{g \in C_i} g \in \mathbb{C}[G]</m>. Then for all <m>x \in G</m>, <m>x z_i x ^{-1}  = \sum_{g \in C_i} x g x^{-1} = z_i</m> and it follows that <m>z_i \in Z(\mathbb{C}[G])</m>. Since that <m>z_i</m>’s are sums of disjoint subsets of a basis of <m>\mathbb{C}[G]</m>, they are linearly independent. Now say <m>\sum_g c_g g</m> belongs to the center. Then for each <m>x \in G</m>, <me>\sum_g c_g xgx^{-1} =
                x(\sum_g c_g g)x^{-1} = \sum_g c_g g</me> and it follows that <m>c_g = c_h</m> whenever <m>g,h</m> are conjugate. This proves that <m>Z(\mathbb{C}[G])</m> is spanned by <m>z_1, \dots, z_h</m>. We conclude that <m>h = \dim_\mathbb{C}(Z(\mathbb{C}[G])) = m</m>.
              </p>
            </proof>
          </proposition>
          
        </section>
      </chapter>

      <chapter xml:id="ch-character">
        <title>Character Theory</title>

        <definition xml:id="def-class-function">
          <statement>
            <p>
              A <em>class function</em> is any function <m>G</m> into <m>F</m> 
            </p>
          </statement>
        </definition>
        
      </chapter>
      

    </part>
-->

    <backmatter xml:id="backmatter">
      <title>Backmatter</title>

      <appendix>
        <title>Foundational Knowledge</title>

        <section xml:id="sec-">
          <title>Sets, Functions, Constructions</title>

          <subsection xml:id="subsec-sets">
            <title>Sets</title>
            
            <p>hi</p>
          </subsection>

          <subsection xml:id="subsec-functions">
            <title>Functions</title>
            
            <p>hi</p>
          </subsection>

          <subsection xml:id="subsec-set-constructions">
            <title>Set Constructions</title>

            <subsubsection xml:id="subsubsec-subsets">
              <title>Subsets</title>

              <p>hi</p>
            </subsubsection>

            <subsubsection xml:id="subsubsec-product-sets">
              <title>Product Sets</title>
              
              <p>hi</p>
            </subsubsection>

            <subsubsection xml:id="subsubsec-quotient-sets">
              <title>Quotient Sets</title>
              
              <p>hi</p>
            </subsubsection>
            

          </subsection>          
        </section>

        <section xml:id="sec-numbers-and-cardinality">
          <title>Number Systems and Cardinality</title>

          <p>hi</p>
          
        </section>

        <section xml:id="sec-matrices">
          <title>Matrices</title>

          <p>hi</p>
          
        </section>

      </appendix>

      <appendix>
        <title>History</title>

        <p>
          Abelian groups are named after Norwegian mathematician Niels Henrik Abel, who made significant contributions to the study of group theory in the early 19th century. Abel was one of the first mathematicians to investigate the properties of groups, which are sets of elements that can be combined under an operation (such as addition or multiplication) that satisfies certain axioms.
        </p>
        <p>
          In particular, Abel studied groups in which the operation is commutative, meaning that the order in which elements are combined does not affect the result. Such groups had been studied before Abel's time, but he was the first to recognize their importance and to develop a systematic theory for them. Abel's work on these groups was influential in the development of abstract algebra, a branch of mathematics that deals with algebraic structures like groups, rings, and fields.
        </p>
        <p>
          The term "Abelian" was coined in honor of Abel's contributions to the study of commutative groups. The adjective "Abelian" is now used to describe any algebraic structure (not just groups) in which the operation is commutative.
        </p>

        <exercises>

          <exercisegroup>
            <title>Computations and Examples</title>
            <introduction>
              <p>
              </p>
            </introduction>
        
            <exercise>
              <title></title>
              
              
              <statement>
                <p>
                  Coming soon to an OER near you!
                </p>
              </statement>

              <hint>
                <p>
                  Coming soon to an OER near you!
                </p>
              </hint>

              <solution>
                <p>
                  Coming soon to an OER near you!
                </p>
              </solution>
            </exercise>
        
          </exercisegroup>
        
          <exercisegroup>
            <title>Formal Proofs</title>
            <introduction>
              <p>
              </p>
            </introduction>
        
            <exercise>
              <title></title>
              
              
              <statement>
                <p>
                  Coming soon to an OER near you!
                </p>
              </statement>

              <hint>
                <p>
                  Coming soon to an OER near you!
                </p>
              </hint>

              <solution>
                <p>
                  Coming soon to an OER near you!
                </p>
              </solution>
            </exercise>
        
          </exercisegroup>
        
          <exercisegroup>
            <title>Qualifying Exam Problems</title>
            <introduction>
              <p>
              </p>
            </introduction>
        
            <exercise>
              <title></title>
              
              
              <statement>
                <p>
                  Coming soon to an OER near you!
                </p>
              </statement>

              <hint>
                <p>
                  Coming soon to an OER near you!
                </p>
              </hint>

              <solution>
                <p>
                  Coming soon to an OER near you!
                </p>
              </solution>
            </exercise>
        
          </exercisegroup>
        </exercises>


      </appendix>

      <appendix>
        <title>Notation</title>

        <notation-list/>
      </appendix>

      <appendix xml:id="appendix-list-definitions">
        <title>List of Definitions</title>
      
        <list-of elements="definition" divisions="chapter" empty="yes"/>
      </appendix>

      <appendix xml:id="appendix-list-results">
        <title>List of Results</title>
      
        <list-of elements="theorem lemma proposition corollary" divisions="chapter" empty="yes"/>
      </appendix>

      <colophon>
        <p> This book was authored in <pretext />. </p>
      </colophon>

      

    </backmatter>

  </book>

</pretext>

